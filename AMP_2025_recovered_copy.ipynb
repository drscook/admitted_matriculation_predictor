{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "972b96da-d76f-4ec9-b4b2-7e701a07aeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- done: race__xxx string not boolean\n",
    "- zip not fully fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dd982f-3630-4ea9-8756-47674ae4dec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import display, HTML, clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming openpyxl pgeocode\n",
    "    %reload_ext autotime\n",
    "clear_output()\n",
    "\n",
    "import pathlib, shutil, warnings, requests, dataclasses, numpy as np, pandas as pd\n",
    "from codetiming import Timer\n",
    "seed = 42\n",
    "catalog = 'dev.bronze.'\n",
    "root = pathlib.Path(f'/Workspace/Users/{username}@tarleton.edu/admitted_matriculation_predictor_2025/')\n",
    "data = root/'data'\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "##########################################\n",
    "############ helper functions ############\n",
    "##########################################\n",
    "tab = '    '\n",
    "divider = '\\n##############################################################################################################\\n'\n",
    "\n",
    "def listify(*args):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        return list(*args)\n",
    "    except Exception as e:\n",
    "        return list(args)\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    return \"(\" + indent('\\n'+qry.strip()+'\\n)', lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=seed):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep()\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "# [warnings.filterwarnings(action='ignore', message=f\".*{w}.*\") for w in [\n",
    "#     \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "#     \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "#     \"errors='ignore' is deprecated\"\n",
    "#     \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "#     \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "#     \"DataFrame is highly fragmented\",\n",
    "# ]]\n",
    "\n",
    "############ pandas functions ############\n",
    "pd.options.display.max_columns = None\n",
    "def disp(df, rows=4, head=True, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    with pd.option_context('display.min_rows', rows, 'display.max_rows', rows):\n",
    "        print(df.shape)\n",
    "        df = df.sort_index(axis=1) if sort else df\n",
    "        X = pd.concat([df.dtypes.to_frame().T, df.head(rows) if head else df.tails(rows)])\n",
    "        display(HTML(X.to_html()))\n",
    "\n",
    "\n",
    "def to_numeric(df, downcast='integer', errors='ignore', **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: s.astype('string').str.lower().str.strip() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "    idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "    return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    return wrapper\n",
    "\n",
    "for f in [disp, to_numeric, prep]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setattr(pd.DataFrame, f.__name__, f)\n",
    "    setattr(pd.Series, f.__name__, wrap(f))\n",
    "\n",
    "\n",
    "def get_nearest(x):\n",
    "    url = f\"http://router.project-osrm.org/nearest/v1/driving/{x['longitude']},{x['latitude']}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['waypoints'][0]['location']\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def get_driving_distance(x, y):\n",
    "    def get_distance(lon, lat):\n",
    "        url = f\"http://router.project-osrm.org/route/v1/driving/{lon},{lat};{y['longitude']},{y['latitude']}?overview=false\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['routes'][0]['distance'] / 1000\n",
    "        else:\n",
    "            return None\n",
    "    lon, lat = x['longitude'], x['latitude']\n",
    "    distance = get_distance(lon, lat)\n",
    "    if distance is None:\n",
    "        x.to_frame().disp()\n",
    "        lon, lat = get_nearest(x)\n",
    "        distance = get_distance(lon, lat)\n",
    "    return distance\n",
    "\n",
    "#########################################\n",
    "################## AMP ##################\n",
    "#########################################\n",
    "@dataclasses.dataclass\n",
    "class Term():\n",
    "    term_code: int = 202408\n",
    "    cycle_day: int = None\n",
    "    cycle_date: str = None\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.get_terms()\n",
    "\n",
    "    def get(self, fcn, nm, overwrite=False, path=None, divide=True, prereq=[], read=True, **kwargs):\n",
    "        dst = data/f'{nm}/{self.term_code}/{nm}_{self.stem}.parquet' if path is None else pathlib.Path(path).with_suffix('.parquet')\n",
    "        new = False\n",
    "        if overwrite:\n",
    "            del self[nm]\n",
    "            dst.unlink(missing_ok=True)\n",
    "        if not nm in self:\n",
    "            if dst.exists():\n",
    "                if read:\n",
    "                    self[nm] = pd.read_parquet(dst)\n",
    "                else:\n",
    "                    self[nm] = None\n",
    "            else:\n",
    "                new = True\n",
    "                for f in listify(prereq):\n",
    "                    f()\n",
    "                print(f'creating {dst.name}: ', end='')\n",
    "                # with Timer(text=lambda secs: f\"creating {dst}: {format_timespan(secs)}\"):\n",
    "                with Timer():\n",
    "                    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    self[nm] = fcn(**kwargs).prep()\n",
    "                    self[nm].to_parquet(dst)\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "        return self[nm], new\n",
    "\n",
    "\n",
    "############################################################\n",
    "################# get zip code information #################\n",
    "############################################################\n",
    "    def get_zips(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            print()\n",
    "            from pgeocode import Nominatim\n",
    "            nomi = Nominatim('us')\n",
    "            df = nomi.query_postal_code(pd.Series(nomi._data['postal_code'])).query(\"state_code.notnull() & state_code not in ['AK', 'HI', 'MH']\").prep().set_index('postal_code').rename_axis('zip')\n",
    "            dct = {\n",
    "                's': 76402,\n",
    "                'm': 76036,\n",
    "                'w': 76708,\n",
    "                'r': 77807,\n",
    "            }\n",
    "            L = [\n",
    "                self.get(\n",
    "                    lambda: X.apply(get_driving_distance, y=df.loc[v], axis=1).rename('distance').reset_index().assign(camp_code=k),\n",
    "                    f'zips_{s}_{k}',\n",
    "                    overwrite=False, # this process takes about 10 hours and we do NOT want it accidentally deleted, so we force user to manually delete these files if they REALLY want to overwrite all that work\n",
    "                    path=data/f'zips/zips_{s}_{k}',\n",
    "                    divide=False,\n",
    "                )[0] for s, X in df.groupby('state_code') for k,v in dct.items() ]\n",
    "            df = pd.concat(L)\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'zips', overwrite, path=data/'zips/zips')\n",
    "        return df\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, overwrite=False, show=False):\n",
    "        self.term_code = int(self.term_code)\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'terms', overwrite, path=data/'terms')\n",
    "        self.cycle_day, self.cycle_date, self.stem = self.get_cycle(self.term_code, self.cycle_day, self.cycle_date)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_cycle(self, term_code, cycle_day=None, cycle_date=None):\n",
    "        stable_date = self.terms.loc[int(term_code),'stable_date']\n",
    "        if cycle_day is None:\n",
    "            if cycle_date is None:\n",
    "                cycle_date = pd.Timestamp.now()\n",
    "            cycle_date = pd.to_datetime(cycle_date).normalize()\n",
    "            cycle_day = (stable_date - cycle_date).days\n",
    "        cycle_date = (stable_date - pd.Timedelta(days=cycle_day)).date()\n",
    "        # cycle_date = str((stable_date - pd.Timedelta(days=cycle_day)).date())\n",
    "        stem = f'{term_code}_{cycle_date}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        return cycle_day, cycle_date, stem\n",
    "\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, overwrite=False, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id,\n",
    "                spriden_pidm as pidm\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self):\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        counter = 0\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > 5:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                cycle_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    cycle_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            # print(a, dt.date())\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                cycle_day, cycle_date, stem = self.get_cycle(term_code, cycle_date=cycle_date)\n",
    "                def fcn():\n",
    "                    df = (\n",
    "                        self.get_spriden()\n",
    "                        .assign(cycle_day=cycle_day, cycle_date=cycle_date)\n",
    "                        .merge(book.parse(sheet).prep(), on='id', how='right')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, 'flags', overwrite, path=flags_prc/f'{term_code}/flags_{stem}', divide=False)[1]:\n",
    "                    counter = 0\n",
    "                    dst = flags_prc/f'{term_code}.parquet'\n",
    "                    dst.unlink(missing_ok=True)\n",
    "                del self['flags']\n",
    "        self.combine_flags()\n",
    "        print(divider)\n",
    "\n",
    "\n",
    "    def combine_flags(self, overwrite=False, show=False):\n",
    "        def fcn(term_code):\n",
    "            F = sorted((flags_prc/f'{term_code}').glob('*.parquet'))+sorted((flags_prc/f'{term_code-2}').glob('*.parquet'))\n",
    "            L = [pd.read_parquet(src) for src in F]\n",
    "            df = pd.concat(L, ignore_index=True).prep()\n",
    "            del L\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                if k in df:\n",
    "                    df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        for x in flags_prc.iterdir():\n",
    "            if x.is_dir():\n",
    "                term_code = int(x.stem)\n",
    "                if term_code%10==8:\n",
    "                    nm = f'flags_{term_code}'\n",
    "                    self.get(fcn, nm, overwrite, path=flags_prc/nm, read=False, term_code=term_code)\n",
    "                    del self[nm]\n",
    "\n",
    "\n",
    "    def get_flags(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pd.read_parquet(flags_prc/f'flags_{self.term_code}.parquet')\n",
    "                .query(f\"cycle_date<='{self.cycle_date}'\")\n",
    "                .sort_values(['pidm','cycle_date'])\n",
    "                .drop_duplicates(subset=['pidm','term_code'], keep='last')\n",
    "            )\n",
    "            states = ['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY']\n",
    "            df.loc[~df['state'].str.upper().isin(states),'zip'] = pd.NA\n",
    "            df['zip'] = df['zip'].str.split('-', expand=True)[0].str[:5]\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'flags', overwrite, prereq=self.combine_flags)\n",
    "        return df\n",
    "\n",
    "\n",
    "    # def get_flags_history(self, overwrite=False, show=False, cutoff=202206):\n",
    "    #     def fcn():\n",
    "    #         import pyarrow.parquet as pq\n",
    "    #         print()\n",
    "    #         L = []\n",
    "    #         for path in sorted(flags_prc.iterdir(), reverse=True):\n",
    "    #             print(path)\n",
    "    #             for src in path.iterdir():\n",
    "    #                 _, term_code, cycle_date, cycle_day = src.stem.split('_')\n",
    "    #                 col = pq.ParquetFile(src).schema.names\n",
    "    #                 df = pd.DataFrame(columns=col).assign(term_code=[int(term_code)], cycle_date=[cycle_date]).fillna(True)\n",
    "    #                 L.append(df)\n",
    "    #         df = pd.concat(L).fillna(False).set_index(['cycle_date','term_code']).sort_index()\n",
    "    #         return df[sorted(df.columns)]\n",
    "    #     df, new = self.get(fcn, 'flags_history', overwrite, path=root/'flags_history')\n",
    "    #     A = df.query(f'term_code>={cutoff}').groupby('term_code').sum().sort_index(ascending=False).T.rename_axis('variable')\n",
    "    #     B = A == A.max()\n",
    "    #     B.insert(0, 'n', B.sum(axis=1))\n",
    "    #     return B.reset_index().sort_values(['n', 'variable'], ascending=[False, True])\n",
    "\n",
    "\n",
    "##############################################\n",
    "############### get admissions ###############\n",
    "##############################################\n",
    "    def get_admissions(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            L = [f\"\"\"\n",
    "select\n",
    "    *\n",
    "from (\n",
    "    select distinct\n",
    "        A.*\n",
    "        ,min(current_date) over (partition by pidm, appl_no) as first_date\n",
    "        ,max(current_date) over (partition by pidm, appl_no) as final_date\n",
    "    from\n",
    "        dev.opeir.opeiradmissions_{self.terms.loc[term_code,'term_desc']} as A\n",
    "        --dev.opeir.admissions_{self.terms.loc[term_code,'term_desc']}_v as A\n",
    "    inner join\n",
    "        {catalog}saturnstvapdc as B\n",
    "    on\n",
    "        apdc_code = stvapdc_code\n",
    "    where\n",
    "        stvapdc_inst_acc_ind is not null  --only accepted\n",
    "    qualify\n",
    "        '{self.cycle_date}' between first_date and final_date --keep only pidm, appl_no where cycle_date falls between its first and last records\n",
    "    )\n",
    "where\n",
    "    current_date <= '{self.cycle_date}'  -- only records before cycle_date\n",
    "qualify\n",
    "    row_number() over (partition by pidm, appl_no order by current_date desc) = 1  -- most current record remaiming for this pidm, appl_no\n",
    "\"\"\"\n",
    "                for term_code in [self.term_code-2, self.term_code]]\n",
    "            qry = join(L, '\\nunion all\\n')\n",
    "\n",
    "            def get_desc(code):\n",
    "                for nm in code.split('_'):\n",
    "                    if len(nm) == 4:\n",
    "                        break\n",
    "                return [f'{code} as {nm}_code, stv{nm}_desc as {nm}_desc', f'left join {catalog}saturnstv{nm} on {code} = stv{nm}_code']\n",
    "            \n",
    "            def coalesce(x,y=False):\n",
    "                return f'coalesce({x}, {y}) as {x}'\n",
    "\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    pidm\n",
    "    ,{self.cycle_day} as cycle_day\n",
    "    ,timestamp('{self.cycle_date}') as cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,final_date\n",
    "    ,{get_desc('term_code')[0]}\n",
    "    ,appl_no\n",
    "    ,{get_desc('apst_code')[0]}\n",
    "    ,{get_desc('apdc_code')[0]}\n",
    "    ,{get_desc('admt_code')[0]}\n",
    "    ,{get_desc('wrsn_code')[0]}\n",
    "    ,{get_desc('levl_code')[0]}\n",
    "    ,{get_desc('styp_code')[0]}    \n",
    "    ,{get_desc('camp_code')[0]}\n",
    "    ,{get_desc('coll_code_1')[0]}\n",
    "    ,{get_desc('dept_code')[0]}\n",
    "    ,{get_desc('majr_code_1')[0]}\n",
    "    ,{get_desc('saradap_resd_code')[0]}\n",
    "    ,gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')[0]}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, stvnatn_nation as natn_desc\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    ,hs_percentile\n",
    "    ,sbgi_code\n",
    "    ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_amp_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "        --{catalog}gorvisa_amp_v\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\n",
    "{get_desc('term_code')[1]}\n",
    "{get_desc('levl_code')[1]}\n",
    "{get_desc('styp_code')[1]}\n",
    "{get_desc('admt_code')[1]}\n",
    "{get_desc('wrsn_code')[1]}\n",
    "{get_desc('apst_code')[1]}\n",
    "{get_desc('apdc_code')[1]}\n",
    "{get_desc('camp_code')[1]}\n",
    "{get_desc('coll_code_1')[1]}\n",
    "{get_desc('dept_code')[1]}\n",
    "{get_desc('majr_code_1')[1]}\n",
    "{get_desc('saradap_resd_code')[1]}\n",
    "{get_desc('gorvisa_natn_code_issue')[1]}\n",
    "{get_desc('spbpers_lgcy_code')[1]}\n",
    "\n",
    "qualify\n",
    "    min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "    and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "\"\"\"\n",
    "\n",
    "# don't delete - could be useful & was hard to create\n",
    "            # stat_codes = ['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY'] # not AK & HI b/c can't get driving distance\n",
    "#     ,{get_desc('spraddr_cnty_code')[0]}\n",
    "#     ,{get_desc('spraddr_stat_code')[0]}\n",
    "#     ,zip_code\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#         ,try_to_number(left(spraddr_zip, 5), '00000') as zip_code\n",
    "#         ,case\n",
    "#             when spraddr_atyp_code = 'PA' then 6\n",
    "#             when spraddr_atyp_code = 'PR' then 5\n",
    "#             when spraddr_atyp_code = 'MA' then 4\n",
    "#             when spraddr_atyp_code = 'BU' then 3\n",
    "#             when spraddr_atyp_code = 'BI' then 2\n",
    "#             when spraddr_atyp_code = 'P1' then 1\n",
    "#             when spraddr_atyp_code = 'P2' then 0\n",
    "#             end as spraddr_atyp_rank\n",
    "#     from\n",
    "#         {catalog}spraddr_amp_v\n",
    "#     where\n",
    "#         spraddr_stat_code in ('{join(stat_codes, \"','\")}')\n",
    "#         and spraddr_zip is not null\n",
    "#     qualify\n",
    "#         row_number() over (partition by spraddr_pidm order by spraddr_atyp_rank desc, spraddr_seqno desc) = 1\n",
    "# )\n",
    "# on\n",
    "#     pidm = spraddr_pidm\n",
    "\n",
    "# {get_desc('spraddr_cnty_code')[1]}\n",
    "# {get_desc('spraddr_stat_code')[1]}\n",
    "\n",
    "            df = run(qry, show).query(\"styp_code in ['n','r','t']\")\n",
    "            return df \n",
    "        df, new = self.get(fcn, 'admissions', overwrite)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_students(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            return (\n",
    "                self.admissions\n",
    "                .merge(self.flags, on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                .merge(self.zips, on=['zip','camp_code'], how='left', suffixes=['', '_zips'])\n",
    "                .set_index(['pidm'])\n",
    "            )\n",
    "        df, new = self.get(fcn, 'students', overwrite, prereq=[\n",
    "            self.get_admissions,\n",
    "            self.get_flags,\n",
    "            self.get_zips,\n",
    "            ])\n",
    "        mask = df['cycle_date_flags'].isnull()  # rows from admissions not on flags - should not be any\n",
    "        if mask.any():\n",
    "            df[mask].disp(4)\n",
    "        return df\n",
    "\n",
    "\n",
    "########################################################\n",
    "############### get course registrations ###############\n",
    "########################################################\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#                 'ssbsect_crn':'crn',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "\n",
    "#         S = self.students[['pidm','term_code','styp_code']]\n",
    "#         return df\n",
    "    def get_registrations(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    sfrstcr_pidm as pidm\n",
    "    ,sfrstcr_term_code as term_code\n",
    "    ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "    ,max(ssbsect_credit_hrs) as credit_hr\n",
    "from\n",
    "    {catalog}saturnsfrstcr as A\n",
    "inner join\n",
    "    {catalog}saturnssbsect as B\n",
    "on\n",
    "    sfrstcr_term_code = ssbsect_term_code\n",
    "    and sfrstcr_crn = ssbsect_crn\n",
    "where\n",
    "    sfrstcr_term_code = {self.term_code}\n",
    "    and sfrstcr_error_flag is null\n",
    "    and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "    and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "    and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "    and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "group by\n",
    "    sfrstcr_pidm\n",
    "    ,sfrstcr_term_code\n",
    "    ,ssbsect_subj_code\n",
    "    ,ssbsect_crse_numb\n",
    "\"\"\"\n",
    "\n",
    "            qry = f\"\"\"\n",
    "with A as {subqry(qry)}\n",
    "select * from A\n",
    "\n",
    "union all\n",
    "\n",
    "select\n",
    "    pidm\n",
    "    ,term_code\n",
    "    ,'_allcrse' as crse_code\n",
    "    ,sum(credit_hr) as credit_hr\n",
    "from A\n",
    "group by\n",
    "    pidm\n",
    "    ,term_code\n",
    "\n",
    "union all\n",
    "\n",
    "select\n",
    "    pidm\n",
    "    ,term_code\n",
    "    ,'_anycrse' as crse_code\n",
    "    ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "from A\n",
    "group by\n",
    "    pidm\n",
    "    ,term_code\n",
    "\"\"\"\n",
    "            df = run(qry, show)\n",
    "            return self.admissions[['pidm','styp_code']].merge(df, how='inner')\n",
    "        df, new = self.get(fcn, 'registrations', overwrite, prereq=self.get_admissions)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_enrollments(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            return self.registrations.query('credit_hr>0').groupby(['crse_code','styp_code']).size().rename('headcount').to_frame()\n",
    "            # A = (self.registrations>0)\n",
    "            # A[A].groupby(['crse_code','styp_code'])[['credit_hr']].agg(lambda x:(x>0).sum())\n",
    "            # return self.registrations.query('credit_hr>0').groupby(['crse_code','styp_code'])[['credit_hr']].agg(lambda x:(x>0).sum())\n",
    "            # return self.registrations.groupby(['crse_code','styp_code'])[['credit_hr']].agg(lambda x:(x>0).sum()).query('credit_hr>0')\n",
    "        df, new = self.get(fcn, 'enrollments', overwrite, prereq=self.get_registrations)\n",
    "        return df\n",
    "####################################\n",
    "############### main ###############\n",
    "####################################\n",
    "self = Term(\n",
    "    term_code=202408,\n",
    "    # cycle_day=50,\n",
    "    cycle_date='2024-06-26',\n",
    ")\n",
    "\n",
    "# self.get_zips()\n",
    "# self.process_flags()\n",
    "self.get_flags(overwrite=True, show=True,)\n",
    "self.get_admissions(overwrite=True, show=True,)\n",
    "self.get_students(overwrite=True, show=True,)\n",
    "self.get_registrations(overwrite=True, show=True,)\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9912210f-83c7-4d6e-b2ce-e672726f5bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class AMP(Term):\n",
    "    crse_code : str = '_anycrse'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "\n",
    "\n",
    "crse_codes = [\n",
    "    '_anycrse',\n",
    "]\n",
    "\n",
    "for crse_code in crse_codes:\n",
    "    self = AMP(\n",
    "        term_code=202408,\n",
    "        crse_code=crse_code,\n",
    "        cycle_date='2024-06-26',\n",
    "    )\n",
    "    # self.get_admissions()#overwrite=True)\n",
    "    # current.get_students(overwrite=True)\n",
    "    self.get_registrations()#overwrite=True)\n",
    "    self.get_enrollments(overwrite=True)\n",
    "\n",
    "\n",
    "    stable = AMP(\n",
    "        term_code=self.term_code,\n",
    "        crse_code=self.crse_code,\n",
    "        cycle_day=0,\n",
    "    )\n",
    "    # stable.get_registrations()#overwrite=True)\n",
    "    stable.get_enrollments(overwrite=True)\n",
    "\n",
    "    # .rename('current')\n",
    "\n",
    "    # self.enrollments_stable = stable.enrollments.copy()\n",
    "    # self.inflation = self.enrollments / self.enrollments_stable\n",
    "    # self.enrollments = self.enrollments.join(stable.enrollments_stable, lsuffix='_current'\n",
    "\n",
    "#     self.df = (\n",
    "#         self.students\n",
    "#         .join(self.registrations.query(f\"crse_code=='{crse_code}'\").set_index('pidm')['credit_hr'].rename('enroll_current') > 0)\n",
    "#         .fillna({'enroll_current':False})    \n",
    "    \n",
    "#     R = self.registrations.query(f\"crse_code=='{crse_code}'\").set_index('pidm')['credit_hr'].rename('enroll_current') > 0\n",
    "# T = self.students.join(R).fillna({'enroll_current':False})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8185768213204940,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AMP_2025_recovered_copy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
