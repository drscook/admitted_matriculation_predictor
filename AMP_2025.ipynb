{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e20b886-4823-4661-a602-6255c4c8ce87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import display, HTML, clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming openpyxl numpy pandas geopandas pgeocode flaml[automl] git+https://github.com/AnotherSamWilson/miceforest.git\n",
    "    dbutils.library.restartPython()\n",
    "    clear_output()\n",
    "    dbutils.notebook.exit('Rerun to use newly installed/updated packages')\n",
    "\n",
    "import pathlib, shutil, pickle, warnings, requests, dataclasses, codetiming, numpy as np, pandas as pd, geopandas as gpd, pgeocode, flaml as fl, miceforest as mf\n",
    "clear_output()\n",
    "catalog = 'dev.bronze.'\n",
    "root = pathlib.Path(f'/Workspace/Users/{username}@tarleton.edu/admitted_matriculation_predictor_2025/')\n",
    "data = root/'data'\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "for w in [\n",
    "    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "    # \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "    # \"errors='ignore' is deprecated\"\n",
    "    # \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "    # \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "    # \"DataFrame is highly fragmented\",\n",
    "    # \"DataFrameGroupBy.apply operated on the grouping columns\",\n",
    "    ]:\n",
    "    warnings.filterwarnings(action='ignore', message=f\".*{w}.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "##########################################\n",
    "############ helper functions ############\n",
    "##########################################\n",
    "tab = '    '\n",
    "divider = '\\n##############################################################################################################\\n'\n",
    "\n",
    "def listify(*args):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        return list(*args)\n",
    "    except Exception as e:\n",
    "        return list(args)\n",
    "\n",
    "def setify(*args):\n",
    "    \"\"\"ensure it is a set\"\"\"\n",
    "    return set(listify(*args))\n",
    "\n",
    "def unique(*args):\n",
    "    \"\"\"get unique items maintaining order\"\"\"\n",
    "    return listify(dict.fromkeys(listify(*args)))\n",
    "\n",
    "def difference(A, B):\n",
    "    return unique([x for x in listify(A) if x not in listify(B)])\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    qry = '\\n' + qry.strip()\n",
    "    qry = '(' + qry + '\\n)' if 'select' in qry else qry\n",
    "    return indent(qry, lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=42):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep().sort_index()\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "def get_desc(code):\n",
    "    for nm in code.split('_'):\n",
    "        if len(nm) == 4:\n",
    "            break\n",
    "    return f'{code} as {nm}_code, (select stv{nm}_desc from {catalog}saturnstv{nm} where {code} = stv{nm}_code limit 1) as {nm}_desc'\n",
    "\n",
    "def coalesce(x, y=False):\n",
    "    return f'coalesce({x}, {y}) as {x}'\n",
    "\n",
    "############ pandas functions ############\n",
    "pd.options.display.max_columns = None\n",
    "def disp(df, rows=4, head=True, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    with pd.option_context('display.min_rows', rows, 'display.max_rows', rows):\n",
    "        print(df.shape)\n",
    "        df = df.sort_index(axis=1) if sort else df\n",
    "        # missing = df.isnull().sum().to_frame().T\n",
    "        # X = pd.concat([df.dtypes.to_frame().T, missing, (missing/df.shape[0]*100).round(2), df.head(rows) if head else df.tails(rows)])\n",
    "        X = df.head(rows) if head else df.tails(rows)\n",
    "        display(HTML(X.to_html()))\n",
    "\n",
    "def inser(df, column, value, loc=0):\n",
    "    df.insert(loc, column, value)\n",
    "    return df\n",
    "\n",
    "def to_numeric(df, downcast='integer', errors='ignore', category=False, **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: s.astype('string').str.lower().str.strip() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s.astype('category') if s.dtype=='string' and category else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "        idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "        return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    return wrapper\n",
    "\n",
    "for f in [disp, inser, to_numeric, prep]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setattr(pd.DataFrame, f.__name__, f)\n",
    "    setattr(pd.Series, f.__name__, wrap(f))\n",
    "\n",
    "def prediction(clf, X, y):\n",
    "    Z = X.copy()\n",
    "    Z['prediction'] = clf.predict_proba(X)[:,1]\n",
    "    Z['actual'] = y\n",
    "    Z['mae'] = np.abs(Z['prediction'] - Z['actual'])\n",
    "    Z['log_loss'] = -1*(Z['actual']*np.log(Z['prediction']) + (1-Z['actual'])*np.log(1-Z['prediction']))\n",
    "    return Z\n",
    "\n",
    "for f in [prediction]:\n",
    "    setattr(fl.automl.automl.AutoML, f.__name__, f)\n",
    "\n",
    "#########################################\n",
    "################## AMP ##################\n",
    "#########################################\n",
    "@dataclasses.dataclass\n",
    "class Term():\n",
    "    term_code: int = 202408\n",
    "    cycle_day: int = None\n",
    "    cycle_date: str = None\n",
    "    overwrite: set = None\n",
    "    seed: int = 42\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.term_code = int(self.term_code)\n",
    "        self.cycle_day, self.cycle_date, self.stable_date, self.stem, self.term_desc, self.year = self.get_cycle(self.term_code, self.cycle_day, self.cycle_date)\n",
    "        self.overwrite = setify(self.overwrite)\n",
    "        # Because these take about 1 hour each, we force user to manually delete parquet files to avoid accidental deletion requring lengthy re-creation\n",
    "        self.overwrite.discard('drivetimes_s')\n",
    "        self.overwrite.discard('drivetimes_m')\n",
    "        self.overwrite.discard('drivetimes_w')\n",
    "        self.overwrite.discard('drivetimes_r')\n",
    "        self.overwrite.discard('drivetimes_l')\n",
    "\n",
    "\n",
    "    def get(self, fcn, dst, prereq=[], *, divide=True, read=True, suffix='.parquet', **kwargs):\n",
    "        nm = str(dst)\n",
    "        if '/' in nm:\n",
    "            dst = pathlib.Path(dst).with_suffix(suffix)\n",
    "            nm = dst.stem\n",
    "        else:\n",
    "            dst = data/f'{dst}/{self.term_code}/{dst}_{self.stem}{suffix}'\n",
    "\n",
    "        if nm in self.overwrite:\n",
    "            del self[nm]\n",
    "            dst.unlink(missing_ok=True)\n",
    "            self.overwrite.remove(nm)\n",
    "\n",
    "        new = False\n",
    "        if not nm in self:\n",
    "            if not dst.exists():\n",
    "                new = True\n",
    "                for f in listify(prereq):\n",
    "                    f()\n",
    "                print(f'creating {dst.name}: ', end='')\n",
    "                with codetiming.Timer():\n",
    "                    rslt = fcn(**kwargs)\n",
    "                    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    if suffix == '.parquet':\n",
    "                        pd.DataFrame(rslt).prep().to_parquet(dst)  # forced to wrap with explicit pd.DataFrame to due strange error under pandas 2.2.3 \"Object of type PlanMetrics is not JSON serializable\" with to_parquet\n",
    "                    elif suffix == 'csv':\n",
    "                        pd.DataFrame(rslt).prep().to_csv(dst)\n",
    "                    else:\n",
    "                        with open(dst, 'wb') as f:\n",
    "                            pickle.dump(rslt, f, pickle.HIGHEST_PROTOCOL)\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "            if read:\n",
    "                if suffix == '.parquet':\n",
    "                    self[nm] = pd.read_parquet(dst)\n",
    "                elif suffix == 'csv':\n",
    "                    self[nm] = pd.read_csv(dst)\n",
    "                else:\n",
    "                    with open(dst, 'rb') as f:\n",
    "                        self[nm] = pickle.load(f)\n",
    "            else:\n",
    "                self[nm] = None\n",
    "        return self[nm], new\n",
    "##################################################\n",
    "################# get drivetimes #################\n",
    "##################################################\n",
    "    def get_zips(self, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pgeocode.Nominatim('us')._data  # get all zips\n",
    "                .prep()\n",
    "                .rename(columns={'postal_code':'zip'})\n",
    "                .query(\"state_code.notnull() & state_code not in [None,'mh']\")\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/zips')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_states(self, show=False):\n",
    "        return set(self.get_zips()['state_code'])\n",
    "\n",
    "\n",
    "    def get_drivetimes(self, show=False):\n",
    "        def fcn():\n",
    "            from pgeocode import Nominatim\n",
    "            from sklearn.metrics.pairwise import haversine_distances\n",
    "            print()\n",
    "            campus_coords = {\n",
    "                's': '-98.215784,32.216217',\n",
    "                'm': '-97.432975,32.582436',\n",
    "                'w': '-97.172176,31.587908',\n",
    "                'r': '-96.467920,30.642055',\n",
    "                'l': '-96.983211,32.462267',\n",
    "                }\n",
    "            url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "            gdf = gpd.read_file(url).prep().set_index('zcta5ce20')  # get all ZCTA https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n",
    "            pts = gdf.sample_points(size=10, method=\"uniform\").explode().apply(lambda g: f\"{g.x},{g.y}\")  # sample 10 random points in each ZCTA\n",
    "            M = []\n",
    "            for k, v in campus_coords.items():\n",
    "                def fcn1():\n",
    "                    print()\n",
    "                    L = []\n",
    "                    i = 0\n",
    "                    di = 200\n",
    "                    I = pts.shape[0]\n",
    "                    while i < I:\n",
    "                        u = join([v, *pts.iloc[i:i+di]],';')\n",
    "                        url = f\"http://router.project-osrm.org/table/v1/driving/{u}?sources={0}&annotations=duration,distance&fallback_speed=1&fallback_coordinate=snapped\"\n",
    "                        response = requests.get(url).json()\n",
    "                        L.append(np.squeeze(response['durations'])[1:]/60)\n",
    "                        i += di\n",
    "                        print(k,i,round(i/I*100))\n",
    "                    df = pts.to_frame()[[]]\n",
    "                    df[k] = np.concatenate(L)\n",
    "                    return df\n",
    "                df, new = self.get(fcn1, root/f'geo/drivetimes_{k}')\n",
    "                M.append(df)\n",
    "            D = pd.concat(M, axis=1).groupby(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "\n",
    "            # There are a few USPS zips without equivalent ZCTA, so we assign them drivetimes for the nearest\n",
    "            Z = self.get_zips().merge(D.query(\"camp_code=='s'\"), how='left').set_index('zip')\n",
    "            mask = Z['drivetime'].isnull()  # zips without a ZTCA\n",
    "            Z = Z[['latitude','longitude']]\n",
    "            X = np.radians(Z[~mask])\n",
    "            Y = np.radians(Z[mask])\n",
    "            M = (\n",
    "                pd.DataFrame(haversine_distances(X, Y), index=X.index, columns=Y.index) # haversine distance between pairs with and without ZCTA\n",
    "                .idxmin()  # find nearest ZCTA\n",
    "                .reset_index()\n",
    "                .set_axis(['new_zip','zip'], axis=1)\n",
    "                .prep()\n",
    "                .merge(D)  # merge the drivetimes for that ZCTA\n",
    "                .drop(columns='zip')\n",
    "                .rename(columns={'new_zip':'zip'})\n",
    "            )\n",
    "            df = pd.concat([D,M], ignore_index=True)\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/drivetimes', self.get_zips)\n",
    "        return df\n",
    "\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, show=False):\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        df, new = self.get(fcn, data/'terms')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_cycle(self, term_code, cycle_day=None, cycle_date=None, show=False):\n",
    "        term_code = int(term_code)\n",
    "        year = term_code // 100\n",
    "        term_desc, stable_date = self.get_terms().loc[term_code,['term_desc','stable_date']]\n",
    "        if cycle_day is None:\n",
    "            if cycle_date is None:\n",
    "                cycle_date = pd.Timestamp.now()\n",
    "            cycle_date = min(pd.to_datetime(cycle_date), pd.Timestamp.now()).normalize()\n",
    "            cycle_day = (stable_date - cycle_date).days\n",
    "        cycle_date = (stable_date - pd.Timedelta(days=cycle_day)).date()\n",
    "        stem = f'{term_code}_{cycle_date}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        return cycle_day, cycle_date, stable_date, stem, term_desc, year\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id,\n",
    "                spriden_pidm as pidm\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self, show=False):\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        counter = 0\n",
    "        divide = False\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > 5:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                current_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    current_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                current_day, current_date, stable_date, stem, term_desc, year = self.get_cycle(term_code, cycle_date=current_date)\n",
    "                def fcn():\n",
    "                    df = (\n",
    "                        self.get_spriden()\n",
    "                        .assign(current_day=current_day, current_date=current_date)\n",
    "                        .merge(book.parse(sheet).prep(), on='id', how='right')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, flags_prc/f'{term_code}/flags_{stem}', read=False, divide=False)[1]:\n",
    "                    divide = True\n",
    "                    counter = 0\n",
    "                    dst = flags_prc/f'flags_{year}.parquet'\n",
    "                    dst.unlink(missing_ok=True)\n",
    "        if divide:\n",
    "            print(divider)\n",
    "            self.combine_flags()\n",
    "\n",
    "\n",
    "    def combine_flags(self, show=False):\n",
    "        def fcn(year):\n",
    "            L = [pd.read_parquet(src) for path in flags_prc.iterdir() if path.is_dir() and str(year) in path.stem for src in path.glob('*.parquet')]\n",
    "            df = pd.concat(L, ignore_index=True).prep()\n",
    "            del L\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                if k in df:\n",
    "                    df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        for year in {int(x.stem)//100 for x in flags_prc.iterdir() if x.is_dir()}:\n",
    "            self.get(fcn, flags_prc/f'flags_{year}', read=False, year=year)\n",
    "\n",
    "\n",
    "    def get_flags(self, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pd.read_parquet(flags_prc/f'flags_{self.year}.parquet')\n",
    "                .query(f\"current_date<='{self.cycle_date}'\")\n",
    "                .sort_values(['pidm','current_date'])\n",
    "                .drop_duplicates(subset=['pidm','term_code'], keep='last')\n",
    "            )\n",
    "            df.loc[~df['state'].isin(self.get_states()),'zip'] = pd.NA\n",
    "            df['zip'] = df['zip'].str.split('-', expand=True)[0].str[:5].to_numeric(errors='coerce')\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'flags', self.combine_flags)\n",
    "        return df\n",
    "##########################################\n",
    "############ get student data ############\n",
    "###############################\n",
    "    def get_students(self, show=False):\n",
    "        def fcn():\n",
    "            df = (self.admissions\n",
    "                  .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                  .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])                \n",
    "            )\n",
    "            mask = df.eval(\"drivetime.isnull() & zip.notnull() & camp_code!='o'\")\n",
    "            if mask.any():\n",
    "                df[mask].set_index(['state','city','zip','camp_code'])[[]].sort_index().reset_index().disp(50)\n",
    "\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            \n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            \n",
    "            \n",
    "            # df['oriented'] = np.where(df['orien_sess'].notnull() | df['registered'].notnull(), 'y', np.where(df['orientation_hold_exists'].notnull(), 'n', 'w'))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "\n",
    "            # df['verified'] = np.where(df['ver_complete'].notnull(), 'y', np.where(df['selected_for_ver'].notnull(), 'n', 'w'))\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            \n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "\n",
    "            df['eager'] = (self.stable_date - df['first_date']).dt.days\n",
    "            df['age'] = (self.stable_date - df['birth_date']).dt.days\n",
    "\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            \n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "\n",
    "\n",
    "\n",
    "            # df['majr_code'] = df['majr_code'].replace({'0000':pd.NA, 'und':pd.NA, 'eled':'eted', 'agri':'unda'})\n",
    "\n",
    "            # df['coll_code'] = df['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "\n",
    "            # df['coll_desc'] = df['coll_code'].map({\n",
    "            #     'an': 'ag & natural_resources',\n",
    "            #     'ba': 'business',\n",
    "            #     'ed': 'education',\n",
    "            #     'en': 'engineering',\n",
    "            #     'hl': 'health sciences',\n",
    "            #     'la': 'liberal & fine arts',\n",
    "            #     'sm': 'science & mathematics',\n",
    "            #     pd.NA: 'no college designated',\n",
    "            # })\n",
    "\n",
    "\n",
    "\n",
    "            # checks = [\n",
    "            #     'cycle_day >= 0',\n",
    "            #     'eager >= cycle_day',\n",
    "            #     'age >= 5000',\n",
    "            #     'distance >= 0',\n",
    "            #     'hs_pctl >=0',\n",
    "            #     'hs_pctl <= 100',\n",
    "            #     'hs_qrtl >= 0',\n",
    "            #     'hs_qrtl <= 4',\n",
    "            #     'act_equiv >= 1',\n",
    "            #     'act_equiv <= 36',\n",
    "            #     'gap_score >= 0',\n",
    "            #     'gap_score <= 100',\n",
    "            # ]\n",
    "            # for check in checks:\n",
    "            #     mask = df.eval(check)\n",
    "            #     assert mask.all(), [check,df[~mask].disp(5)]\n",
    "            mask = df['cycle_date_flags'].isnull()  # rows from admissions not on flags - should not be any\n",
    "            if mask.any():\n",
    "                display(df[mask]['styp_code'].value_counts().sort_index().to_frame().T)\n",
    "            return df.set_index(['pidm'])\n",
    "        df, new = self.get(fcn, 'students', [self.get_admissions,self.get_flags,self.get_drivetimes])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def newest(self, qry, part, tbl='', sel=''):\n",
    "        \"\"\"The OPEIR daily snapshot experienced occasional glitched causing incomplete copies.\n",
    "        Consequently, record can vanished then reappear later. This function fixes this issue.\"\"\"\n",
    "        # A, B = [indent(s.strip()) for s in qry.rsplit('from',1)]\n",
    "        part = join(part, ', ')\n",
    "        if tbl == '':\n",
    "            tbl = qry\n",
    "        if sel != '':\n",
    "            sel = ','+join(sel)\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    {part}\n",
    "    ,current_date\n",
    "    ,min(current_date) over (partition by {part}) as first_date\n",
    "    ,max(current_date) over (partition by {part}) as last_date\n",
    "    ,least(greatest(timestamp('{self.cycle_date}'), min(current_date) over ()), max(current_date) over ()) as cycle_date\n",
    "from\n",
    "    {qry.strip()}\n",
    "qualify\n",
    "    cycle_date between first_date and last_date  -- keep records where cycle_date falls between its first & last appearance (+5 days for safety)\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    *\n",
    "from {subqry(qry)}\n",
    "where\n",
    "    current_date <= '{self.cycle_date}'  -- discard records after cycle_date\n",
    "qualify\n",
    "    row_number() over (partition by {part} order by current_date desc) = 1  -- keep most recent remaining record\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select distinct\n",
    "    pidm\n",
    "    --,{self.cycle_day} as cycle_day\n",
    "    ,cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,last_date\n",
    "    ,{get_desc('term_code')}\n",
    "    ,{get_desc('levl_code')}\n",
    "    ,{get_desc('styp_code')}\n",
    "    ,{get_desc('camp_code')}\n",
    "    ,{get_desc('coll_code_1')}\n",
    "    ,{get_desc('dept_code')}\n",
    "    ,{get_desc('majr_code_1')}\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    {indent(sel)}\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {tbl}\n",
    "using\n",
    "    ({part}, current_date)\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\"\"\"\n",
    "        return qry\n",
    "\n",
    "\n",
    "    def get_admissions(self, show=False):\n",
    "        def fcn():\n",
    "            # tbl = f'dev.opeir.opeiradmissions_{self.term_desc}'\n",
    "            tbl = f'dev.opeir.admissions_{self.term_desc}_v'\n",
    "            qry = self.newest(\n",
    "                part = 'pidm, appl_no',\n",
    "                sel = [\n",
    "                    'appl_no',\n",
    "                    get_desc('apst_code'),\n",
    "                    get_desc('apdc_code'),\n",
    "                    get_desc('admt_code'),\n",
    "                    get_desc('saradap_resd_code'),\n",
    "                    'hs_percentile',\n",
    "                    # 'sbgi_code',\n",
    "                ],\n",
    "                tbl = tbl,\n",
    "                qry = f\"\"\"\n",
    "    {tbl} as A\n",
    "inner join\n",
    "    {catalog}saturnstvapdc as B\n",
    "on\n",
    "    apdc_code = stvapdc_code\n",
    "where\n",
    "    stvapdc_inst_acc_ind is not null  --only accepted\n",
    "\"\"\")\n",
    "            df = run(qry, show)\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'admissions')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_registrations(self, show=False):\n",
    "        def fcn():\n",
    "            tbl = f'dev.opeir.opeirregistration_{self.term_desc}'\n",
    "            if spark.catalog.tableExists(tbl):\n",
    "                qry = self.newest(\n",
    "                    part = [\n",
    "                        'pidm',\n",
    "                        # 'subj_code',\n",
    "                        # 'crse_numb',\n",
    "                        'crn',\n",
    "                    ],\n",
    "                    sel = [\n",
    "                        'crn',\n",
    "                        'subj_code',\n",
    "                        'crse_numb',\n",
    "                        'subj_code || crse_numb as crse_code',\n",
    "                        'credit_hr',\n",
    "                        ],\n",
    "                    qry = tbl,\n",
    "                )\n",
    "                df = run(qry, show)\n",
    "            else:\n",
    "                df = pd.DataFrame(columns=['pidm','crse_code','credit_hr'])\n",
    "            return df#.set_index(['crse_code','pidm'])\n",
    "        df, new = self.get(fcn, 'registrations')\n",
    "        return df\n",
    "\n",
    "self = Term(\n",
    "    cycle_day=0,\n",
    "    term_code=202408,\n",
    "    overwrite=[\n",
    "        # 'terms',\n",
    "        # 'zips',\n",
    "        # 'drivetimes',\n",
    "        # 'flags',\n",
    "        'admissions',\n",
    "        # 'students',\n",
    "        'registrations',\n",
    "    ]\n",
    ")\n",
    "# self.process_flags()\n",
    "# self.combine_flags()\n",
    "# # self.get_zips()\n",
    "# # self.get_drivetimes()\n",
    "# self.get_registrations(show=True)\n",
    "# self.get_admissions(show=True)\n",
    "# # A = self.get_admissions(show=True)\n",
    "# # self.get_students()\n",
    "# self.get_terms().loc[202501]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad00a5c6-ea0d-4668-aed1-1801494cd0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class AMP(Term):\n",
    "    date: str = ''\n",
    "    crse_code: str = '_headcnt'\n",
    "    year: int = 2024\n",
    "    time_budget: int = 60\n",
    "    overwrite: set = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert len(self.date)==5, \"Please specify date using 'mm-dd' format (2 digit month & 2 digit day)\"\n",
    "        self.year = int(self.year)\n",
    "        self.term_code = 100*self.year+8\n",
    "        self.cycle_date = f'{self.year}-{self.date}'\n",
    "        super().__post_init__()\n",
    "        kwargs = {k: self[k] for k in ['cycle_date','term_code','overwrite']}\n",
    "        self.current = Term(**kwargs)\n",
    "        self.summer  = Term(**kwargs | {'term_code': 100*self.year+6})\n",
    "        self.stable  = Term(**kwargs | {'cycle_day': 0})\n",
    "        self.idx = 'styp_code'\n",
    "        self.agg = ['styp_desc','camp_desc']\n",
    "        self.features = {\n",
    "            'act_equiv':pd.NA,\n",
    "            'age':pd.NA,\n",
    "            'camp_desc':'stephenville',\n",
    "            'drivetime':pd.NA,\n",
    "            'eager':pd.NA,\n",
    "            'fafsa': False,\n",
    "            'finaid': False,\n",
    "            'gap_score':0,\n",
    "            'gender':pd.NA,\n",
    "            'hs_qrtl':pd.NA,\n",
    "            'lgcy':False,\n",
    "            'oriented':False,\n",
    "            'race_asian':False,\n",
    "            'race_black':False,\n",
    "            'race_hispanic':False,\n",
    "            'race_native':False,\n",
    "            'race_pacific':False,\n",
    "            'race_white':False,\n",
    "            'schlship':False,\n",
    "            'ssb':False,\n",
    "            'tsi_math':False,\n",
    "            'tsi_reading':False,\n",
    "            'tsi_writing':False,\n",
    "            'verified':False,\n",
    "            'waiver':False,\n",
    "            'credit_hr':0,\n",
    "            'current':False,\n",
    "        }\n",
    "        self.get_raw()\n",
    "        self.get_prepared()\n",
    "        # if self.year < max(years):\n",
    "        #     self.get_multipliers()\n",
    "        #     self.get_models()\n",
    "\n",
    "\n",
    "    def get_raw(self):\n",
    "        def fcn():\n",
    "            df = pd.concat([x.get_admissions() for x in [self.current, self.summer]], ignore_index=True)\n",
    "            mask = df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "            df = (\n",
    "                df\n",
    "                .loc[mask]\n",
    "                .query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    "                .merge(self.current.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                .merge(self.current.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])\n",
    "            )\n",
    "            df.query('current_date_flags.isnull()')['first_date'].value_counts().disp(10)\n",
    "            dups = df.duplicated('pidm', keep=False)\n",
    "            if dups.any():\n",
    "                df[dups].disp(20)\n",
    "                assert 1==2\n",
    "            return df.set_index('pidm').sort_index()\n",
    "        df, new = self.get(fcn, f'raw', [self.current.get_admissions, self.summer.get_admissions, self.current.get_flags, self.current.get_drivetimes])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_prepared(self):\n",
    "        def fcn():\n",
    "            # df = (\n",
    "            #     self.get_raw()\n",
    "            #     .merge(self.current.get_flags(), on='pidm', how='left', suffixes=['', '_flags'])\n",
    "            #     .merge(self.current.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])\n",
    "\n",
    "            #     .fillna(self.features)\n",
    "            #     .prep(category=True)\n",
    "            #     .set_index('pidm')\n",
    "            # )\n",
    "            df = self.get_raw()\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "            df['eager'] = (self.stable_date - df['first_date']).dt.days\n",
    "            df['age'] = (self.stable_date - df['birth_date']).dt.days\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "\n",
    "            for k in ['current','stable']:\n",
    "                df[k] = self[k].get_registrations().groupby('pidm')['credit_hr'].sum()\n",
    "                df[k] = df[k].fillna(0)\n",
    "            df['credit_hr'] = df['current'].copy()\n",
    "            if self.crse_code != '_tot_sch':\n",
    "                if self.crse_code != '_headcnt':\n",
    "                    for k in ['current','stable']:\n",
    "                        df[k] = self[k].get_registrations().query(f\"crse_code=='{self.crse_code}'\").groupby('pidm')['credit_hr'].sum()\n",
    "                for k in ['current','stable']:\n",
    "                    df[k] = df[k].fillna(0)>0\n",
    "\n",
    "            df = (\n",
    "                df\n",
    "                .fillna(self.features)\n",
    "                .prep(category=True)\n",
    "                .set_index(difference(listify(self.idx)+listify(self.agg), self.features), append=True)\n",
    "            )\n",
    "            def fcn1(Z):\n",
    "                y = Z.pop('stable')\n",
    "                imp = mf.ImputationKernel(Z[list(self.features)].reset_index(drop=True), random_state=self.seed)\n",
    "\n",
    "                # imp = mf.ImputationKernel(Z[Z.columns.intersection(self.features)].reset_index(drop=True), random_state=self.seed)\n",
    "                imp.mice(10)\n",
    "                return imp.complete_data().set_index(Z.index), y\n",
    "\n",
    "            return {key: fcn1(Z) for key, Z in df.groupby(self.idx)}\n",
    "        dct, new = self.get(fcn, f'prepared', [\n",
    "            self.get_raw,\n",
    "            self.current.get_flags,\n",
    "            self.current.get_drivetimes,\n",
    "            self.current.get_registrations,\n",
    "            self.stable.get_registrations,\n",
    "            ], suffix='.pkl')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_enrollments(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                self.stable.get_registrations()\n",
    "                .query(f\"levl_code=='ug' & styp_code in ['n','t','r']\")\n",
    "                .join(self.current.get_students().assign(admitted=lambda x: x['current_date'].notnull().prep())['admitted'])\n",
    "                .fillna({'admitted':False})\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, f'enrollments', [self.stable.get_registrations,self.current.get_students])\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def get_multipliers(self):\n",
    "        def fcn():\n",
    "            return 1 / self.get_enrollments().groupby(['crse_code',*listify(self.idx)])['admitted'].mean().rename('mlt')\n",
    "        df, new = self.get(fcn, f'multipliers', self.get_enrollments)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_models(self):\n",
    "        def fcn():\n",
    "            def fcn1(Z):\n",
    "                dct = {\n",
    "                    'time_budget':self.time_budget,\n",
    "                    'task':'classification',\n",
    "                    'verbose':0,\n",
    "                    'metric':'log_loss',\n",
    "                    'eval_method':'cv',\n",
    "                    'n_splits':3,\n",
    "                    'seed':self.seed,\n",
    "                    # 'early_stop':True,\n",
    "                    'estimator_list': ['xgboost','lgbm','rf'],\n",
    "                }\n",
    "                clf = fl.AutoML(**dct)\n",
    "                clf.fit(*Z, **dct)\n",
    "                # y = X[[]].join(self.stable.get_registrations().loc[self.crse_code]['credit_hr'].rename('stable')>0).fillna(False)\n",
    "                # y = X[[]].join(self.stable.get_registrations().loc[self.crse_code]['credit_hr'].rename('stable')).fillna(0)>0\n",
    "                # y = X.join(self.stable.get_registrations().loc[self.crse_code])['credit_hr'].fillna(0)>0\n",
    "                \n",
    "                # ['credit_hr'].rename('stable')).fillna(0)>0\n",
    "                # print(type(y))\n",
    "                # y.disp(1)\n",
    "                # clf.fit(X, y, **dct)\n",
    "                return clf\n",
    "            return {key: fcn1(Z) for key, Z in self.get_prepared().items()}\n",
    "        clf, new = self.get(fcn, f'models', self.get_enrollments, suffix='.pickle')\n",
    "        return clf\n",
    "\n",
    "\n",
    "    def get_predictions(self, learners=dict()):\n",
    "        def fcn():\n",
    "            dct = {\n",
    "                'crse_code': self.crse_code,\n",
    "                'prediction_year': self.year,\n",
    "                'learner_year': learner.year,\n",
    "            }\n",
    "\n",
    "            L = [\n",
    "                clf.prediction(*self.get_prepared()[key])\n",
    "                .assign(**dct, mlt=learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "                .set_index(list(dct), append=True)\n",
    "\n",
    "                # .set_index(list(dct.keys()))\n",
    "\n",
    "                #  .reset_index()\n",
    "                # .inser('mlt', learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "                # .inser('learner_year', learner.year)\n",
    "                # .inser('prediction_year', self.year)\n",
    "                # .inser('crse_code', self.crse_code)\n",
    "                for year, learner in learners.items() if 'models' in learner\n",
    "                for key, clf in learner.get_models().items()]\n",
    "            return pd.concat(L) if len(L)>0 else pd.DataFrame()\n",
    "        df, new = self.get(fcn, f'predictions', self.get_prepared)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_forecasts(self):\n",
    "        def fcn():\n",
    "            return {key:\n",
    "                self.get_predictions()\n",
    "                .assign(forecast=lambda Z: Z['prediction']*Z['mlt'])\n",
    "                .groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key))\n",
    "                [['forecast']].sum()\n",
    "                for key in self.agg}\n",
    "            # def fcn1(Z):\n",
    "            #     return (Z['prediction']*Z['mlt']).sum()\n",
    "            # df = self.get_predictions().copy()\n",
    "            # df['prediction'] *= df['mlt']\n",
    "            # return df.groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key))['prediction'].sum()\n",
    "            # return self.get_predictions().groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key)).apply(fcn1)\n",
    "        dct, new = self.get(fcn, f'forecasts', suffix='pickle')\n",
    "        return dct\n",
    "\n",
    "\n",
    "# years = [2022,2023,2024,2025]\n",
    "years = [2025,2024]\n",
    "years = [2025]\n",
    "years = listify(years)\n",
    "amps = {year: AMP(\n",
    "        date='03-28',\n",
    "        crse_code='_headcnt',\n",
    "        time_budget=10,\n",
    "        year=year,\n",
    "        overwrite={\n",
    "            # 'flags',\n",
    "            # 'admissions',\n",
    "            # 'students',\n",
    "            # 'registrations',\n",
    "            # 'raw',\n",
    "            'prepared',\n",
    "            'enrollments',\n",
    "            'multipliers',\n",
    "            'models',\n",
    "            'predictions',\n",
    "            'forecasts',\n",
    "            },\n",
    "    ) for year in years}\n",
    "\n",
    "# for self in amps.values():\n",
    "#     self.get_predictions(amps)\n",
    "\n",
    "\n",
    "# F = pd.concat([self.forecasts for self in amp.values()]).sort_index(ascending=[True, False,False])\n",
    "# F.to_csv(data/f'forecast\n",
    "\n",
    "# self = amps[2024]\n",
    "# self.get_predictions()\n",
    "# self.get_forecasts()\n",
    "# self.get_admissions()\n",
    "# self.current.get_registrations(show=True)\n",
    "# self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097fa422-fcd5-4394-92d5-8a2fb82eac02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "self.prepared['n'][0]\n",
    "# df = self.current.flags.copy()#.reset_index()\n",
    "# df = self.raw.copy().reset_index()\n",
    "# # df.groupby('pidm').size().sort_values()\n",
    "# df\n",
    "# df[df.duplicated('pidm', keep=False)].sort_values('pidm').disp(2,sort=True)\n",
    "# df.query('current_date_flags.isnull()')['first_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb94ea4-3846-4da1-b585-cad7f9c9d275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "df = self.raw.copy()#.reset_index()\n",
    "A = self.current.get_admissions().set_index('pidm')\n",
    "self.crse_code = 'math1314'\n",
    "for k in ['current','stable']:\n",
    "    A[k] = self[k].get_registrations().groupby('pidm')['credit_hr'].sum()\n",
    "    A[k] = A[k].fillna(0)\n",
    "A.insert(A.shape[1]-2, 'credit_hr', A['current'].copy())\n",
    "\n",
    "if self.crse_code != '_tot_sch':\n",
    "    if self.crse_code != '_headcnt':\n",
    "        for k in ['current','stable']:\n",
    "            A[k] = self[k].get_registrations().query(f\"crse_code=='{self.crse_code}'\").groupby('pidm')['credit_hr'].sum()\n",
    "    for k in ['current','stable']:\n",
    "        A[k] = A[k].fillna(0)>0\n",
    "A['credit_hr'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df451569-7de4-4d37-9b33-2839983e2a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "df = self.raw.copy()#.reset_index()\n",
    "A = self.current.get_admissions().set_index('pidm')\n",
    "R = self.stable.get_registrations()\n",
    "self.crse_code = 'math1314'\n",
    "# f = lambda X, c: X.query(f\"crse_code=='{c}'\")\n",
    "# g = lambda X: X.groupby('pidm')['credit_hr'].sum()\n",
    "\n",
    "for k in ['current','stable']:\n",
    "    A[k] = self[k].get_registrations().groupby('pidm')['credit_hr'].sum().fillna(0)\n",
    "A['credit_hr'] = A['current'].copy()\n",
    "\n",
    "# if self.crse_code != '_tot_sch':\n",
    "#     if self.crse_code != '_headcnt':\n",
    "#         for k in ['current','stable']:\n",
    "#             A[k] = self[k].get_registrations().query(f\"crse_code=='{c}'\").groupby('pidm')['credit_hr'].sum().fillna(0)\n",
    "#     for k in ['current','stable']:\n",
    "#         A[k] = A[k]>0\n",
    "A\n",
    "#         A['stable' ] = g(f(self.stable.get_registrations() , self.crse_code))\n",
    "\n",
    "\n",
    "\n",
    "# A['credit_hr'] = g(self.current.get_registrations()).fillna(0)\n",
    "# A['current'] = A['credit_hr' ]\n",
    "# A['stable' ] = g(self.stable.get_registrations()).fillna(0)\n",
    "# if self.crse_code != '_tot_sch':\n",
    "#     if self.crse_code != '_headcnt':\n",
    "#         A['current'] = g(f(self.current.get_registrations(), self.crse_code))\n",
    "#         A['stable' ] = g(f(self.stable.get_registrations() , self.crse_code))\n",
    "    \n",
    "\n",
    "\n",
    "# elif self.crse_code == '_headcnt':\n",
    "#     A['current']\n",
    "\n",
    "# A['current'] = g(self.current.get_registrations())\n",
    "# A['stable' ] = g(self.stable.get_registrations())\n",
    "\n",
    "\n",
    "\n",
    "# if self.crse_code == '_tot_sch':\n",
    "#     A['current'] = g(self.current.get_registrations())\n",
    "#     A['stable' ] = g(self.stable.get_registrations())\n",
    "# elif self.crse_code == '_headcnt':\n",
    "#     A['current'] = g(self.current.get_registrations())>0\n",
    "#     A['stable' ] = g(self.stable.get_registrations())>0\n",
    "\n",
    "\n",
    "# A['current'] = g(f(self.current.get_registrations(), self.crse_code))>0\n",
    "# A['stable' ] = g(f(self.stable.get_registrations() , self.crse_code))>0\n",
    "# A\n",
    "# # c = g(R.query(f\"crse_code=='{self.crse_code}'\"))>0\n",
    "# # A['current'] = c\n",
    "\n",
    "# # # # # A['current'] = (g(A.query(f\"crse_code=='{self.crse_code}'\"))>0).fillna(False)\n",
    "# # A['current'].fillna(False).sum()\n",
    "\n",
    "# # A\n",
    "# # B = A.groupby(['crse_code','pidm'])['credit_hr'].sum()\n",
    "# # B.sort_values()\n",
    "# # A.query('pidm==1092371')\n",
    "# # B = pd.concat([A,A.assign(crse_code='_tot_sch')])\n",
    "# # B\n",
    "# # .set_index(['crse_code','pidm'])['credit_hr']#.query(f\"crse_code=='_tot_sch'\")\n",
    "\n",
    "# # R\n",
    "# # df\n",
    "# #   .merge(self.current.get_registrations().query(f\"crse_code=='_tot_sch'\")[['pidm','credit_hr']], on='pidm', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23c93bb-0478-4119-a07b-c84d8f393b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import display, HTML, clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming openpyxl numpy pandas geopandas pgeocode flaml[automl] git+https://github.com/AnotherSamWilson/miceforest.git\n",
    "    dbutils.library.restartPython()\n",
    "    clear_output()\n",
    "    dbutils.notebook.exit('Rerun to use newly installed/updated packages')\n",
    "\n",
    "import pathlib, shutil, pickle, warnings, requests, dataclasses, codetiming, numpy as np, pandas as pd, geopandas as gpd, pgeocode, flaml as fl, miceforest as mf\n",
    "clear_output()\n",
    "catalog = 'dev.bronze.'\n",
    "root = pathlib.Path(f'/Workspace/Users/{username}@tarleton.edu/admitted_matriculation_predictor_2025/')\n",
    "data = root/'data'\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "for w in [\n",
    "    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "    # \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "    # \"errors='ignore' is deprecated\"\n",
    "    # \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "    # \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "    # \"DataFrame is highly fragmented\",\n",
    "    # \"DataFrameGroupBy.apply operated on the grouping columns\",\n",
    "    ]:\n",
    "    warnings.filterwarnings(action='ignore', message=f\".*{w}.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "##########################################\n",
    "############ helper functions ############\n",
    "##########################################\n",
    "tab = '    '\n",
    "divider = '\\n##############################################################################################################\\n'\n",
    "\n",
    "def listify(*args):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        return list(*args)\n",
    "    except Exception as e:\n",
    "        return list(args)\n",
    "\n",
    "def setify(*args):\n",
    "    \"\"\"ensure it is a set\"\"\"\n",
    "    return set(listify(*args))\n",
    "\n",
    "def unique(*args):\n",
    "    \"\"\"get unique items maintaining order\"\"\"\n",
    "    return listify(dict.fromkeys(listify(*args)))\n",
    "\n",
    "def difference(A, B):\n",
    "    return unique([x for x in listify(A) if x not in listify(B)])\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    return '(' + indent('\\n'+qry.strip()+'\\n)', lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=42):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep().sort_index()\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "def get_desc(code):\n",
    "    for nm in code.split('_'):\n",
    "        if len(nm) == 4:\n",
    "            break\n",
    "    return f'{code} as {nm}_code, (select stv{nm}_desc from {catalog}saturnstv{nm} where {code} = stv{nm}_code limit 1) as {nm}_desc'\n",
    "\n",
    "def coalesce(x, y=False):\n",
    "    return f'coalesce({x}, {y}) as {x}'\n",
    "\n",
    "############ pandas functions ############\n",
    "pd.options.display.max_columns = None\n",
    "def disp(df, rows=4, head=True, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    with pd.option_context('display.min_rows', rows, 'display.max_rows', rows):\n",
    "        print(df.shape)\n",
    "        df = df.sort_index(axis=1) if sort else df\n",
    "        missing = df.isnull().sum().to_frame().T\n",
    "        X = pd.concat([df.dtypes.to_frame().T, missing, (missing/df.shape[0]*100).round(2), df.head(rows) if head else df.tails(rows)])\n",
    "        display(HTML(X.to_html()))\n",
    "\n",
    "def inser(df, column, value, loc=0):\n",
    "    df.insert(loc, column, value)\n",
    "    return df\n",
    "\n",
    "def to_numeric(df, downcast='integer', errors='ignore', **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: s.astype('string').str.lower().str.strip() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "        idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "        return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    return wrapper\n",
    "\n",
    "for f in [disp, inser, to_numeric, prep]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setattr(pd.DataFrame, f.__name__, f)\n",
    "    setattr(pd.Series, f.__name__, wrap(f))\n",
    "\n",
    "def prediction(clf, X, y):\n",
    "    Z = X.copy()\n",
    "    Z['prediction'] = clf.predict_proba(X)[:,1]\n",
    "    Z['actual'] = y\n",
    "    Z['mae'] = np.abs(Z['prediction'] - Z['actual'])\n",
    "    Z['log_loss'] = -1*(Z['actual']*np.log(Z['prediction']) + (1-Z['actual'])*np.log(1-Z['prediction']))\n",
    "    return Z\n",
    "\n",
    "for f in [prediction]:\n",
    "    setattr(fl.automl.automl.AutoML, f.__name__, f)\n",
    "\n",
    "#########################################\n",
    "################## AMP ##################\n",
    "#########################################\n",
    "@dataclasses.dataclass\n",
    "class Term():\n",
    "    term_code: int = 202408\n",
    "    cycle_day: int = None\n",
    "    cycle_date: str = None\n",
    "    overwrite: set = None\n",
    "    seed: int = 42\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.term_code = int(self.term_code)\n",
    "        self.cycle_day, self.cycle_date, self.stable_date, self.stem, self.term_desc, self.year = self.get_cycle(self.term_code, self.cycle_day, self.cycle_date)\n",
    "        self.overwrite = setify(self.overwrite)\n",
    "        # Because these take about 1 hour each, we force user to manually delete parquet files to avoid accidental deletion requring lengthy re-creation\n",
    "        self.overwrite.discard('drivetimes_s')\n",
    "        self.overwrite.discard('drivetimes_m')\n",
    "        self.overwrite.discard('drivetimes_w')\n",
    "        self.overwrite.discard('drivetimes_r')\n",
    "        self.overwrite.discard('drivetimes_l')\n",
    "\n",
    "\n",
    "    def get(self, fcn, dst, prereq=[], *, divide=True, read=True, suffix='.parquet', **kwargs):\n",
    "        nm = str(dst)\n",
    "        if '/' in nm:\n",
    "            dst = pathlib.Path(dst).with_suffix(suffix)\n",
    "            nm = dst.stem\n",
    "        else:\n",
    "            dst = data/f'{dst}/{self.term_code}/{dst}_{self.stem}{suffix}'\n",
    "\n",
    "        if nm in self.overwrite:\n",
    "            del self[nm]\n",
    "            dst.unlink(missing_ok=True)\n",
    "            self.overwrite.remove(nm)\n",
    "\n",
    "        new = False\n",
    "        if not nm in self:\n",
    "            if not dst.exists():\n",
    "                new = True\n",
    "                for f in listify(prereq):\n",
    "                    f()\n",
    "                print(f'creating {dst.name}: ', end='')\n",
    "                with codetiming.Timer():\n",
    "                    rslt = fcn(**kwargs)\n",
    "                    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    if suffix == '.parquet':\n",
    "                        pd.DataFrame(rslt).prep().to_parquet(dst)  # forced to wrap with explicit pd.DataFrame to due strange error under pandas 2.2.3 \"Object of type PlanMetrics is not JSON serializable\" with to_parquet\n",
    "                    elif suffix == 'csv':\n",
    "                        pd.DataFrame(rslt).prep().to_csv(dst)\n",
    "                    else:\n",
    "                        with open(dst, 'wb') as f:\n",
    "                            pickle.dump(rslt, f, pickle.HIGHEST_PROTOCOL)\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "            if read:\n",
    "                if suffix == '.parquet':\n",
    "                    self[nm] = pd.read_parquet(dst)\n",
    "                elif suffix == 'csv':\n",
    "                    self[nm] = pd.read_csv(dst)\n",
    "                else:\n",
    "                    with open(dst, 'rb') as f:\n",
    "                        self[nm] = pickle.load(f)\n",
    "            else:\n",
    "                self[nm] = None\n",
    "        return self[nm], new\n",
    "##################################################\n",
    "################# get drivetimes #################\n",
    "##################################################\n",
    "    def get_zips(self, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pgeocode.Nominatim('us')._data  # get all zips\n",
    "                .prep()\n",
    "                .rename(columns={'postal_code':'zip'})\n",
    "                .query(\"state_code.notnull() & state_code not in [None,'mh']\")\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/zips')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_states(self, show=False):\n",
    "        return set(self.get_zips()['state_code'])\n",
    "\n",
    "\n",
    "    def get_drivetimes(self, show=False):\n",
    "        def fcn():\n",
    "            from pgeocode import Nominatim\n",
    "            from sklearn.metrics.pairwise import haversine_distances\n",
    "            print()\n",
    "            campus_coords = {\n",
    "                's': '-98.215784,32.216217',\n",
    "                'm': '-97.432975,32.582436',\n",
    "                'w': '-97.172176,31.587908',\n",
    "                'r': '-96.467920,30.642055',\n",
    "                'l': '-96.983211,32.462267',\n",
    "                }\n",
    "            url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "            gdf = gpd.read_file(url).prep().set_index('zcta5ce20')  # get all ZCTA https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n",
    "            pts = gdf.sample_points(size=10, method=\"uniform\").explode().apply(lambda g: f\"{g.x},{g.y}\")  # sample 10 random points in each ZCTA\n",
    "            M = []\n",
    "            for k, v in campus_coords.items():\n",
    "                def fcn1():\n",
    "                    print()\n",
    "                    L = []\n",
    "                    i = 0\n",
    "                    di = 200\n",
    "                    I = pts.shape[0]\n",
    "                    while i < I:\n",
    "                        u = join([v, *pts.iloc[i:i+di]],';')\n",
    "                        url = f\"http://router.project-osrm.org/table/v1/driving/{u}?sources={0}&annotations=duration,distance&fallback_speed=1&fallback_coordinate=snapped\"\n",
    "                        response = requests.get(url).json()\n",
    "                        L.append(np.squeeze(response['durations'])[1:]/60)\n",
    "                        i += di\n",
    "                        print(k,i,round(i/I*100))\n",
    "                    df = pts.to_frame()[[]]\n",
    "                    df[k] = np.concatenate(L)\n",
    "                    return df\n",
    "                df, new = self.get(fcn1, root/f'geo/drivetimes_{k}')\n",
    "                M.append(df)\n",
    "            D = pd.concat(M, axis=1).groupby(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "\n",
    "            # There are a few USPS zips without equivalent ZCTA, so we assign them drivetimes for the nearest\n",
    "            Z = self.get_zips().merge(D.query(\"camp_code=='s'\"), how='left').set_index('zip')\n",
    "            mask = Z['drivetime'].isnull()  # zips without a ZTCA\n",
    "            Z = Z[['latitude','longitude']]\n",
    "            X = np.radians(Z[~mask])\n",
    "            Y = np.radians(Z[mask])\n",
    "            M = (\n",
    "                pd.DataFrame(haversine_distances(X, Y), index=X.index, columns=Y.index) # haversine distance between pairs with and without ZCTA\n",
    "                .idxmin()  # find nearest ZCTA\n",
    "                .reset_index()\n",
    "                .set_axis(['new_zip','zip'], axis=1)\n",
    "                .prep()\n",
    "                .merge(D)  # merge the drivetimes for that ZCTA\n",
    "                .drop(columns='zip')\n",
    "                .rename(columns={'new_zip':'zip'})\n",
    "            )\n",
    "            df = pd.concat([D,M], ignore_index=True)\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/drivetimes', self.get_zips)\n",
    "        return df\n",
    "\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, show=False):\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        df, new = self.get(fcn, data/'terms')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_cycle(self, term_code, cycle_day=None, cycle_date=None, show=False):\n",
    "        term_code = int(term_code)\n",
    "        year = term_code // 100\n",
    "        term_desc, stable_date = self.get_terms().loc[term_code,['term_desc','stable_date']]\n",
    "        if cycle_day is None:\n",
    "            if cycle_date is None:\n",
    "                cycle_date = pd.Timestamp.now()\n",
    "            cycle_date = min(pd.to_datetime(cycle_date), pd.Timestamp.now()).normalize()\n",
    "            cycle_day = (stable_date - cycle_date).days\n",
    "        cycle_date = (stable_date - pd.Timedelta(days=cycle_day)).date()\n",
    "        stem = f'{term_code}_{cycle_date}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        return cycle_day, cycle_date, stable_date, stem, term_desc, year\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id,\n",
    "                spriden_pidm as pidm\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self, show=False):\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        counter = 0\n",
    "        divide = False\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > 5:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                current_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    current_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                current_day, current_date, stable_date, stem, term_desc, year = self.get_cycle(term_code, cycle_date=current_date)\n",
    "                def fcn():\n",
    "                    df = (\n",
    "                        self.get_spriden()\n",
    "                        .assign(current_day=current_day, current_date=current_date)\n",
    "                        .merge(book.parse(sheet).prep(), on='id', how='right')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, flags_prc/f'{term_code}/flags_{stem}', read=False, divide=False)[1]:\n",
    "                    divide = True\n",
    "                    counter = 0\n",
    "                    dst = flags_prc/f'flags_{year}.parquet'\n",
    "                    dst.unlink(missing_ok=True)\n",
    "        if divide:\n",
    "            print(divider)\n",
    "            self.combine_flags()\n",
    "\n",
    "\n",
    "    def combine_flags(self, show=False):\n",
    "        def fcn(year):\n",
    "            L = [pd.read_parquet(src) for path in flags_prc.iterdir() if path.is_dir() and str(year) in path.stem for src in path.glob('*.parquet')]\n",
    "            df = pd.concat(L, ignore_index=True).prep()\n",
    "            del L\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                if k in df:\n",
    "                    df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        for year in {int(x.stem)//100 for x in flags_prc.iterdir() if x.is_dir()}:\n",
    "            self.get(fcn, flags_prc/f'flags_{year}', read=False, year=year)\n",
    "\n",
    "\n",
    "    def get_flags(self, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pd.read_parquet(flags_prc/f'flags_{self.year}.parquet')\n",
    "                .query(f\"current_date<='{self.cycle_date}'\")\n",
    "                .sort_values(['pidm','current_date'])\n",
    "                .drop_duplicates(subset=['pidm','term_code'], keep='last')\n",
    "            )\n",
    "            df.loc[~df['state'].isin(self.get_states()),'zip'] = pd.NA\n",
    "            df['zip'] = df['zip'].str.split('-', expand=True)[0].str[:5].to_numeric(errors='coerce')\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'flags', self.combine_flags)\n",
    "        return df\n",
    "##########################################\n",
    "############ get student data ############\n",
    "###############################\n",
    "    def get_students(self, show=False):\n",
    "        def fcn():\n",
    "            df = (self.admissions\n",
    "                  .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                  .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])                \n",
    "            )\n",
    "            mask = df.eval(\"drivetime.isnull() & zip.notnull() & camp_code!='o'\")\n",
    "            if mask.any():\n",
    "                df[mask].set_index(['state','city','zip','camp_code'])[[]].sort_index().reset_index().disp(50)\n",
    "\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            \n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            \n",
    "            \n",
    "            # df['oriented'] = np.where(df['orien_sess'].notnull() | df['registered'].notnull(), 'y', np.where(df['orientation_hold_exists'].notnull(), 'n', 'w'))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "\n",
    "            # df['verified'] = np.where(df['ver_complete'].notnull(), 'y', np.where(df['selected_for_ver'].notnull(), 'n', 'w'))\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            \n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "\n",
    "            df['eager'] = (self.stable_date - df['first_date']).dt.days\n",
    "            df['age'] = (self.stable_date - df['birth_date']).dt.days\n",
    "\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            \n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "\n",
    "\n",
    "\n",
    "            # df['majr_code'] = df['majr_code'].replace({'0000':pd.NA, 'und':pd.NA, 'eled':'eted', 'agri':'unda'})\n",
    "\n",
    "            # df['coll_code'] = df['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "\n",
    "            # df['coll_desc'] = df['coll_code'].map({\n",
    "            #     'an': 'ag & natural_resources',\n",
    "            #     'ba': 'business',\n",
    "            #     'ed': 'education',\n",
    "            #     'en': 'engineering',\n",
    "            #     'hl': 'health sciences',\n",
    "            #     'la': 'liberal & fine arts',\n",
    "            #     'sm': 'science & mathematics',\n",
    "            #     pd.NA: 'no college designated',\n",
    "            # })\n",
    "\n",
    "\n",
    "\n",
    "            # checks = [\n",
    "            #     'cycle_day >= 0',\n",
    "            #     'eager >= cycle_day',\n",
    "            #     'age >= 5000',\n",
    "            #     'distance >= 0',\n",
    "            #     'hs_pctl >=0',\n",
    "            #     'hs_pctl <= 100',\n",
    "            #     'hs_qrtl >= 0',\n",
    "            #     'hs_qrtl <= 4',\n",
    "            #     'act_equiv >= 1',\n",
    "            #     'act_equiv <= 36',\n",
    "            #     'gap_score >= 0',\n",
    "            #     'gap_score <= 100',\n",
    "            # ]\n",
    "            # for check in checks:\n",
    "            #     mask = df.eval(check)\n",
    "            #     assert mask.all(), [check,df[~mask].disp(5)]\n",
    "            mask = df['cycle_date_flags'].isnull()  # rows from admissions not on flags - should not be any\n",
    "            if mask.any():\n",
    "                display(df[mask]['styp_code'].value_counts().sort_index().to_frame().T)\n",
    "            return df.set_index(['pidm'])\n",
    "        df, new = self.get(fcn, 'students', [self.get_admissions,self.get_flags,self.get_drivetimes])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def newest(self, qry, part, sel='0 as temp'):\n",
    "        \"\"\"The OPEIR daily snapshot experienced occasional glitched causing incomplete copies.\n",
    "        Consequently, record can vanished then reappear later. This function fixes this issue.\"\"\"\n",
    "        A, B = [indent(s.strip()) for s in qry.rsplit('from',1)]\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    *\n",
    "from (\n",
    "    {A}\n",
    "        ,min(current_date) over (partition by {part}) as first_date\n",
    "        ,max(current_date) over (partition by {part}) as last_date\n",
    "        ,least(greatest(timestamp('{self.cycle_date}'), min(current_date) over ()), max(current_date) over ()) as cycle_date\n",
    "    from\n",
    "        {B}\n",
    "    qualify\n",
    "        cycle_date between first_date and last_date  -- keep records where cycle_date falls between its first & last appearance (+5 days for safety)\n",
    "    )\n",
    "where\n",
    "    current_date <= '{self.cycle_date}'  -- discard records after cycle_date\n",
    "qualify\n",
    "    row_number() over (partition by {part} order by current_date desc) = 1  -- keep most recent remaining record\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    pidm\n",
    "    --,{self.cycle_day} as cycle_day\n",
    "    ,cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,last_date\n",
    "    ,{get_desc('term_code')}\n",
    "    ,{get_desc('levl_code')}\n",
    "    ,{get_desc('styp_code')}\n",
    "    ,{get_desc('camp_code')}\n",
    "    ,{get_desc('coll_code_1')}\n",
    "    ,{get_desc('dept_code')}\n",
    "    ,{get_desc('majr_code_1')}\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    ,{indent(join(sel))}\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\"\"\"\n",
    "        return qry\n",
    "\n",
    "\n",
    "    def get_admissions(self, show=False):\n",
    "        def fcn():\n",
    "            qry = self.newest(\n",
    "                part = 'pidm, appl_no',\n",
    "                sel = [\n",
    "                    \n",
    "                    'appl_no',\n",
    "                    get_desc('apst_code'),\n",
    "                    get_desc('apdc_code'),\n",
    "                    get_desc('admt_code'),\n",
    "                    get_desc('saradap_resd_code'),\n",
    "                    'hs_percentile',\n",
    "                    # 'sbgi_code',\n",
    "                ],\n",
    "                qry = f\"\"\"\n",
    "select distinct\n",
    "    A.*\n",
    "from\n",
    "    --dev.opeir.opeiradmissions_{self.term_desc} as A\n",
    "    dev.opeir.admissions_{self.term_desc}_v as A\n",
    "inner join\n",
    "    {catalog}saturnstvapdc as B\n",
    "on\n",
    "    apdc_code = stvapdc_code\n",
    "where\n",
    "    stvapdc_inst_acc_ind is not null  --only accepted\n",
    "\"\"\")\n",
    "            df = run(qry, show)\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'admissions')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_registrations(self, show=False):\n",
    "        def fcn():\n",
    "            tbl = f'dev.opeir.opeirregistration_{self.term_desc}'\n",
    "            if spark.catalog.tableExists(tbl):\n",
    "                grp = 'pidm, term_code, levl_code, styp_code'\n",
    "                qry = self.newest(\n",
    "                    part = 'pidm, subj_code, crse_numb',\n",
    "                    sel = [\n",
    "                        'subj_code || crse_numb as crse_code',\n",
    "                        'crn',\n",
    "                        'credit_hr',\n",
    "                        ],\n",
    "                    qry=f\"\"\"\n",
    "select distinct\n",
    "    A.*\n",
    "from\n",
    "    {tbl} as A\n",
    "\"\"\")\n",
    "                df = run(qry, show)\n",
    "            else:\n",
    "                df = pd.DataFrame(columns=['pidm','crse_code','credit_hr'])\n",
    "            return df#.set_index(['crse_code','pidm'])\n",
    "        df, new = self.get(fcn, 'registrations')\n",
    "        return df\n",
    "\n",
    "self = Term(\n",
    "    cycle_day=0,\n",
    "    term_code=202408,\n",
    "    overwrite=[\n",
    "        # 'terms',\n",
    "        # 'zips',\n",
    "        # 'drivetimes',\n",
    "        # 'flags',\n",
    "        # 'admissions',\n",
    "        # 'students',\n",
    "        # 'registrations',\n",
    "    ]\n",
    ")\n",
    "# self.process_flags()\n",
    "# self.combine_flags()\n",
    "# # self.get_zips()\n",
    "# # self.get_drivetimes()\n",
    "# # self.get_registrations(show=True)\n",
    "# A = self.get_admissions(show=True)\n",
    "# # A = self.get_admissions(show=True)\n",
    "# # self.get_students()\n",
    "# self.get_terms().loc[202501]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d46765c-85ca-47a7-9151-1a82e7ce7253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "self.prepared.copy()['credit_hr'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61d2ac4-aa60-4f96-81cc-5bf9417b5fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "df = self.raw.copy()#.reset_index()\n",
    "mask = df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "df.loc[mask]\n",
    "# df['mask'] = df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "# df[df.pop('mask')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0479e156-0514-438e-844e-3bab986189e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fbfa851-482d-4c55-bc26-94df03647eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# amps[2024].current.get_registrations()\n",
    "self = amps[2024]\n",
    "df = (self.get_raw()\n",
    "        .merge(self.current.get_flags(), on='pidm', how='left', suffixes=['', '_flags'])\n",
    "        .merge(self.current.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])\n",
    "        .merge(self.current.get_registrations().query(f\"crse_code=='_tot_sch'\")[['pidm','credit_hr']], on='pidm', how='left')\n",
    "        .query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    ")\n",
    "df.query(\"current_date_flags.isnull()\").groupby('first_date').size()#.sort_values('first_date')#.groupby('styp_code').size()#.sort_values('first_date')#.disp(3, sort=True)#].value_counts()#.groupby('styp_code').size()\n",
    "df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3b1bd8-c447-424f-901d-5630c9a2ca5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select\n",
    "    spriden_id as id\n",
    "    ,A.* except (apdc_code)\n",
    "    ,{get_desc('apdc_code')}\n",
    "from\n",
    "    dev.opeir.admissions_fall2025_v as A\n",
    "left join\n",
    "    {catalog}saturnspriden as B\n",
    "on\n",
    "    A.pidm = B.spriden_pidm\n",
    "where\n",
    "    pidm = 1174806\n",
    "order by\n",
    "    current_date desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "run(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1287cae-d62f-4004-ac36-7c894bb9a528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.current.get_flags().query('pidm==1174806')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c603c42-069e-418f-bcd0-04cf252888fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "F = pd.read_parquet('/Volumes/aiml/flags/flags_volume/flags_2025.parquet')\n",
    "F.query('pidm==1174806')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38347c2a-b415-48ff-8f29-8e3c61c0919d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for path in sorted(flags_prc.iterdir()):\n",
    "    if path.is_dir():\n",
    "        for src in sorted(path.iterdir()):\n",
    "            df = pd.read_parquet(src)\n",
    "            if 'cycle_date' in df:\n",
    "                print(src)\n",
    "                # src.unlink()\n",
    "                df = df.rename(columns=lambda x:x.replace('cycle','current'))\n",
    "                df.to_parquet(src)\n",
    "                # assert 1==2\n",
    "            # del df\n",
    "            # df = pd.read_parquet(src)\n",
    "            # display(df)\n",
    "            # k += 1\n",
    "            # if k > 3:\n",
    "            #     assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9747f8c2-fa3f-4160-969b-bffe678de7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pd.read_parquet(flags_prc/f'flags_2024.parquet')\n",
    "pd.read_parquet('/Volumes/aiml/flags/flags_volume/202408/flags_202408_2023-07-07_+441.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b874948d-7c42-4a74-b804-a2ec2ca66b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "df = pd.concat([x.get_admissions() for x in [self.current, self.summer]], ignore_index=True)\n",
    "mask = df.groupby('pidm', group_keys=False, sort=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))#.sort_index()\n",
    "# df\n",
    "# return df[df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9595148a-da35-407e-8493-2377ffffd24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['mask'] = df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "df = df[df.pop('mask')]\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db47aac5-cabc-4a78-9bdc-7d8e307e6209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mask = df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "df = df.loc[df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))]\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72d9a4d-56a6-4bbb-ab65-d38ada21a51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['pidm'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c6dfe7-4277-479b-bd91-afb0af3684e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[df.pop('mask')].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b586850e-1149-4b4c-a1d2-3523f8f2feca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import display, HTML, clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming openpyxl numpy pandas geopandas pgeocode flaml[automl] git+https://github.com/AnotherSamWilson/miceforest.git\n",
    "    dbutils.library.restartPython()\n",
    "    clear_output()\n",
    "    dbutils.notebook.exit('Rerun to use newly installed/updated packages')\n",
    "\n",
    "import pathlib, shutil, pickle, warnings, requests, dataclasses, codetiming, numpy as np, pandas as pd, geopandas as gpd, pgeocode, flaml as fl, miceforest as mf\n",
    "clear_output()\n",
    "catalog = 'dev.bronze.'\n",
    "root = pathlib.Path(f'/Workspace/Users/{username}@tarleton.edu/admitted_matriculation_predictor_2025/')\n",
    "data = root/'data'\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "for w in [\n",
    "    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "    \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "    \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "    \"errors='ignore' is deprecated\"\n",
    "    \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "    \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "    \"DataFrame is highly fragmented\",\n",
    "    \"DataFrameGroupBy.apply operated on the grouping columns\",\n",
    "    ]:\n",
    "    warnings.filterwarnings(action='ignore', message=f\".*{w}.*\", append=True)\n",
    "\n",
    "##########################################\n",
    "############ helper functions ############\n",
    "##########################################\n",
    "tab = '    '\n",
    "divider = '\\n##############################################################################################################\\n'\n",
    "\n",
    "def listify(*args):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        return list(*args)\n",
    "    except Exception as e:\n",
    "        return list(args)\n",
    "\n",
    "def setify(*args):\n",
    "    \"\"\"ensure it is a set\"\"\"\n",
    "    return set(listify(*args))\n",
    "\n",
    "def unique(*args):\n",
    "    \"\"\"get unique items maintaining order\"\"\"\n",
    "    return listify(dict.fromkeys(listify(*args)))\n",
    "\n",
    "def difference(A, B):\n",
    "    return unique([x for x in listify(A) if x not in listify(B)])\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    return '(' + indent('\\n'+qry.strip()+'\\n)', lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=42):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep().sort_index()\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "def get_desc(code):\n",
    "    for nm in code.split('_'):\n",
    "        if len(nm) == 4:\n",
    "            break\n",
    "    return f'{code} as {nm}_code, (select stv{nm}_desc from {catalog}saturnstv{nm} where {code} = stv{nm}_code limit 1) as {nm}_desc'\n",
    "\n",
    "def coalesce(x, y=False):\n",
    "    return f'coalesce({x}, {y}) as {x}'\n",
    "\n",
    "############ pandas functions ############\n",
    "pd.options.display.max_columns = None\n",
    "def disp(df, rows=4, head=True, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    with pd.option_context('display.min_rows', rows, 'display.max_rows', rows):\n",
    "        print(df.shape)\n",
    "        df = df.sort_index(axis=1) if sort else df\n",
    "        missing = df.isnull().sum().to_frame().T\n",
    "        X = pd.concat([df.dtypes.to_frame().T, missing, (missing/df.shape[0]*100).round(2), df.head(rows) if head else df.tails(rows)])\n",
    "        display(HTML(X.to_html()))\n",
    "\n",
    "def inser(df, column, value, loc=0):\n",
    "    df.insert(loc, column, value)\n",
    "    return df\n",
    "\n",
    "def to_numeric(df, downcast='integer', errors='ignore', **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: s.astype('string').str.lower().str.strip() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "        idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "        return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    return wrapper\n",
    "\n",
    "for f in [disp, inser, to_numeric, prep]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setattr(pd.DataFrame, f.__name__, f)\n",
    "    setattr(pd.Series, f.__name__, wrap(f))\n",
    "\n",
    "def prediction(clf, X, y):\n",
    "    Z = X.copy()\n",
    "    Z['prediction'] = clf.predict_proba(X)[:,1]\n",
    "    Z['actual'] = y\n",
    "    Z['mae'] = np.abs(Z['prediction'] - Z['actual'])\n",
    "    Z['log_loss'] = -1*(Z['actual']*np.log(Z['prediction']) + (1-Z['actual'])*np.log(1-Z['prediction']))\n",
    "    return Z\n",
    "\n",
    "for f in [prediction]:\n",
    "    setattr(fl.automl.automl.AutoML, f.__name__, f)\n",
    "\n",
    "#########################################\n",
    "################## AMP ##################\n",
    "#########################################\n",
    "@dataclasses.dataclass\n",
    "class Term():\n",
    "    term_code: int = 202408\n",
    "    cycle_day: int = None\n",
    "    cycle_date: str = None\n",
    "    overwrite: set = None\n",
    "    seed: int = 42\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.term_code = int(self.term_code)\n",
    "        self.cycle_day, self.cycle_date, self.stable_date, self.stem = self.get_cycle(self.term_code, self.cycle_day, self.cycle_date)\n",
    "        self.overwrite = setify(self.overwrite)\n",
    "        # Because these take about 1 hour each, we force user to manually delete parquet files to avoid accidental deletion requring lengthy re-creation\n",
    "        self.overwrite.discard('drivetimes_s')\n",
    "        self.overwrite.discard('drivetimes_m')\n",
    "        self.overwrite.discard('drivetimes_w')\n",
    "        self.overwrite.discard('drivetimes_r')\n",
    "        self.overwrite.discard('drivetimes_l')\n",
    "\n",
    "\n",
    "    def get(self, fcn, dst, prereq=[], *, divide=True, read=True, suffix='.parquet', **kwargs):\n",
    "        nm = str(dst)\n",
    "        if '/' in nm:\n",
    "            dst = pathlib.Path(dst).with_suffix(suffix)\n",
    "            nm = dst.stem\n",
    "        else:\n",
    "            dst = data/f'{dst}/{self.term_code}/{dst}_{self.stem}{suffix}'\n",
    "\n",
    "        if nm in self.overwrite:\n",
    "            del self[nm]\n",
    "            dst.unlink(missing_ok=True)\n",
    "            self.overwrite.remove(nm)\n",
    "\n",
    "        new = False\n",
    "        if not nm in self:\n",
    "            if not dst.exists():\n",
    "                new = True\n",
    "                for f in listify(prereq):\n",
    "                    f()\n",
    "                print(f'creating {dst.name}: ', end='')\n",
    "                with codetiming.Timer():\n",
    "                    rslt = fcn(**kwargs)\n",
    "                    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    if suffix == '.parquet':\n",
    "                        pd.DataFrame(rslt).prep().to_parquet(dst)  # forced to wrap with explicit pd.DataFrame to due strange error under pandas 2.2.3 \"Object of type PlanMetrics is not JSON serializable\" with to_parquet\n",
    "                    elif suffix == 'csv':\n",
    "                        pd.DataFrame(rslt).prep().to_csv(dst)\n",
    "                    else:\n",
    "                        with open(dst, 'wb') as f:\n",
    "                            pickle.dump(rslt, f, pickle.HIGHEST_PROTOCOL)\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "            if read:\n",
    "                if suffix == '.parquet':\n",
    "                    self[nm] = pd.read_parquet(dst)\n",
    "                elif suffix == 'csv':\n",
    "                    self[nm] = pd.read_csv(dst)\n",
    "                else:\n",
    "                    with open(dst, 'rb') as f:\n",
    "                        self[nm] = pickle.load(f)\n",
    "            else:\n",
    "                self[nm] = None\n",
    "        return self[nm], new\n",
    "##################################################\n",
    "################# get drivetimes #################\n",
    "##################################################\n",
    "    def get_zips(self, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pgeocode.Nominatim('us')._data  # get all zips\n",
    "                .prep()\n",
    "                .rename(columns={'postal_code':'zip'})\n",
    "                .query(\"state_code.notnull() & state_code not in [None,'mh']\")\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/zips')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_states(self, show=False):\n",
    "        return set(self.get_zips()['state_code'])\n",
    "\n",
    "\n",
    "    def get_drivetimes(self, show=False):\n",
    "        def fcn():\n",
    "            from pgeocode import Nominatim\n",
    "            from sklearn.metrics.pairwise import haversine_distances\n",
    "            print()\n",
    "            campus_coords = {\n",
    "                's': '-98.215784,32.216217',\n",
    "                'm': '-97.432975,32.582436',\n",
    "                'w': '-97.172176,31.587908',\n",
    "                'r': '-96.467920,30.642055',\n",
    "                'l': '-96.983211,32.462267',\n",
    "                }\n",
    "            url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "            gdf = gpd.read_file(url).prep().set_index('zcta5ce20')  # get all ZCTA https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n",
    "            pts = gdf.sample_points(size=10, method=\"uniform\").explode().apply(lambda g: f\"{g.x},{g.y}\")  # sample 10 random points in each ZCTA\n",
    "            M = []\n",
    "            for k, v in campus_coords.items():\n",
    "                def fcn1():\n",
    "                    print()\n",
    "                    L = []\n",
    "                    i = 0\n",
    "                    di = 200\n",
    "                    I = pts.shape[0]\n",
    "                    while i < I:\n",
    "                        u = join([v, *pts.iloc[i:i+di]],';')\n",
    "                        url = f\"http://router.project-osrm.org/table/v1/driving/{u}?sources={0}&annotations=duration,distance&fallback_speed=1&fallback_coordinate=snapped\"\n",
    "                        response = requests.get(url).json()\n",
    "                        L.append(np.squeeze(response['durations'])[1:]/60)\n",
    "                        i += di\n",
    "                        print(k,i,round(i/I*100))\n",
    "                    df = pts.to_frame()[[]]\n",
    "                    df[k] = np.concatenate(L)\n",
    "                    return df\n",
    "                df, new = self.get(fcn1, root/f'geo/drivetimes_{k}')\n",
    "                M.append(df)\n",
    "            D = pd.concat(M, axis=1).groupby(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "\n",
    "            # There are a few USPS zips without equivalent ZCTA, so we assign them drivetimes for the nearest\n",
    "            Z = self.get_zips().merge(D.query(\"camp_code=='s'\"), how='left').set_index('zip')\n",
    "            mask = Z['drivetime'].isnull()  # zips without a ZTCA\n",
    "            Z = Z[['latitude','longitude']]\n",
    "            X = np.radians(Z[~mask])\n",
    "            Y = np.radians(Z[mask])\n",
    "            M = (\n",
    "                pd.DataFrame(haversine_distances(X, Y), index=X.index, columns=Y.index) # haversine distance between pairs with and without ZCTA\n",
    "                .idxmin()  # find nearest ZCTA\n",
    "                .reset_index()\n",
    "                .set_axis(['new_zip','zip'], axis=1)\n",
    "                .prep()\n",
    "                .merge(D)  # merge the drivetimes for that ZCTA\n",
    "                .drop(columns='zip')\n",
    "                .rename(columns={'new_zip':'zip'})\n",
    "            )\n",
    "            df = pd.concat([D,M], ignore_index=True)\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/drivetimes', self.get_zips)\n",
    "        return df\n",
    "\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, show=False):\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        df, new = self.get(fcn, data/'terms')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_cycle(self, term_code, cycle_day=None, cycle_date=None, show=False):\n",
    "        stable_date = self.get_terms().loc[int(term_code),'stable_date']\n",
    "        if cycle_day is None:\n",
    "            if cycle_date is None:\n",
    "                cycle_date = pd.Timestamp.now()\n",
    "            cycle_date = min(pd.to_datetime(cycle_date), pd.Timestamp.now()).normalize()\n",
    "            cycle_day = (stable_date - cycle_date).days\n",
    "        cycle_date = (stable_date - pd.Timedelta(days=cycle_day)).date()\n",
    "        stem = f'{term_code}_{cycle_date}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        return cycle_day, cycle_date, stable_date, stem\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id,\n",
    "                spriden_pidm as pidm\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self, show=False):\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        counter = 0\n",
    "        divide = False\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > 5:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                cycle_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    cycle_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                cycle_day, cycle_date, stable_date, stem = self.get_cycle(term_code, cycle_date=cycle_date)\n",
    "                def fcn():\n",
    "                    df = (\n",
    "                        self.get_spriden()\n",
    "                        .assign(cycle_day=cycle_day, cycle_date=cycle_date)\n",
    "                        .merge(book.parse(sheet).prep(), on='id', how='right')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, flags_prc/f'{term_code}/flags_{stem}', read=False, divide=False)[1]:\n",
    "                    divide = True\n",
    "                    counter = 0\n",
    "                    dst = flags_prc/f'flags_{term_code}.parquet'\n",
    "                    dst.unlink(missing_ok=True)\n",
    "        if divide:\n",
    "            print(divider)\n",
    "            self.combine_flags()\n",
    "\n",
    "\n",
    "    def combine_flags(self, show=False):\n",
    "        def fcn(term_code):\n",
    "            F = sorted((flags_prc/f'{term_code}').glob('*.parquet'))+sorted((flags_prc/f'{term_code-2}').glob('*.parquet'))\n",
    "            L = [pd.read_parquet(src) for src in F]\n",
    "            df = pd.concat(L, ignore_index=True).prep()\n",
    "            del L\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                if k in df:\n",
    "                    df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        divide = False\n",
    "        for x in flags_prc.iterdir():\n",
    "            if x.is_dir():\n",
    "                term_code = int(x.stem)\n",
    "                if term_code%10==8:\n",
    "                    if self.get(fcn, flags_prc/f'flags_{term_code}', read=False, divide=False, term_code=term_code)[1]:\n",
    "                        divide = True\n",
    "        if divide:\n",
    "            print(divider)\n",
    "\n",
    "\n",
    "    def get_flags(self, show=False):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pd.read_parquet(flags_prc/f'flags_{self.term_code}.parquet')\n",
    "                .query(f\"cycle_date<='{self.cycle_date}'\")\n",
    "                .sort_values(['pidm','cycle_date'])\n",
    "                .drop_duplicates(subset=['pidm','term_code'], keep='last')\n",
    "            )\n",
    "            df.loc[~df['state'].isin(self.get_states()),'zip'] = pd.NA\n",
    "            df['zip'] = df['zip'].str.split('-', expand=True)[0].str[:5].to_numeric(errors='coerce')\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'flags', self.combine_flags)\n",
    "        return df\n",
    "##########################################\n",
    "############ get student data ############\n",
    "##########################################\n",
    "\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             qry = self.newest('pidm, subj_code, crse_numb', f\"\"\"\n",
    "# select distinct\n",
    "#     ,pidm\n",
    "#     ,subj_code\n",
    "#     ,crse_numb\n",
    "#     ,credit_hr\n",
    "#     ,current_date\n",
    "# from\n",
    "#     dev.opeir.opeirregistration_{self.get_terms().loc[self.term_code,'term_desc']}\n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with CTE as {subqry(qry)}\n",
    "\n",
    "# --individual courses\n",
    "# select\n",
    "#     pidm\n",
    "#     ,subj_code || crse_numb as crse_code\n",
    "#     ,credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "\n",
    "# union all\n",
    "\n",
    "# --total credit hours\n",
    "# select\n",
    "#     pidm\n",
    "#     ,'_tot_sch' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     pidm\n",
    "\n",
    "# union all\n",
    "\n",
    "# --headcount \n",
    "# select\n",
    "#     pidm\n",
    "#     ,'_headcnt' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     pidm\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "#     def temp(self, sel):\n",
    "#         def fcn:\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     pidm\n",
    "#     ,{self.cycle_day} as cycle_day\n",
    "#     ,timestamp('{self.cycle_date}') as cycle_date\n",
    "#     ,current_date\n",
    "#     ,first_date\n",
    "#     ,last_date\n",
    "#     ,{get_desc('term_code')}\n",
    "#     ,{get_desc('levl_code')}\n",
    "#     ,{get_desc('styp_code')}\n",
    "#     ,{get_desc('camp_code')}\n",
    "#     ,{get_desc('coll_code_1')}\n",
    "#     ,{get_desc('dept_code')}\n",
    "#     ,{get_desc('majr_code_1')}\n",
    "#     --,gender\n",
    "#     ,spbpers_sex as gender\n",
    "#     ,birth_date\n",
    "#     ,{get_desc('spbpers_lgcy_code')}\n",
    "#     ,gorvisa_vtyp_code is not null as international\n",
    "#     ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "#     ,{coalesce('race_asian')}\n",
    "#     ,{coalesce('race_black')}\n",
    "#     ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "#     ,{coalesce('race_native')}\n",
    "#     ,{coalesce('race_pacific')}\n",
    "#     ,{coalesce('race_white')}\n",
    "#     ,{indent(join(sel))}\n",
    "# from {subqry(qry)} as A\n",
    "\n",
    "# left join\n",
    "#     {catalog}spbpers_v\n",
    "# on\n",
    "#     pidm = spbpers_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}generalgorvisa\n",
    "#     qualify\n",
    "#         row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorvisa_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         gorprac_pidm\n",
    "#         ,max(gorprac_race_cde='AS') as race_asian\n",
    "#         ,max(gorprac_race_cde='BL') as race_black\n",
    "#         ,max(gorprac_race_cde='IN') as race_native\n",
    "#         ,max(gorprac_race_cde='HA') as race_pacific\n",
    "#         ,max(gorprac_race_cde='WH') as race_white\n",
    "#     from\n",
    "#         {catalog}generalgorprac\n",
    "#     group by\n",
    "#         gorprac_pidm\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorprac_pidm\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "    def get_admissions(self, show=False):\n",
    "        def fcn():\n",
    "            # get summer & fall admits\n",
    "            L = [self.newest('pidm, appl_no', f\"\"\"\n",
    "select distinct\n",
    "    A.*\n",
    "from\n",
    "    --dev.opeir.opeiradmissions_{self.get_terms().loc[term_code,'term_desc']} as A\n",
    "    dev.opeir.admissions_{self.get_terms().loc[term_code,'term_desc']}_v as A\n",
    "inner join\n",
    "    {catalog}saturnstvapdc as B\n",
    "on\n",
    "    apdc_code = stvapdc_code\n",
    "where\n",
    "    stvapdc_inst_acc_ind is not null  --only accepted\n",
    "\"\"\")\n",
    "                for term_code in [self.term_code-2, self.term_code]]\n",
    "            qry = join(L, '\\nunion all\\n')\n",
    "\n",
    "\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    pidm\n",
    "    ,{self.cycle_day} as cycle_day\n",
    "    ,timestamp('{self.cycle_date}') as cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,last_date\n",
    "    ,{get_desc('term_code')}\n",
    "    ,appl_no\n",
    "    ,{get_desc('apst_code')}\n",
    "    ,{get_desc('apdc_code')}\n",
    "    ,{get_desc('admt_code')}\n",
    "    ,{get_desc('levl_code')}\n",
    "    ,{get_desc('styp_code')}\n",
    "    ,{get_desc('camp_code')}\n",
    "    ,{get_desc('coll_code_1')}\n",
    "    ,{get_desc('dept_code')}\n",
    "    ,{get_desc('majr_code_1')}\n",
    "    ,{get_desc('saradap_resd_code')}\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    ,hs_percentile\n",
    "    --,sbgi_code\n",
    "    ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "qualify\n",
    "    min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "    and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "\"\"\"\n",
    "            df = run(qry, show)\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'admissions')\n",
    "        return df\n",
    "\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "\n",
    "# sel = [\n",
    "#     'appl_no',\n",
    "#     get_desc('apst_code'),\n",
    "#     get_desc('apdc_code'),\n",
    "#     get_desc('admt_code'),\n",
    "#     get_desc('saradap_resd_code'),\n",
    "#     'hs_percentile',\n",
    "#     # 'sbgi_code',\n",
    "#     'enrolled_ind='Y' as enrolled_ind',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "#     def get_admissions(self, show=False):\n",
    "#         def fcn():\n",
    "#             # get summer & fall admits\n",
    "#             L = [self.newest('pidm, appl_no', f\"\"\"\n",
    "# select distinct\n",
    "#     A.*\n",
    "# from\n",
    "#     --dev.opeir.opeiradmissions_{self.get_terms().loc[term_code,'term_desc']} as A\n",
    "#     dev.opeir.admissions_{self.get_terms().loc[term_code,'term_desc']}_v as A\n",
    "# inner join\n",
    "#     {catalog}saturnstvapdc as B\n",
    "# on\n",
    "#     apdc_code = stvapdc_code\n",
    "# where\n",
    "#     stvapdc_inst_acc_ind is not null  --only accepted\n",
    "# \"\"\")\n",
    "#                 for term_code in [self.term_code-2, self.term_code]]\n",
    "#             qry = join(L, '\\nunion all\\n')\n",
    "\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     pidm\n",
    "#     ,{self.cycle_day} as cycle_day\n",
    "#     ,timestamp('{self.cycle_date}') as cycle_date\n",
    "#     ,current_date\n",
    "#     ,first_date\n",
    "#     ,last_date\n",
    "#     ,{get_desc('term_code')}\n",
    "#     ,appl_no\n",
    "#     ,{get_desc('apst_code')}\n",
    "#     ,{get_desc('apdc_code')}\n",
    "#     ,{get_desc('admt_code')}\n",
    "#     ,{get_desc('levl_code')}\n",
    "#     ,{get_desc('styp_code')}\n",
    "#     ,{get_desc('camp_code')}\n",
    "#     ,{get_desc('coll_code_1')}\n",
    "#     ,{get_desc('dept_code')}\n",
    "#     ,{get_desc('majr_code_1')}\n",
    "#     ,{get_desc('saradap_resd_code')}\n",
    "#     --,gender\n",
    "#     ,spbpers_sex as gender\n",
    "#     ,birth_date\n",
    "#     ,{get_desc('spbpers_lgcy_code')}\n",
    "#     ,gorvisa_vtyp_code is not null as international\n",
    "#     ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "#     ,{coalesce('race_asian')}\n",
    "#     ,{coalesce('race_black')}\n",
    "#     ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "#     ,{coalesce('race_native')}\n",
    "#     ,{coalesce('race_pacific')}\n",
    "#     ,{coalesce('race_white')}\n",
    "#     ,hs_percentile\n",
    "#     --,sbgi_code\n",
    "#     ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "# from {subqry(qry)} as A\n",
    "\n",
    "# left join\n",
    "#     {catalog}spbpers_v\n",
    "# on\n",
    "#     pidm = spbpers_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}generalgorvisa\n",
    "#     qualify\n",
    "#         row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorvisa_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         gorprac_pidm\n",
    "#         ,max(gorprac_race_cde='AS') as race_asian\n",
    "#         ,max(gorprac_race_cde='BL') as race_black\n",
    "#         ,max(gorprac_race_cde='IN') as race_native\n",
    "#         ,max(gorprac_race_cde='HA') as race_pacific\n",
    "#         ,max(gorprac_race_cde='WH') as race_white\n",
    "#     from\n",
    "#         {catalog}generalgorprac\n",
    "#     group by\n",
    "#         gorprac_pidm\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorprac_pidm\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'admissions')\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "#     def get_admissions(self, show=False):\n",
    "#         def fcn():\n",
    "#             # get summer & fall admits\n",
    "#             L = [self.newest('pidm, appl_no', f\"\"\"\n",
    "# select distinct\n",
    "#     A.*\n",
    "# from\n",
    "#     --dev.opeir.opeiradmissions_{self.get_terms().loc[term_code,'term_desc']} as A\n",
    "#     dev.opeir.admissions_{self.get_terms().loc[term_code,'term_desc']}_v as A\n",
    "# inner join\n",
    "#     {catalog}saturnstvapdc as B\n",
    "# on\n",
    "#     apdc_code = stvapdc_code\n",
    "# where\n",
    "#     stvapdc_inst_acc_ind is not null  --only accepted\n",
    "# \"\"\")\n",
    "#                 for term_code in [self.term_code-2, self.term_code]]\n",
    "#             qry = join(L, '\\nunion all\\n')\n",
    "\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     pidm\n",
    "#     ,{self.cycle_day} as cycle_day\n",
    "#     ,timestamp('{self.cycle_date}') as cycle_date\n",
    "#     ,current_date\n",
    "#     ,first_date\n",
    "#     ,last_date\n",
    "#     ,{get_desc('term_code')}\n",
    "#     ,appl_no\n",
    "#     ,{get_desc('apst_code')}\n",
    "#     ,{get_desc('apdc_code')}\n",
    "#     ,{get_desc('admt_code')}\n",
    "#     ,{get_desc('levl_code')}\n",
    "#     ,{get_desc('styp_code')}\n",
    "#     ,{get_desc('camp_code')}\n",
    "#     ,{get_desc('coll_code_1')}\n",
    "#     ,{get_desc('dept_code')}\n",
    "#     ,{get_desc('majr_code_1')}\n",
    "#     ,{get_desc('saradap_resd_code')}\n",
    "#     --,gender\n",
    "#     ,spbpers_sex as gender\n",
    "#     ,birth_date\n",
    "#     ,{get_desc('spbpers_lgcy_code')}\n",
    "#     ,gorvisa_vtyp_code is not null as international\n",
    "#     ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "#     ,{coalesce('race_asian')}\n",
    "#     ,{coalesce('race_black')}\n",
    "#     ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "#     ,{coalesce('race_native')}\n",
    "#     ,{coalesce('race_pacific')}\n",
    "#     ,{coalesce('race_white')}\n",
    "#     ,hs_percentile\n",
    "#     --,sbgi_code\n",
    "#     ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "# from {subqry(qry)} as A\n",
    "\n",
    "# left join\n",
    "#     {catalog}spbpers_v\n",
    "# on\n",
    "#     pidm = spbpers_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}generalgorvisa\n",
    "#     qualify\n",
    "#         row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorvisa_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         gorprac_pidm\n",
    "#         ,max(gorprac_race_cde='AS') as race_asian\n",
    "#         ,max(gorprac_race_cde='BL') as race_black\n",
    "#         ,max(gorprac_race_cde='IN') as race_native\n",
    "#         ,max(gorprac_race_cde='HA') as race_pacific\n",
    "#         ,max(gorprac_race_cde='WH') as race_white\n",
    "#     from\n",
    "#         {catalog}generalgorprac\n",
    "#     group by\n",
    "#         gorprac_pidm\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorprac_pidm\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'admissions')\n",
    "#         return df\n",
    "\n",
    "\n",
    "    def get_students(self, show=False):\n",
    "        def fcn():\n",
    "            df = (self.admissions\n",
    "                  .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                  .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])                \n",
    "            )\n",
    "            mask = df.eval(\"drivetime.isnull() & zip.notnull() & camp_code!='o'\")\n",
    "            if mask.any():\n",
    "                df[mask].set_index(['state','city','zip','camp_code'])[[]].sort_index().reset_index().disp(50)\n",
    "\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            \n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            \n",
    "            \n",
    "            # df['oriented'] = np.where(df['orien_sess'].notnull() | df['registered'].notnull(), 'y', np.where(df['orientation_hold_exists'].notnull(), 'n', 'w'))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "\n",
    "            # df['verified'] = np.where(df['ver_complete'].notnull(), 'y', np.where(df['selected_for_ver'].notnull(), 'n', 'w'))\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            \n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "\n",
    "            df['eager'] = (self.stable_date - df['first_date']).dt.days\n",
    "            df['age'] = (self.stable_date - df['birth_date']).dt.days\n",
    "\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            \n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "\n",
    "\n",
    "\n",
    "            # df['majr_code'] = df['majr_code'].replace({'0000':pd.NA, 'und':pd.NA, 'eled':'eted', 'agri':'unda'})\n",
    "\n",
    "            # df['coll_code'] = df['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "\n",
    "            # df['coll_desc'] = df['coll_code'].map({\n",
    "            #     'an': 'ag & natural_resources',\n",
    "            #     'ba': 'business',\n",
    "            #     'ed': 'education',\n",
    "            #     'en': 'engineering',\n",
    "            #     'hl': 'health sciences',\n",
    "            #     'la': 'liberal & fine arts',\n",
    "            #     'sm': 'science & mathematics',\n",
    "            #     pd.NA: 'no college designated',\n",
    "            # })\n",
    "\n",
    "\n",
    "\n",
    "            # checks = [\n",
    "            #     'cycle_day >= 0',\n",
    "            #     'eager >= cycle_day',\n",
    "            #     'age >= 5000',\n",
    "            #     'distance >= 0',\n",
    "            #     'hs_pctl >=0',\n",
    "            #     'hs_pctl <= 100',\n",
    "            #     'hs_qrtl >= 0',\n",
    "            #     'hs_qrtl <= 4',\n",
    "            #     'act_equiv >= 1',\n",
    "            #     'act_equiv <= 36',\n",
    "            #     'gap_score >= 0',\n",
    "            #     'gap_score <= 100',\n",
    "            # ]\n",
    "            # for check in checks:\n",
    "            #     mask = df.eval(check)\n",
    "            #     assert mask.all(), [check,df[~mask].disp(5)]\n",
    "            mask = df['cycle_date_flags'].isnull()  # rows from admissions not on flags - should not be any\n",
    "            if mask.any():\n",
    "                display(df[mask]['styp_code'].value_counts().sort_index().to_frame().T)\n",
    "            return df.set_index(['pidm'])\n",
    "        df, new = self.get(fcn, 'students', [self.get_admissions,self.get_flags,self.get_drivetimes])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def newest(self, qry, part, sel='0 as temp'):\n",
    "        \"\"\"The OPEIR daily snapshot experienced occasional glitched causing incomplete copies.\n",
    "        Consequently, record can vanished then reappear later. This function fixes this issue.\"\"\"\n",
    "        A, B = [indent(s.strip()) for s in qry.rsplit('from',1)]\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    *\n",
    "from (\n",
    "    {A}\n",
    "        ,min(current_date) over (partition by {part}) as first_date\n",
    "        ,max(current_date) over (partition by {part}) as last_date\n",
    "    from\n",
    "        {B}\n",
    "    qualify\n",
    "        '{self.cycle_date}' between first_date and dateadd(day, 5, last_date)  -- keep records where cycle_date falls between its first & last appearance (+5 days for safety)\n",
    "    )\n",
    "where\n",
    "    current_date <= '{self.cycle_date}'  -- discard records after cycle_date\n",
    "qualify\n",
    "    row_number() over (partition by {part} order by current_date desc) = 1  -- keep most recent remaining record\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    pidm\n",
    "    ,{self.cycle_day} as cycle_day\n",
    "    ,timestamp('{self.cycle_date}') as cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,last_date\n",
    "    ,{get_desc('term_code')}\n",
    "    ,{get_desc('levl_code')}\n",
    "    ,{get_desc('styp_code')}\n",
    "    ,{get_desc('camp_code')}\n",
    "    ,{get_desc('coll_code_1')}\n",
    "    ,{get_desc('dept_code')}\n",
    "    ,{get_desc('majr_code_1')}\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    ,{indent(join(sel))}\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\"\"\"\n",
    "        return qry\n",
    "\n",
    "\n",
    "    def get_admissions(self, show=False):\n",
    "        def fcn():\n",
    "            qry = self.newest(\n",
    "                part = 'pidm, appl_no',\n",
    "                sel = [\n",
    "                    'appl_no',\n",
    "                    get_desc('apst_code'),\n",
    "                    get_desc('apdc_code'),\n",
    "                    get_desc('admt_code'),\n",
    "                    get_desc('saradap_resd_code'),\n",
    "                    'hs_percentile',\n",
    "                    # 'sbgi_code',\n",
    "                ],\n",
    "                qry = f\"\"\"\n",
    "select distinct\n",
    "    A.*\n",
    "from\n",
    "    --dev.opeir.opeiradmissions_{self.get_terms().loc[term_code,'term_desc']} as A\n",
    "    dev.opeir.admissions_{self.get_terms().loc[term_code,'term_desc']}_v as A\n",
    "inner join\n",
    "    {catalog}saturnstvapdc as B\n",
    "on\n",
    "    apdc_code = stvapdc_code\n",
    "where\n",
    "    stvapdc_inst_acc_ind is not null  --only accepted\n",
    "\"\"\")\n",
    "                for term_code in [self.term_code-2, self.term_code]]\n",
    "            qry = join(L, '\\nunion all\\n')\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     *\n",
    "#     ,min(levl_code='UG') over (partition by pidm) as lev\n",
    "# from {subqry(qry)}\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     *\n",
    "# from {subqry(qry)}\n",
    "# where\n",
    "#     lev\n",
    "# \"\"\"\n",
    "\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "# \"\"\"\n",
    "            df = run(qry, show)\n",
    "            # df = (\n",
    "            #     run(qry, show)\n",
    "            #     .sort_values('appl_no')\n",
    "            #     .groupby('pidm')\n",
    "            #     .filter(lambda x: (x['levl_code']=='ug').all())\n",
    "            # )\n",
    "            \n",
    "            \n",
    "            return df\n",
    "        df, new = self.get(fcn, 'registrations')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_registrations(self, show=False):\n",
    "        def fcn():\n",
    "            grp = 'pidm, term_code, levl_code, styp_code'\n",
    "            qry = self.newest(\n",
    "                part = 'pidm, subj_code, crse_numb',\n",
    "                sel = [\n",
    "                    'subj_code || crse_numb as crse_code',\n",
    "                    'crn',\n",
    "                    'credit_hr',\n",
    "                    ],\n",
    "                qry=f\"\"\"\n",
    "select distinct\n",
    "    A.*\n",
    "from\n",
    "    dev.opeir.opeirregistration_{self.get_terms().loc[self.term_code,'term_desc']} as A\n",
    "\"\"\")\n",
    "            df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'registrations')\n",
    "        return df\n",
    "\n",
    "self = Term(\n",
    "    cycle_day=0,\n",
    "    term_code=202408,\n",
    "    overwrite=[\n",
    "        # 'terms',\n",
    "        # 'zips',\n",
    "        # 'drivetimes',\n",
    "        # 'flags',\n",
    "        'admissions',\n",
    "        # 'students',\n",
    "        'registrations',\n",
    "    ]\n",
    ")\n",
    "# self.process_flags()\n",
    "# self.get_zips()\n",
    "# self.get_drivetimes()\n",
    "# self.get_registrations(show=True)\n",
    "self.get_admissions(show=True)\n",
    "# self.get_students()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b88bade-654c-4d71-aa28-4c608081ef43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select\n",
    "    pidm\n",
    "    ,0 as cycle_day\n",
    "    ,timestamp('2024-09-20') as cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,last_date\n",
    "    ,term_code as term_code, (select stvterm_desc from dev.bronze.saturnstvterm where term_code = stvterm_code limit 1) as term_desc\n",
    "    ,levl_code as levl_code, (select stvlevl_desc from dev.bronze.saturnstvlevl where levl_code = stvlevl_code limit 1) as levl_desc\n",
    "    ,styp_code as styp_code, (select stvstyp_desc from dev.bronze.saturnstvstyp where styp_code = stvstyp_code limit 1) as styp_desc\n",
    "    ,camp_code as camp_code, (select stvcamp_desc from dev.bronze.saturnstvcamp where camp_code = stvcamp_code limit 1) as camp_desc\n",
    "    ,coll_code_1 as coll_code, (select stvcoll_desc from dev.bronze.saturnstvcoll where coll_code_1 = stvcoll_code limit 1) as coll_desc\n",
    "    ,dept_code as dept_code, (select stvdept_desc from dev.bronze.saturnstvdept where dept_code = stvdept_code limit 1) as dept_desc\n",
    "    ,majr_code_1 as majr_code, (select stvmajr_desc from dev.bronze.saturnstvmajr where majr_code_1 = stvmajr_code limit 1) as majr_desc\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,spbpers_lgcy_code as lgcy_code, (select stvlgcy_desc from dev.bronze.saturnstvlgcy where spbpers_lgcy_code = stvlgcy_code limit 1) as lgcy_desc\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from dev.bronze.saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,coalesce(race_asian, False) as race_asian\n",
    "    ,coalesce(race_black, False) as race_black\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,coalesce(race_native, False) as race_native\n",
    "    ,coalesce(race_pacific, False) as race_pacific\n",
    "    ,coalesce(race_white, False) as race_white\n",
    "    ,appl_no\n",
    "    ,apst_code as apst_code, (select stvapst_desc from dev.bronze.saturnstvapst where apst_code = stvapst_code limit 1) as apst_desc\n",
    "    ,apdc_code as apdc_code, (select stvapdc_desc from dev.bronze.saturnstvapdc where apdc_code = stvapdc_code limit 1) as apdc_desc\n",
    "    ,admt_code as admt_code, (select stvadmt_desc from dev.bronze.saturnstvadmt where admt_code = stvadmt_code limit 1) as admt_desc\n",
    "    ,saradap_resd_code as resd_code, (select stvresd_desc from dev.bronze.saturnstvresd where saradap_resd_code = stvresd_code limit 1) as resd_desc\n",
    "    ,hs_percentile\n",
    "from (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        select distinct\n",
    "            A.*\n",
    "            ,min(current_date) over (partition by pidm, appl_no) as first_date\n",
    "            ,max(current_date) over (partition by pidm, appl_no) as last_date\n",
    "        from\n",
    "            --dev.opeir.opeiradmissions_summer2024 as A\n",
    "            dev.opeir.admissions_summer2024_v as A\n",
    "        inner join\n",
    "            dev.bronze.saturnstvapdc as B\n",
    "        on\n",
    "            apdc_code = stvapdc_code\n",
    "        where\n",
    "            stvapdc_inst_acc_ind is not null  --only accepted\n",
    "        qualify\n",
    "            '2024-09-20' between first_date and dateadd(day, 5, last_date)  -- keep records where cycle_date falls between its first & last appearance (+5 days for safety)\n",
    "        )\n",
    "    where\n",
    "        current_date <= '2024-09-20'  -- discard records after cycle_date\n",
    "    qualify\n",
    "        row_number() over (partition by pidm, appl_no order by current_date desc) = 1  -- keep most recent remaining record\n",
    "    ) as A\n",
    "\n",
    "left join\n",
    "    dev.bronze.spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        dev.bronze.generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        dev.bronze.generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\"\"\"\n",
    "df = run(qry)\n",
    "df['term_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde331b1-9883-49b8-9800-21fcc3ea5d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    self.get_admissions(show=True).copy()\n",
    "    # .sort_values('appl_no')\n",
    "    # .groupby('pidm')\n",
    "    # .cumcount()\n",
    "    # # .filter(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']))\n",
    "    # .last()\n",
    ")\n",
    "# df.describe()\n",
    "df['term_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d77f97-c7ff-44c5-987c-7b056e1ac97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "A = self.get_registrations().reset_index()\n",
    "A.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3219745-91e8-4491-9039-4f28ec006cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col = ['levl_code','styp_code']\n",
    "df = self.get_admissions().set_index('pidm')[col].join(self.get_registrations().loc['_headcnt',col], how='outer', rsuffix='_r')\n",
    "mask = df.notnull().all(axis=1) & ((df['levl_code']!=df['levl_code_r']) | (df['styp_code']!=df['styp_code_r']))\n",
    "df[mask]\n",
    "# self.get_registrations().loc['_headcnt',col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6984254-7802-4c01-a87e-48d8448c66bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class AMP(Term):\n",
    "    date: str = ''\n",
    "    crse_code: str = '_headcnt'\n",
    "    year: int = 2024\n",
    "    time_budget: int = 60\n",
    "    overwrite: set = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.year = int(self.year)\n",
    "        self.term_code = 100*self.year+8\n",
    "        assert len(self.date)==5, \"Please specify date using 'mm-dd' format (2 digit month & 2 digit day)\"\n",
    "        self.cycle_date = f'{self.year}-{self.date}'\n",
    "        super().__post_init__()\n",
    "        kwargs = {k: self[k] for k in ['cycle_date','term_code','overwrite']}\n",
    "        self.current = Term(**kwargs)\n",
    "        self.stable = Term(**kwargs, cycle_day=0)\n",
    "        # self.idx = 'styp_desc'\n",
    "        self.idx = 'styp_code'\n",
    "        self.agg = ['styp_desc','camp_desc']\n",
    "        self.features = {\n",
    "            'act_equiv':pd.NA,\n",
    "            'age':pd.NA,\n",
    "            'camp_desc':'stephenville',\n",
    "            'drivetime':pd.NA,\n",
    "            'eager':pd.NA,\n",
    "            'fafsa': False,\n",
    "            'finaid': False,\n",
    "            'gap_score':0,\n",
    "            'gender':pd.NA,\n",
    "            'hs_qrtl':pd.NA,\n",
    "            'lgcy':False,\n",
    "            'oriented':False,\n",
    "            'race_asian':False,\n",
    "            'race_black':False,\n",
    "            'race_hispanic':False,\n",
    "            'race_native':False,\n",
    "            'race_pacific':False,\n",
    "            'race_white':False,\n",
    "            'schlship':False,\n",
    "            'ssb':False,\n",
    "            'tsi_math':False,\n",
    "            'tsi_reading':False,\n",
    "            'tsi_writing':False,\n",
    "            'verified':False,\n",
    "            'waiver':False,\n",
    "            'credit_hr':0,\n",
    "            'current':False,\n",
    "        }\n",
    "        self.get_prepared()\n",
    "        if self.year < max(years):\n",
    "            self.get_multipliers()\n",
    "            self.get_models()\n",
    "\n",
    "\n",
    "    def get_prepared(self):\n",
    "        def fcn():\n",
    "            df = self.current.get_students()\n",
    "            try:\n",
    "                df['credit_hr'] = self.current.get_registrations().loc['_tot_sch','credit_hr']\n",
    "            except:\n",
    "                df['credit_hr'] = 0\n",
    "                # print('credit_hr')\n",
    "                clear_output()\n",
    "\n",
    "            try:\n",
    "                df['current'] = self.current.get_registrations().loc[self.crse_code,'credit_hr']>0\n",
    "            except:\n",
    "                df['current'] = False\n",
    "                # print('current')\n",
    "                clear_output()\n",
    "\n",
    "            try:\n",
    "                df['stable'] = self.stable.get_registrations().loc[self.crse_code,'credit_hr']>0\n",
    "            except:\n",
    "                df['stable'] = False\n",
    "                # print('stable')\n",
    "                clear_output()\n",
    "\n",
    "            df = (\n",
    "                df\n",
    "                # self.current.get_students()\n",
    "                # .join(self.current.get_registrations().loc['_tot_sch'  ]['credit_hr'])\n",
    "                # .join(self.current.get_registrations().loc[self.crse_code]['credit_hr'].rename('current')>0)\n",
    "                # .join(self.stable .get_registrations().loc[self.crse_code]['credit_hr'].rename('stable' )>0)\n",
    "                .fillna(self.features|{'stable':False})\n",
    "                .query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    "                .prep()\n",
    "                .set_index(difference(listify(self.idx)+listify(self.agg), self.features), append=True)\n",
    "                .sort_index()\n",
    "            )\n",
    "            for k,v in df.select_dtypes('string').items():\n",
    "                df[k] = pd.Categorical(v)\n",
    "\n",
    "            def fcn1(Z):\n",
    "                y = Z.pop('stable')\n",
    "                imp = mf.ImputationKernel(Z[Z.columns.intersection(self.features)].reset_index(drop=True), random_state=self.seed)\n",
    "                imp.mice(10)\n",
    "                return imp.complete_data().set_index(Z.index), y\n",
    "            return {key: fcn1(Z) for key, Z in df.groupby(self.idx)}\n",
    "        dct, new = self.get(fcn, f'prepared', self.current.get_students, suffix='.pkl')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_enrollments(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                self.stable.get_registrations()\n",
    "                .query(f\"levl_code=='ug' & styp_code in ['n','t','r']\")\n",
    "                .join(self.current.get_students().assign(admitted=lambda x: x['current_date'].notnull().prep())['admitted'])\n",
    "                .fillna({'admitted':False})\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, f'enrollments', [self.stable.get_registrations,self.current.get_students])\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def get_multipliers(self):\n",
    "        def fcn():\n",
    "            return 1 / self.get_enrollments().groupby(['crse_code',*listify(self.idx)])['admitted'].mean().rename('mlt')\n",
    "        df, new = self.get(fcn, f'multipliers', self.get_enrollments)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_models(self):\n",
    "        def fcn():\n",
    "            def fcn1(Z):\n",
    "                dct = {\n",
    "                    'time_budget':self.time_budget,\n",
    "                    'task':'classification',\n",
    "                    'verbose':0,\n",
    "                    'metric':'log_loss',\n",
    "                    'eval_method':'cv',\n",
    "                    'n_splits':3,\n",
    "                    'seed':self.seed,\n",
    "                    # 'early_stop':True,\n",
    "                    'estimator_list': ['xgboost','lgbm','rf'],\n",
    "                }\n",
    "                clf = fl.AutoML(**dct)\n",
    "                clf.fit(*Z, **dct)\n",
    "                # y = X[[]].join(self.stable.get_registrations().loc[self.crse_code]['credit_hr'].rename('stable')>0).fillna(False)\n",
    "                # y = X[[]].join(self.stable.get_registrations().loc[self.crse_code]['credit_hr'].rename('stable')).fillna(0)>0\n",
    "                # y = X.join(self.stable.get_registrations().loc[self.crse_code])['credit_hr'].fillna(0)>0\n",
    "                \n",
    "                # ['credit_hr'].rename('stable')).fillna(0)>0\n",
    "                # print(type(y))\n",
    "                # y.disp(1)\n",
    "                # clf.fit(X, y, **dct)\n",
    "                return clf\n",
    "            return {key: fcn1(Z) for key, Z in self.get_prepared().items()}\n",
    "        clf, new = self.get(fcn, f'models', self.get_enrollments, suffix='.pickle')\n",
    "        return clf\n",
    "\n",
    "\n",
    "    def get_predictions(self, learners=dict()):\n",
    "        def fcn():\n",
    "            dct = {\n",
    "                'crse_code': self.crse_code,\n",
    "                'prediction_year': self.year,\n",
    "                'learner_year': learner.year,\n",
    "            }\n",
    "\n",
    "            L = [\n",
    "                clf.prediction(*self.get_prepared()[key])\n",
    "                .assign(**dct, mlt=learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "                .set_index(list(dct), append=True)\n",
    "\n",
    "                # .set_index(list(dct.keys()))\n",
    "\n",
    "                #  .reset_index()\n",
    "                # .inser('mlt', learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "                # .inser('learner_year', learner.year)\n",
    "                # .inser('prediction_year', self.year)\n",
    "                # .inser('crse_code', self.crse_code)\n",
    "                for year, learner in learners.items() if 'models' in learner\n",
    "                for key, clf in learner.get_models().items()]\n",
    "            return pd.concat(L) if len(L)>0 else pd.DataFrame()\n",
    "        df, new = self.get(fcn, f'predictions', self.get_prepared)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_forecasts(self):\n",
    "        def fcn():\n",
    "            return {key:\n",
    "                self.get_predictions()\n",
    "                .assign(forecast=lambda Z: Z['prediction']*Z['mlt'])\n",
    "                .groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key))\n",
    "                [['forecast']].sum()\n",
    "                for key in self.agg}\n",
    "            # def fcn1(Z):\n",
    "            #     return (Z['prediction']*Z['mlt']).sum()\n",
    "            # df = self.get_predictions().copy()\n",
    "            # df['prediction'] *= df['mlt']\n",
    "            # return df.groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key))['prediction'].sum()\n",
    "            # return self.get_predictions().groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key)).apply(fcn1)\n",
    "        dct, new = self.get(fcn, f'forecasts', suffix='pickle')\n",
    "        return dct\n",
    "\n",
    "\n",
    "# years = [2022,2023,2024,2025]\n",
    "years = [2025,2024]\n",
    "years = listify(years)\n",
    "amps = {year: AMP(\n",
    "        date='03-28',\n",
    "        crse_code='_headcnt',\n",
    "        time_budget=10,\n",
    "        year=year,\n",
    "        overwrite={\n",
    "            # 'flags',\n",
    "            # 'admissions',\n",
    "            # 'students',\n",
    "            # 'registrations',\n",
    "            # 'prepared',\n",
    "            'enrollments',\n",
    "            'multipliers',\n",
    "            'models',\n",
    "            'predictions',\n",
    "            'forecasts',\n",
    "            },\n",
    "    ) for year in years}\n",
    "\n",
    "for self in amps.values():\n",
    "    self.get_predictions(amps)\n",
    "\n",
    "\n",
    "# F = pd.concat([self.forecasts for self in amp.values()]).sort_index(ascending=[True, False,False])\n",
    "# F.to_csv(data/f'forecast\n",
    "\n",
    "# self = amps[2024]\n",
    "# self.get_predictions()\n",
    "# self.get_forecasts()\n",
    "# self.get_admissions()\n",
    "# self.current.get_registrations(show=True)\n",
    "self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3db5ca-fcf8-4029-83b0-2622d5cec375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.predictions.groupby(['prediction_year','learner_year','styp_code'])['prediction'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca59739-b32c-440c-a830-e82cce039059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "learners = amps\n",
    "def fcn():\n",
    "    L = [\n",
    "        clf.prediction(*self.get_prepared()[key])\n",
    "        #  .reset_index()\n",
    "        .inser('mlt', learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "        .inser('learner_year', learner.year)\n",
    "        .inser('prediction_year', self.year)\n",
    "        .inser('crse_code', self.crse_code)\n",
    "        for year, learner in learners.items() if 'models' in learner\n",
    "        for key, clf in learner.get_models().items()]\n",
    "    return pd.concat(L) if len(L)>0 else pd.DataFrame()\n",
    "# fcn()\n",
    "self.get_predictions(learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8661b52b-6891-4e5e-b8e0-f5bc26045f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "learners = amps\n",
    "L = [\n",
    "    clf.prediction(*self.get_prepared()[key])\n",
    "    #  .reset_index()\n",
    "    .inser('mlt', learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "    .inser('learner_year', learner.year)\n",
    "    .inser('prediction_year', self.year)\n",
    "    .inser('crse_code', self.crse_code)\n",
    "    for year, learner in learners.items() if 'models' in learner\n",
    "    for key, clf in learner.get_models().items()]\n",
    "pd.concat(L) if len(L)>0 else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb84537a-2cee-456d-9c40-6e08284aab11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "learner = amps[2024]\n",
    "self = amps[2025]\n",
    "key = ('n',)\n",
    "clf = learner.get_models()[key]\n",
    "(\n",
    "    clf.prediction(*self.get_prepared()[key])\n",
    "    .inser('mlt', learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "    .inser('learner_year', learner.year)\n",
    "    .inser('prediction_year', self.year)\n",
    "    .inser('crse_code', self.crse_code)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20abeba-9845-4d15-998b-b28c267fe5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = self.current.get_students()\n",
    "A = self.current.get_registrations().loc['_tot_sch','credit_hr']#.fillna(0)\n",
    "A\n",
    "df['credit_hr'] = self.current.get_registrations().loc['_tot_sch','credit_hr']\n",
    "df['credit_hr'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b06020-04a5-471b-98b6-253fce57fb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "X = self.get_prepared()[('n',)][0]\n",
    "X['current'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c193eca-bb6a-422a-8b62-522d6109b413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, contextlib\n",
    "with open(os.devnull, 'w') as fnull:\n",
    "    with contextlib.redirect_stdout(fnull):\n",
    "        with contextlib.redirect_stderr(fnull):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                try:\n",
    "                    spark.sql(\"SELECT * FROM does_not_exist\").show()\n",
    "                except:\n",
    "                    print('oops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159f26a8-f1d2-45a5-8cf2-4a31e3b03e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amps[2024].get_prepared()[('n',)][1].disp(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f2d9df-e151-48f2-8304-31ff4dd273c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amps[2025].get_prepared()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb8bc0e-34ff-4fef-ba78-895a8d17aa9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class AMP(Term):\n",
    "    date: str = ''\n",
    "    crse_code: str = '_headcnt'\n",
    "    year: int = 2024\n",
    "    time_budget: int = 60\n",
    "    overwrite: set = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.year = int(self.year)\n",
    "        self.term_code = 100*self.year+8\n",
    "        assert len(self.date)==5, \"Please specify date using 'mm-dd' format (2 digit month & 2 digit day)\"\n",
    "        self.cycle_date = f'{self.year}-{self.date}'\n",
    "        super().__post_init__()\n",
    "        kwargs = {k: self[k] for k in ['cycle_date','term_code','overwrite']}\n",
    "        self.current = Term(**kwargs)\n",
    "        self.stable = Term(**kwargs, cycle_day=0)\n",
    "        self.idx = ['styp_desc']\n",
    "        self.agg = ['styp_desc','camp_desc']\n",
    "        self.features = {\n",
    "            'act_equiv':pd.NA,\n",
    "            'age':pd.NA,\n",
    "            'camp_desc':'stephenville',\n",
    "            'drivetime':pd.NA,\n",
    "            'eager':pd.NA,\n",
    "            'fafsa': False,\n",
    "            'finaid': False,\n",
    "            'gap_score':0,\n",
    "            'gender':pd.NA,\n",
    "            'hs_qrtl':pd.NA,\n",
    "            'lgcy':False,\n",
    "            'oriented':False,\n",
    "            'race_asian':False,\n",
    "            'race_black':False,\n",
    "            'race_hispanic':False,\n",
    "            'race_native':False,\n",
    "            'race_pacific':False,\n",
    "            'race_white':False,\n",
    "            'schlship':False,\n",
    "            'ssb':False,\n",
    "            'tsi_math':False,\n",
    "            'tsi_reading':False,\n",
    "            'tsi_writing':False,\n",
    "            'verified':False,\n",
    "            'waiver':False,\n",
    "            'credit_hr':0,\n",
    "            'current':False,\n",
    "        }\n",
    "        self.get_prepared()\n",
    "        self.get_multipliers()\n",
    "        if self.year < max(years):\n",
    "            self.get_models()\n",
    "\n",
    "\n",
    "    def get_prepared(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                self.current.get_students()\n",
    "                # .join(self.current.get_registrations().loc['_tot_sch'  ]['credit_hr'])\n",
    "                # .join(self.current.get_registrations().loc[self.crse_code]['credit_hr'].rename('current')>0)\n",
    "                .join(self.stable .get_registrations().loc[self.crse_code]['credit_hr'].rename('stable' )>0)\n",
    "                .query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    "                .fillna(self.features|{'stable':False})\n",
    "                .prep()\n",
    "                # .set_index([x for x in self.idx if x not in self.features], append=True)\n",
    "                .set_index(difference(self.idx+self.agg, self.features), append=True)\n",
    "                .sort_index()\n",
    "            )\n",
    "            for k,v in df.select_dtypes('string').items():\n",
    "                df[k] = pd.Categorical(v)\n",
    "\n",
    "            def fcn1(Z):\n",
    "                y = Z.pop('stable')\n",
    "                imp = mf.ImputationKernel(Z[Z.columns.intersection(self.features)].reset_index(drop=True), random_state=self.seed)\n",
    "                imp.mice(10)\n",
    "                return imp.complete_data().set_index(Z.index), y\n",
    "            return {key: fcn1(Z) for key, Z in df.groupby(self.idx)}\n",
    "        dct, new = self.get(fcn, f'prepared', [self.current.get_students,self.current.get_registrations,self.stable.get_registrations], suffix='.pkl')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_enrollments(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                self.stable.get_registrations()\n",
    "                .query(f\"levl_code=='ug' & styp_code in ['n','t','r']\")\n",
    "                .join(self.current.get_students().assign(admitted=lambda x: x['current_date'].notnull().prep())['admitted'])\n",
    "                .fillna({'admitted':False})\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, f'enrollments', [self.stable.get_registrations,self.current.get_students])\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def get_multipliers(self):\n",
    "        def fcn():\n",
    "            return 1 / self.get_enrollments().groupby(['crse_code',*self.idx])['admitted'].mean().rename('mlt')\n",
    "        df, new = self.get(fcn, f'multipliers', self.get_enrollments)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_models(self):\n",
    "        def fcn():\n",
    "            def fcn1(Z):\n",
    "                dct = {\n",
    "                    'time_budget':self.time_budget,\n",
    "                    'task':'classification',\n",
    "                    'verbose':0,\n",
    "                    'metric':'log_loss',\n",
    "                    'eval_method':'cv',\n",
    "                    'n_splits':3,\n",
    "                    'seed':self.seed,\n",
    "                    # 'early_stop':True,\n",
    "                    'estimator_list': ['xgboost','lgbm','rf'],\n",
    "                }\n",
    "                clf = fl.AutoML(**dct)\n",
    "                clf.fit(*Z, **dct)\n",
    "                return clf\n",
    "            return {key: fcn1(Z) for key, Z in self.get_prepared().items()}\n",
    "        clf, new = self.get(fcn, f'models', self.get_enrollments, suffix='.pickle')\n",
    "        return clf\n",
    "\n",
    "\n",
    "    def get_predictions(self, learners=dict()):\n",
    "        def fcn():\n",
    "            L = [#learner.get_enrollments().loc[self.crse_code,*key]['mlt']\n",
    "                clf.prediction(*self.get_prepared()[key])\n",
    "                #  .reset_index()\n",
    "                 .inser('mlt', learner.get_multipliers().loc[self.crse_code,*key]['mlt'])\n",
    "                 .inser('learner_year', learner.year)\n",
    "                 .inser('prediction_year', self.year)\n",
    "                 .inser('crse_code', self.crse_code)\n",
    "                 for year, learner in learners.items() if 'models' in learner\n",
    "                 for key, clf in learner.get_models().items()]\n",
    "            return pd.concat(L) if len(L)>0 else pd.DataFrame()\n",
    "            # return pd.DataFrame() if L == [] else pd.concat(L).set_index(['forecast_year','learner_year'], append=True)\n",
    "\n",
    "        # def fcn():\n",
    "        #     L = [clf.prediction(*self.get_prepared()[key]).assign(forecast_year=self.year, learner_year=learner.year) for year, learner in learners.items() if 'models' in learner and self.year!=learner.year for key, clf in learner.get_models().items()]\n",
    "        #     return pd.DataFrame() if L == [] else pd.concat(L).set_index(['forecast_year','learner_year'], append=True)\n",
    "\n",
    "            # return pd.concat([\n",
    "            #     clf.prediction(*self.get_prepared()[key])\n",
    "            #     .assign(forecast_year=self.year, learner_year=learner.year)\n",
    "            #     .set_index(['forecast_year','learner_year'], append=True)\n",
    "            # for year, learner in learners.items() if 'models' in learner and self.year!=learner.year for key, clf in learner.get_models().items()])\n",
    "        df, new = self.get(fcn, f'predictions', self.get_prepared)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_forecasts(self):\n",
    "        def fcn():\n",
    "            return {key:\n",
    "                self.get_predictions()\n",
    "                .assign(forecast=lambda Z: Z['prediction']*Z['mlt'])\n",
    "                .groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key))\n",
    "                [['forecast']].sum()\n",
    "                for key in self.agg}\n",
    "            # def fcn1(Z):\n",
    "            #     return (Z['prediction']*Z['mlt']).sum()\n",
    "            # df = self.get_predictions().copy()\n",
    "            # df['prediction'] *= df['mlt']\n",
    "            # return df.groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key))['prediction'].sum()\n",
    "            # return self.get_predictions().groupby(unique('crse_code','prediction_year','learner_year',*self.idx,key)).apply(fcn1)\n",
    "        dct, new = self.get(fcn, f'forecasts', suffix='pickle')\n",
    "        return dct\n",
    "\n",
    "\n",
    "\n",
    "    # def get_predictions(self, learners):\n",
    "    #     def fcn():\n",
    "    #         return [\n",
    "    #             pd.concat([clf.prediction(*self.get_prepared()[key]) for key, clf in learner.models.items()])\n",
    "    #             .assign(forecast_year=self.year, learner_year=learner.year)\n",
    "    #             .set_index(['forecast_year','learner_year'], append=True)\n",
    "    #         for year, learner in learners.items() if 'models' in learner]\n",
    "    #     lst, new = self.get(fcn, f'predictions', suffix='.pickle')\n",
    "    #     return lst\n",
    "\n",
    "    # def get_predictions(self, learners):\n",
    "    #     def fcn():\n",
    "    #         return {year:\n",
    "    #             pd.concat([clf.prediction(*self.get_prepared()[key]).assign(forecast_year=self.year, learner_year=learner.year)\n",
    "                \n",
    "    #             for key, clf in learner.models.items()])\n",
    "    #         for year, learner in learners.items() if 'models' in learner}\n",
    "    #         # for year, learner in learners.items() if year < max(learners.keys())}\n",
    "    #     dct, new = self.get(fcn, f'predictions', suffix='.pickle')\n",
    "    #     return dct\n",
    "\n",
    "\n",
    "    # def get_forecasts(self, learners):\n",
    "    #     def fcn():\n",
    "    #         L = list()\n",
    "    #         for year, learner in learners.items():\n",
    "    #             if year < max(years):\n",
    "    #                 for key, clf in learner.get_models().items():\n",
    "    #                     y = clf.prediction(self.get_prepared()[key])\n",
    "    #                     s = pd.Series({\n",
    "    #                         'forecast': round(y['forecast'].sum() * learner.get_enrollments().loc[learner.crse_code,styp]['mlt']),\n",
    "    #                         'styp_code': styp,\n",
    "    #                         'forecast_year': self.year,\n",
    "    #                         'learner_year': learner.year,\n",
    "    #                         }).to_frame().T.set_index(['styp_code','forecast_year','learner_year'])\n",
    "    #                     if self.year < max(years):\n",
    "    #                         s['actual'] = self.get_enrollments().loc[self.crse_code,styp]['stable']\n",
    "    #                         s['error'] = s['forecast'] - s['actual']\n",
    "    #                         s['error_pct'] = round(s['error'] / s['actual'] * 100, 2)\n",
    "    #                     # display(s.prep())\n",
    "    #                     # assert 1==2\n",
    "    #                     L.append(s.prep())\n",
    "    #         df = pd.concat(L)\n",
    "    #         return df\n",
    "    #     df, new = self.get(fcn, f'forecasts')\n",
    "    #     return df\n",
    "\n",
    "\n",
    "\n",
    "# years = [2022,2023,2024,2025]\n",
    "years = [2024,2025]\n",
    "years = listify(years)\n",
    "amps = {year: AMP(\n",
    "        date='03-28',\n",
    "        crse_code='_headcnt',\n",
    "        time_budget=10,\n",
    "        year=year,\n",
    "        overwrite={\n",
    "            'flags',\n",
    "            'admissions',\n",
    "            'students',\n",
    "            'registrations',\n",
    "            'prepared',\n",
    "            'enrollments',\n",
    "            'multipliers',\n",
    "            'models',\n",
    "            'predictions'\n",
    "            'forecasts',\n",
    "            },\n",
    "    ) for year in years}\n",
    "\n",
    "for self in amps.values():\n",
    "    self.get_predictions(amps)\n",
    "\n",
    "\n",
    "# F = pd.concat([self.forecasts for self in amp.values()]).sort_index(ascending=[True, False,False])\n",
    "# F.to_csv(data/f'forecast\n",
    "\n",
    "# self = amps[2024]\n",
    "# self.get_predictions()\n",
    "# self.get_forecasts()\n",
    "# self.get_admissions()\n",
    "# self.current.get_registrations(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757b7785-bf82-40da-a29f-9551008c1022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2025]\n",
    "# self.get_prepared().keys()\n",
    "self.stable.get_registrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c20fb8c-228d-42fb-be44-03256f0b5c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "self.get_prepared().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162de051-5fe6-40a0-9b89-d2551685dfb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_prepared().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691d1456-4b23-4791-a794-5da8a7f4b375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_enrollments()\n",
    "# self.get_multipliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398fc63a-77cb-45c5-9b8d-61be787dd195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_enrollments().groupby(['crse_code','styp_code'])['credit_hr'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f538472-6db5-464c-ae30-23c78cd74407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "self.stable.overwrite\n",
    "self.stable.get_registrations()\n",
    "# self.get_multipliers().loc['_anycrse','n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294c13f2-1509-4a64-9997-c52e88784398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "A = self.get_predictions().copy()\n",
    "A['prediction'] *= A['mlt']\n",
    "key='camp_desc'\n",
    "g = A.groupby(unique('crse_code','prediction_year','learner_year',*self.grp,key))\n",
    "df = g[['prediction']].sum()#.round()\n",
    "err = g[['log_loss','mae']].mean()\n",
    "\n",
    "if 'models' in self:\n",
    "    df = df.join(self.get_enrollments()['actual'])\n",
    "    df['error'] = df['prediction'] - df['actual']\n",
    "    df['error%'] = (df['error'] / df['actual'] * 100).round(2)\n",
    "    df = df.join(err)\n",
    "\n",
    "df.prep().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859218f4-9fad-49e4-9b1a-bb2d93569789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amps[2024]\n",
    "self.get_enrollments()\n",
    "key = 'crse_code'\n",
    "df = self.get_predictions().copy()\n",
    "df['prediction'] *= df['mlt']\n",
    "df['error'] = np.abs(df['error'])\n",
    "# df = df.groupby(unique('crse_code','prediction_year','learner_year',*self.grp,key))[['prediction']].sum().round()\n",
    "\n",
    "\n",
    "df = df.groupby(unique('crse_code','prediction_year','learner_year',*self.grp,key)).agg({'log_loss':'mean', 'error':'mean', 'prediction':'sum'})\n",
    "\n",
    "if 'models' in self:\n",
    "    df = df.join(self.get_enrollments()['actual'])\n",
    "    df['error'] = df['prediction'] - df['actual']\n",
    "    df['error%'] = (df['error'] / df['actual'] * 100).round(2)\n",
    "df.prep()\n",
    "\n",
    "\n",
    "# def fcn1(Z):\n",
    "#     # m = self.get_enrollments().loc[Z.name[0],Z.name[3]])\n",
    "#     return Z[['prediction']].sum()\n",
    "# key = 'styp_code'\n",
    "# A = df.groupby(unique('crse_code','prediction_year','learner_year','styp_code',key)).apply(fcn1)\n",
    "# B = A.join(E)\n",
    "# B['prediction'] *= B['mlt']\n",
    "# B\n",
    "# E.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3dd875-6d16-4fc2-9353-07192e003d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "E = pd.concat(self.get_enrollments() for self in amps.values())\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840e340d-1f29-4ddd-944b-7ef75bacb4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_enrollments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76488eff-2493-44b8-8e3a-7753f3d6f92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "L=[3,1,1,2,1]\n",
    "def unique(*args):\n",
    "    return listify(dict.fromkeys(listify(*args)))\n",
    "\n",
    "# list(dict.fromkeys(L))\n",
    "unique(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a19767-a6b0-44e7-8995-dad46cd74683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "A = amps[2024].get_students()\n",
    "def f(X):\n",
    "    print(X.name)\n",
    "    return X['cycle_day'].max()\n",
    "A.groupby(['styp_desc','term_desc']).apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6804599c-07ab-4f28-b4d0-56743ff714fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.predictions[2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2906e5-df7c-4e04-b548-89101ff1195f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amps[2024].current.get_registrations()#.loc[self.crse_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da84207c-37d8-4e91-a8ad-3403cdbd1778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.prepared['n'][0]#['current'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7246ff4-f368-46f2-8e0e-9ac401c66ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    print(x+y)\n",
    "L = [1,2]\n",
    "f(*L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22cf9175-99ab-4d95-8c02-8147930caf08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "self = amp[2024]\n",
    "# self.get_prepared()['n'][0]\n",
    "self.get_enrollments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4486cd-4e87-4b7e-82c1-8a7bea8a1bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self = amp[2024]\n",
    "# self.year\n",
    "# # self.get_admissions()\n",
    "# self.get_students()['drivetime'].isnull().mean()\n",
    "\n",
    "# self.current.get_students().merge(self.get_registrations().query(f\"crse_code=='_allcsre'\")[['pidm','credit_hr']])\n",
    "# self.current.get_students().merge(self.get_registrations().query(f\"crse_code=='_allcrse'\")[['pidm','credit_hr']], how='left')\n",
    "# .join(self.current.get_registrations().query(f\"crse_code=='_allcsre'\")['credit_hr'])\n",
    "self.get_prepared()['n'][0]['current'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfd39562-b156-4beb-b00d-e50a9765d622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## useful old code\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#                 'ssbsect_crn':'crn',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "\n",
    "#         S = self.students[['pidm','term_code','styp_code']]\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "    #     def fcn():\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     sfrstcr_pidm as pidm\n",
    "#     ,sfrstcr_term_code as term_code\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     --and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     sfrstcr_pidm\n",
    "#     ,sfrstcr_term_code\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# don't delete - could be useful & was hard to create\n",
    "            # stat_codes = ['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY'] # not AK & HI b/c can't get driving distance\n",
    "#     ,{get_desc('spraddr_cnty_code')[0]}\n",
    "#     ,{get_desc('spraddr_stat_code')[0]}\n",
    "#     ,zip_code\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#         ,try_to_number(left(spraddr_zip, 5), '00000') as zip_code\n",
    "#         ,case\n",
    "#             when spraddr_atyp_code = 'PA' then 6\n",
    "#             when spraddr_atyp_code = 'PR' then 5\n",
    "#             when spraddr_atyp_code = 'MA' then 4\n",
    "#             when spraddr_atyp_code = 'BU' then 3\n",
    "#             when spraddr_atyp_code = 'BI' then 2\n",
    "#             when spraddr_atyp_code = 'P1' then 1\n",
    "#             when spraddr_atyp_code = 'P2' then 0\n",
    "#             end as spraddr_atyp_rank\n",
    "#     from\n",
    "#         {catalog}spraddr_amp_v\n",
    "#     where\n",
    "#         spraddr_stat_code in ('{join(stat_codes, \"','\")}')\n",
    "#         and spraddr_zip is not null\n",
    "#     qualify\n",
    "#         row_number() over (partition by spraddr_pidm order by spraddr_atyp_rank desc, spraddr_seqno desc) = 1\n",
    "# )\n",
    "# on\n",
    "#     pidm = spraddr_pidm\n",
    "\n",
    "# {get_desc('spraddr_cnty_code')[1]}\n",
    "# {get_desc('spraddr_stat_code')[1]}\n",
    "\n",
    "\n",
    "\n",
    "    # def get_zips(self, show=False):\n",
    "    #     \"\"\"takes ~3 hours toget zip codes and find nearest point on road network to the provided representative point\"\"\"\n",
    "    #     def fcn():\n",
    "    #         from pgeocode import Nominatim\n",
    "    #         nomi = Nominatim('us')\n",
    "    #         df = nomi.query_postal_code(pd.Series(nomi._data['postal_code'])).query(\"state_code.notnull() & state_code not in ['AK', 'HI', 'MH']\").prep().set_index('postal_code').rename_axis('zip')\n",
    "    #         nearest = lambda x: join(requests.get(f\"http://router.project-osrm.org/nearest/v1/driving/{x['longitude']},{x['latitude']}\").json()['waypoints'][0]['location'],',')\n",
    "    #         df['point'] = df.apply(nearest, axis=1)\n",
    "    #         return df\n",
    "    #     df, new = self.get(fcn, root/'zips')\n",
    "    #     self.states = set(df['state_code'])\n",
    "    #     return df\n",
    "\n",
    "\n",
    "    # def get_drivetimes(self, show=False):\n",
    "    #     def fcn():\n",
    "    #         campus_coords = {\n",
    "    #             's': [-98.215784,32.216217],\n",
    "    #             # 'm': '-97.432975,32.582436',\n",
    "    #             # 'w': 76708,\n",
    "    #             # 'r': 77807,\n",
    "    #             }\n",
    "\n",
    "    #         url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "    #         gdf = gpd.read_file(url).prep().set_index('zcta5ce20').iloc[:5]\n",
    "    #         pts = gdf.sample_points(size=5,method=\"uniform\").explode()#.apply(lambda geom: f\"{geom.x},{geom.y}\")\n",
    "    #         df = pts.to_frame()[[]]\n",
    "    #         url = \"http://router.project-osrm.org/table/v1/driving\"\n",
    "    #         headers = {\"Content-Type\": \"application/json\"}\n",
    "    #         for k, v in campus_coords.items():\n",
    "    #             u = [v, *pts]\n",
    "    #             print(u)\n",
    "    #             data = {\n",
    "    #                 \"coordinates\": u,\n",
    "    #                 \"annotations\": [\"duration\", \"distance\"],\n",
    "    #                 \"sources\": 0,\n",
    "    #             }\n",
    "    #             response = requests.post(url, json=data, headers=headers)\n",
    "    #             print(response.json())\n",
    "    #             assert 1==2\n",
    "\n",
    "            # for k, v in campus_coords.items():\n",
    "            #     u = join([v, *pts], ';')\n",
    "            #     url = f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={0}&annotations=duration,distance\"\n",
    "            #     print(url)\n",
    "            #     print(requests.get(url))\n",
    "            #     df[k] = np.squeeze(requests.get(url).json()['durations'])[1:]/60\n",
    "    #         # df.disp(10)\n",
    "    #         # df = df.groupby('zip').min()\n",
    "    #         # df.disp(10)\n",
    "    #         df = df.groupby(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "    #         return df\n",
    "\n",
    "            # df = self.zips.iloc[34339:34349].copy()\n",
    "            # u = join(df.apply(lambda x: f\"{x['longitude']},{x['latitude']}\", axis=1),';')\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     df[k] = np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={df.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['distances'])/1609\n",
    "\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     df[k] = np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={df.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['durations'])/60\n",
    "\n",
    "            # self.zips = self.zips.iloc[34339:34349]\n",
    "            # self.zips.disp(20)\n",
    "            # u = join(self.zips['point'],';')\n",
    "            \n",
    "            # dct = dict()\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     i = self.zips.index.get_loc(z)\n",
    "            #     print(self.zips.iloc[i])\n",
    "            #     url = f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={0}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     print(url)\n",
    "            #     response = requests.get(url).json()\n",
    "            #     for k,v in response.items():\n",
    "            #         print(k)\n",
    "            #         display(v)\n",
    "            #         print()\n",
    "            #     print(response['distance'])\n",
    "            #     dct[k] = np.squeeze(response['durations'])\n",
    "\n",
    "            # # dct = {k: np.squeeze(\n",
    "            # #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={self.zips.index.get_loc(z)}&fallback_speed=600&fallback_coordinate=snapped\"\n",
    "            # #     ).json()['durations'])/60 for k, z in campus_zips.items()}\n",
    "            # dct = {k: np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={self.zips.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['distances']) for k, z in campus_zips.items()}\n",
    "\n",
    "            # df = pd.DataFrame(dct, index=self.zips.index).stack().rename_axis(['zip','camp_code']).rename('drivetime') / 1609\n",
    "            # return df\n",
    "            # print()\n",
    "            # dct = {k: self.zips.loc[y] for k, y in {\n",
    "            #     's': 76402,\n",
    "            #     'm': 76036,\n",
    "            #     'w': 76708,\n",
    "            #     'r': 77807,\n",
    "            #     }.items()}\n",
    "            # L = [\n",
    "            #     self.get(\n",
    "            #         lambda: X.apply(get_driving_distance, y=y, axis=1).rename('distance').reset_index().assign(camp_code=k),\n",
    "            #         root/f'distances/distances_{s}_{k}',\n",
    "            #         divide=False,\n",
    "            #     )[0] for s, X in self.zips.groupby('state_code') for k, y in dct.items()]\n",
    "            # return pd.concat(L, ignore_index=True)\n",
    "        # df, new = self.get(fcn, root/'drivetimes')#, self.get_zips)\n",
    "        # return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def get_flags_history(self, cutoff=202206):\n",
    "    #     def fcn():\n",
    "    #         import pyarrow.parquet as pq\n",
    "    #         print()\n",
    "    #         L = []\n",
    "    #         for path in sorted(flags_prc.iterdir(), reverse=True):\n",
    "    #             print(path)\n",
    "    #             for src in path.iterdir():\n",
    "    #                 _, term_code, cycle_date, cycle_day = src.stem.split('_')\n",
    "    #                 col = pq.ParquetFile(src).schema.names\n",
    "    #                 df = pd.DataFrame(columns=col).assign(term_code=[int(term_code)], cycle_date=[cycle_date]).fillna(True)\n",
    "    #                 L.append(df)\n",
    "    #         df = pd.concat(L).fillna(False).set_index(['cycle_date','term_code']).sort_index()\n",
    "    #         return df[sorted(df.columns)]\n",
    "    #     df, new = self.get(fcn, path=data/'flags_history')\n",
    "    #     A = df.query(f'term_code>={cutoff}').groupby('term_code').sum().sort_index(ascending=False).T.rename_axis('variable')\n",
    "    #     B = A == A.max()\n",
    "    #     B.insert(0, 'n', B.sum(axis=1))\n",
    "    #     return B.reset_index().sort_values(['n', 'variable'], ascending=[False, True])\n",
    "\n",
    "\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "# [warnings.filterwarnings(action='ignore', message=f\".*{w}.*\") for w in [\n",
    "#     \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "#     \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "#     \"errors='ignore' is deprecated\"\n",
    "#     \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "#     \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "#     \"DataFrame is highly fragmented\",\n",
    "# ]]\n",
    "\n",
    "\n",
    "\n",
    "# for fore in amp.values():\n",
    "#     for base in amp.values():\n",
    "#         if base.year < max(years):\n",
    "#             for styp, clf in base.get_models().items():\n",
    "#                 for k in ['predictions','headcounts']:\n",
    "#                     fore.__dict__.setdefault(k, dict()).setdefault(styp, dict())\n",
    "#                 y = clf.prediction(fore.get_prepared()[styp])\n",
    "#                 fore.predictions[styp][base.year] = y\n",
    "#                 s = (\n",
    "#                     (y[['pred']].sum() * base.get_enrollments().loc[base.crse_code,styp]['mlt']).round()\n",
    "#                     .rename(base.year).to_frame().T.rename_axis('base_year')\n",
    "#                     .assign(styp_code=styp, forecast_year=fore.year)\n",
    "#                     .reset_index().set_index(['styp_code','forecast_year','base_year'])\n",
    "#                 )\n",
    "#                 if fore.year < max(years):\n",
    "#                     s['true'] = fore.get_enrollments().loc[base.crse_code,styp]['stable']\n",
    "#                     s['error'] = s['pred'] - s['true']\n",
    "#                     s['error_pct'] = round(s['error'] / s['true'] * 100, 2)\n",
    "#                 fore.headcounts[styp][base.year] = s.prep()\n",
    "#     fore.forecasts = {styp: pd.concat(v.values()) for styp, v in fore.headcounts.items()}\n",
    "# amp[2023].forecasts['n']\n",
    "\n",
    "\n",
    "\n",
    "# def get_desc(code):\n",
    "#     for nm in code.split('_'):\n",
    "#         if len(nm) == 4:\n",
    "#             break\n",
    "#     return [f'{code} as {nm}_code, stv{nm}_desc as {nm}_desc', f'left join {catalog}saturnstv{nm} on {code} = stv{nm}_code']\n",
    "\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     pidm\n",
    "#     ,{self.cycle_day} as cycle_day\n",
    "#     ,timestamp('{self.cycle_date}') as cycle_date\n",
    "#     ,current_date\n",
    "#     ,first_date\n",
    "#     ,final_date\n",
    "#     ,{get_desc('term_code')[0]}\n",
    "#     ,appl_no\n",
    "#     ,{get_desc('apst_code')[0]}\n",
    "#     ,{get_desc('apdc_code')[0]}\n",
    "#     ,{get_desc('admt_code')[0]}\n",
    "#     ,{get_desc('wrsn_code')[0]}\n",
    "#     ,{get_desc('levl_code')[0]}\n",
    "#     ,{get_desc('styp_code')[0]}\n",
    "#     ,{get_desc('camp_code')[0]}\n",
    "#     ,{get_desc('coll_code_1')[0]}\n",
    "#     ,{get_desc('dept_code')[0]}\n",
    "#     ,{get_desc('majr_code_1')[0]}\n",
    "#     ,{get_desc('saradap_resd_code')[0]}\n",
    "#     ,gender\n",
    "#     ,birth_date\n",
    "#     ,{get_desc('spbpers_lgcy_code')[0]}\n",
    "#     ,gorvisa_vtyp_code is not null as international\n",
    "#     ,gorvisa_natn_code_issue as natn_code, stvnatn_nation as natn_desc\n",
    "#     ,{coalesce('race_asian')}\n",
    "#     ,{coalesce('race_black')}\n",
    "#     ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "#     ,{coalesce('race_native')}\n",
    "#     ,{coalesce('race_pacific')}\n",
    "#     ,{coalesce('race_white')}\n",
    "#     ,hs_percentile\n",
    "#     ,sbgi_code\n",
    "#     ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "# from {subqry(qry)} as A\n",
    "\n",
    "# left join\n",
    "#     {catalog}spbpers_amp_v\n",
    "# on\n",
    "#     pidm = spbpers_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}generalgorvisa\n",
    "#         --{catalog}gorvisa_amp_v\n",
    "#     qualify\n",
    "#         row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorvisa_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         gorprac_pidm\n",
    "#         ,max(gorprac_race_cde='AS') as race_asian\n",
    "#         ,max(gorprac_race_cde='BL') as race_black\n",
    "#         ,max(gorprac_race_cde='IN') as race_native\n",
    "#         ,max(gorprac_race_cde='HA') as race_pacific\n",
    "#         ,max(gorprac_race_cde='WH') as race_white\n",
    "#     from\n",
    "#         {catalog}generalgorprac\n",
    "#     group by\n",
    "#         gorprac_pidm\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorprac_pidm\n",
    "\n",
    "# {get_desc('term_code')[1]}\n",
    "# {get_desc('levl_code')[1]}\n",
    "# {get_desc('styp_code')[1]}\n",
    "# {get_desc('admt_code')[1]}\n",
    "# {get_desc('wrsn_code')[1]}\n",
    "# {get_desc('apst_code')[1]}\n",
    "# {get_desc('apdc_code')[1]}\n",
    "# {get_desc('camp_code')[1]}\n",
    "# {get_desc('coll_code_1')[1]}\n",
    "# {get_desc('dept_code')[1]}\n",
    "# {get_desc('majr_code_1')[1]}\n",
    "# {get_desc('saradap_resd_code')[1]}\n",
    "# {get_desc('gorvisa_natn_code_issue')[1]}\n",
    "# {get_desc('spbpers_lgcy_code')[1]}\n",
    "\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "#     and ssbsect_credit_hrs > 0\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             grp = join([\n",
    "#                 'pidm',\n",
    "#                 'term_code','term_desc',\n",
    "#                 'levl_code','levl_desc',\n",
    "#                 'styp_code','styp_desc',\n",
    "#                 ], ', ')\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     sfrstcr_pidm as pidm\n",
    "#     ,{get_desc('ssbsect_term_code')}\n",
    "#     ,{get_desc('sgbstdn_levl_code')}\n",
    "#     ,{get_desc('sgbstdn_styp_code')}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term parts\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "#     and ssbsect_credit_hrs > 0\n",
    "# group by\n",
    "#     {grp}, crse_code\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with CTE as {subqry(qry)}\n",
    "\n",
    "# --individual courses\n",
    "# select\n",
    "#     *\n",
    "# from\n",
    "#     CTE\n",
    "\n",
    "# union all\n",
    "\n",
    "# --total credit hours\n",
    "# select\n",
    "#     {grp}\n",
    "#     ,'_total_sch' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     {grp}\n",
    "\n",
    "# union all\n",
    "\n",
    "# --headcount \n",
    "# select\n",
    "#     {grp}\n",
    "#     ,'_headcount' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     {grp}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8185768213204940,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AMP_2025",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
