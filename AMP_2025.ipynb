{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66d214e-5b29-4cd7-b18a-090abac60882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userlastname = 'cook'\n",
    "from IPython.display import clear_output, display\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming Jinja2 openpyxl numpy pandas geopandas scikit-learn pgeocode flaml[automl] git+https://github.com/AnotherSamWilson/miceforest.git\n",
    "    dbutils.library.restartPython()\n",
    "    clear_output()\n",
    "    dbutils.notebook.exit('Rerun to use newly installed/updated packages')\n",
    "\n",
    "import os, sys, copy, pathlib, shutil, pickle, warnings, requests, dataclasses, time, codetiming, numpy as np, pandas as pd, sklearn as sk, geopandas as gpd, pgeocode, miceforest as mf, flaml as fl\n",
    "from pgeocode import Nominatim\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "clear_output()\n",
    "pd.options.display.max_columns = None\n",
    "sk.set_config(transform_output=\"pandas\")\n",
    "now = pd.Timestamp.now()\n",
    "eps = np.finfo(float).eps\n",
    "tab = '    '\n",
    "divider = '##############################################################################################################'\n",
    "catalog = 'dev.bronze.'\n",
    "root = pathlib.Path(f'/Volumes/aiml/amp')\n",
    "geo = root/f'amp_cook_files/202508/geo'\n",
    "shr = root/f'amp_cook_files/202508/data'\n",
    "usr = root/f'amp_{userlastname}_files/202508/output'\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sk.exceptions.InconsistentVersionWarning)\n",
    "for w in [\n",
    "    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "    ]:\n",
    "    warnings.filterwarnings(action='ignore', message=f\".*{w}.*\")\n",
    "\n",
    "\n",
    "############ helper functions ############\n",
    "def dt(*args):\n",
    "    return pd.to_datetime(args).dropna().min().normalize()\n",
    "\n",
    "def setmeth(cls, fcn):\n",
    "    \"\"\"monkey-patch new method into a mutable class (fails for immutable class)\"\"\"\n",
    "    setattr(cls, fcn.__name__, fcn)\n",
    "\n",
    "def listify(*args, sort=False, reverse=False):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        L = list(*args)\n",
    "    except Exception as e:\n",
    "        L = list(args)\n",
    "    if sort:\n",
    "        try:\n",
    "            L = sorted(L, reverse=reverse) \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return L\n",
    "\n",
    "def setify(*args):\n",
    "    \"\"\"ensure it is a set\"\"\"\n",
    "    return set(listify(*args))\n",
    "\n",
    "def unpack(*args, **kwargs):\n",
    "    L = [y for x in args for y in (unpack(*x) if isinstance(x, (list,tuple,set)) else listify(x))]\n",
    "    return listify(L, **kwargs)\n",
    "\n",
    "def unique(*args, **kwargs):\n",
    "    L = dict.fromkeys(unpack(*args))\n",
    "    return listify(L, **kwargs)\n",
    "\n",
    "def difference(A, B, **kwargs):\n",
    "    return unique([x for x in listify(A) if x not in listify(B)], **kwargs)\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    qry = '\\n' + qry.strip()\n",
    "    qry = '(' + qry + '\\n)' if 'select' in qry else qry\n",
    "    return indent(qry, lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=42):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep().sort_index()\n",
    "\n",
    "\n",
    "############ pandas functions ############\n",
    "def disp(X, max_rows=3, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    print(type(X), X.shape)\n",
    "    X = (X.sort_index(axis=1) if sort else X).reset_index()\n",
    "    Y = pd.DataFrame({'dtype':X.dtypes.astype('string'), 'missing_pct':X.isnull().mean()*100}).T.rename_axis('column').reset_index().prep(case='')\n",
    "    print(X.shape)\n",
    "    display(Y)\n",
    "    display(X.head(max_rows))\n",
    "\n",
    "# def disp(X, max_rows=3, precision=None, sort=False, **props):\n",
    "# def disp(X, max_rows=3, sort=False):#, precision=3, **props):\n",
    "#     \"\"\"convenient display method\"\"\"\n",
    "#     X = (X.sort_index(axis=1) if sort else X).reset_index()\n",
    "#     Y = pd.DataFrame({'dtype':X.dtypes.astype('string'), 'missing_pct':X.isnull().mean()*100}).T.rename_axis('column').prep(case='')\n",
    "#     print(X.shape)\n",
    "#     display(Y)\n",
    "#     display(X.head(max_rows))\n",
    "    # X = X.head(max_rows)\n",
    "    # display(X)\n",
    "    # props = {\n",
    "    #     'text-align': 'center',\n",
    "    #     'vertical-align': 'top',\n",
    "    #     'border': '1px dotted black',\n",
    "    #     'width': 'auto',\n",
    "    #     'font-size': '16px',\n",
    "    #     } | props\n",
    "    # fmt = {'precision': precision, 'hyperlinks': 'html'}\n",
    "    # # display(X.head(max_rows).reset_index())\n",
    "    # # display(X)\n",
    "    # display(X.style\n",
    "    #     .format(**fmt)\n",
    "    #     # .format_index(**fmt, axis=0)\n",
    "    #     # .format_index(**fmt, axis=1)\n",
    "    #     # .set_table_styles([{'selector':k, 'props':[*props.items()]} for k in ['th','td']])\n",
    "    #     # .set_table_attributes('style=\"border-collapse: collapse\"')\n",
    "    # )\n",
    "    # assert 1==2\n",
    "\n",
    "def to_numeric(df, case='lower', downcast='integer', errors='ignore', category=False, **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        case = case if case in dir(pd.Series().str) else 'strip'\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: getattr(s.astype('string').str.strip().str,case)() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s.astype('category') if s.dtype=='string' and category else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "        idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "        return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def groupb(df, by=None, **kwargs):\n",
    "    \"\"\"my preferred defaults for groupby\"\"\"\n",
    "    kwargs = {'axis':0,'level':None,'as_index':True,'sort':False,'group_keys':False,'observed':False,'dropna':False}|kwargs\n",
    "    return df.groupby(by, **kwargs)\n",
    "\n",
    "def get_incoming(df):\n",
    "    return df.query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    "\n",
    "def get_duplicates(df, subset='pidm', quit=True, rows=10):\n",
    "    mask = df.groupb(subset, sort=True).transform('size') > 1\n",
    "    if mask.any():\n",
    "        df[mask].disp(rows)\n",
    "        if quit:\n",
    "            raise Exception(f'{mask.sum()} duplicates detected')\n",
    "    return df[mask]\n",
    "\n",
    "def get_missing(df, rows=-1):\n",
    "    miss = df.isnull().mean()*100\n",
    "    if miss.any():\n",
    "        miss[miss>0].sort_values(ascending=False).round(1).disp(rows)\n",
    "    return miss\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    wrapper.__name__ = fcn.__name__\n",
    "    return wrapper\n",
    "\n",
    "for fcn in [\n",
    "    disp,\n",
    "    to_numeric,\n",
    "    prep,\n",
    "    get_incoming,\n",
    "    get_duplicates,\n",
    "    get_missing,\n",
    "    groupb,\n",
    "    ]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setmeth(pd.DataFrame, fcn)\n",
    "    setmeth(pd.Series, wrap(fcn))\n",
    "\n",
    "############ file i/o functions ############\n",
    "def get_size(path):\n",
    "    os.system(f'du -h {path}')\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "def mkdir(path):\n",
    "    path = pathlib.Path(path)\n",
    "    (path if path.suffix == '' else path.parent).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def reset(path):\n",
    "    rm(path)\n",
    "    mkdir(path)\n",
    "    return path\n",
    "\n",
    "def prepr(X):\n",
    "    if isinstance(X, (pd.DataFrame,pd.Series)):\n",
    "        return X.prep()\n",
    "    elif isinstance(X, dict):\n",
    "        return {k: prepr(v) for k, v in X.items()}\n",
    "    elif isinstance(X, (list,tuple,set)):\n",
    "        return type(X)(prepr(v) for v in X)\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def dump(path, obj):\n",
    "    path = reset(path)\n",
    "    obj = prepr(obj)\n",
    "    if path.suffix == '.parquet':\n",
    "        pd.DataFrame(obj).to_parquet(path)  # forced to wrap with explicit pd.DataFrame to due strange error under pandas 2.2.3 \"Object of type PlanMetrics is not JSON serializable\" with to_parquet\n",
    "    elif path.suffix == '.csv':\n",
    "        pd.DataFrame(obj).to_csv(path)\n",
    "    else:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return obj\n",
    "\n",
    "def load(path):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.suffix == '.parquet':\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.suffix == '.csv':\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def get_desc(code):\n",
    "    for nm in code.split('_'):\n",
    "        if len(nm) == 4:\n",
    "            break\n",
    "    return f'{code} as {nm}_code, (select stv{nm}_desc from {catalog}saturnstv{nm} where {code} = stv{nm}_code limit 1) as {nm}_desc'\n",
    "\n",
    "def coalesce(x, y=False):\n",
    "    return f'coalesce({x}, {y}) as {x}'\n",
    "\n",
    "races = [f'race_{r}' for r in ['asian','black','hispanic','native','pacific','white']]\n",
    "\n",
    "def prediction(clf, X, y, cross=False):\n",
    "    Z = X.copy()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        p = (cross_val_predict(clf.model, X, y, cv=min(10,y.sum()), method='predict_proba') if cross and y.sum()>1 else clf.predict_proba(X)).T[1]\n",
    "    return pd.DataFrame({'prediction': p, 'actual': y, 'error': p-y, 'cv_score':clf.best_loss})\n",
    "setmeth(fl.automl.automl.AutoML, prediction)\n",
    "\n",
    "\n",
    "def custom_log_loss(X_val, y_val, estimator, labels, X_train, y_train, weight_val=None, weight_train=None, config=None, groups_val=None, groups_train=None):\n",
    "    \"\"\"Some (crse,styp) are entirely False which causes an error with built-in log_loss. We create a custom_log_loss simply to set labels=[False, True] https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML/\"\"\"\n",
    "    start = time.time()\n",
    "    y_pred = estimator.predict_proba(X_val)\n",
    "    pred_time = (time.time() - start) / len(X_val)\n",
    "    val_loss = log_loss(y_val, y_pred, labels=[False,True], sample_weight=weight_val)\n",
    "    y_pred = estimator.predict_proba(X_train)\n",
    "    train_loss = log_loss(y_train, y_pred, labels=[False,True], sample_weight=weight_train)\n",
    "    return val_loss, {\"val_loss\": val_loss, \"train_loss\": train_loss, \"pred_time\": pred_time}\n",
    "\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "@dataclasses.dataclass\n",
    "class Data():\n",
    "    term_code: int\n",
    "    amp_date: str = ''\n",
    "    overwrite: set = None\n",
    "    seed: int = 42\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.overwrite = setify(self.overwrite)\n",
    "        # Because these take about 1 hour each, force user to manually delete drivetimes parquet files to avoid accidental deletion requring lengthy re-creation\n",
    "        self.overwrite.discard('drivetimes')\n",
    "        self.amp_date = dt(self.amp_date, now)\n",
    "        self.amp_date += pd.Timedelta(days=2-self.amp_date.weekday())  # move to closest Wednesday (Flags release)\n",
    "        self.year = self.term_code // 100\n",
    "        self.term_code, self.amp_date, self.amp_day, self.stable_date, self.term_desc, self.stem = self.get_dates(self.term_code, self.amp_date)\n",
    "\n",
    "\n",
    "    def get_dates(self, term_code, current_date):\n",
    "        term_code = int(term_code)\n",
    "        term_desc, stable_date = self.get_terms().loc[term_code,['term_desc','stable_date']]\n",
    "        current_date = dt(current_date).date()\n",
    "        stable_date = dt(stable_date).date()\n",
    "        current_day = (stable_date - current_date).days\n",
    "        stem = f'{current_date}_{term_code}_{\"-\" if current_day < 0 else \"+\"}{rjust(abs(current_day),3,0)}'\n",
    "        return term_code, current_date, current_day, stable_date, term_desc, stem\n",
    "\n",
    "    def get_dst(self, path, nm, suffix='.parquet'):\n",
    "        # nm = file.split('_')[0]\n",
    "        if path in [shr,usr]:\n",
    "            dst = path/f\"{self.amp_date}/{self.term_code}/{nm.split('_')[0]}/{self.stem}_{nm}\"\n",
    "        else:\n",
    "            dst = path/nm\n",
    "        return dst.with_suffix(suffix)\n",
    "\n",
    "\n",
    "    def get(self, fcn, path, nm, suffix='.parquet', prereq=[], divide=True, read=True, **kwargs):\n",
    "        dst = self.get_dst(path, nm, suffix)\n",
    "        if nm in self.overwrite:\n",
    "            del self[nm]\n",
    "            reset(dst)\n",
    "            self.overwrite.remove(nm)\n",
    "\n",
    "        new = False\n",
    "        if not nm in self:\n",
    "            if not dst.exists():\n",
    "                [f() for f in unique(prereq)]\n",
    "                print(f'creating {dst}', end=': ')\n",
    "                with codetiming.Timer():\n",
    "                    self[nm] = dump(dst, fcn(**kwargs))\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "                new = True\n",
    "            elif read:\n",
    "                self[nm] = load(dst)\n",
    "            else:\n",
    "                self[nm] = None\n",
    "        return self[nm], new\n",
    "    # def get_dst(self, path, file, suffix='.parquet'):\n",
    "    #     nm = file.split('_')[0]\n",
    "    #     if path in [shr,usr]:\n",
    "    #         dst = path/f\"{self.amp_date}/{self.term_code}/{nm}/{self.stem}_{file}\"\n",
    "    #     else:\n",
    "    #         dst = path/file\n",
    "    #     return dst.with_suffix(suffix), nm\n",
    "\n",
    "\n",
    "    # def get(self, fcn, path, file, suffix='.parquet', prereq=[], divide=True, read=True, **kwargs):\n",
    "    #     dst, nm = self.get_dst(path, file, suffix)\n",
    "    #     if nm in self.overwrite:\n",
    "    #         del self[nm]\n",
    "    #         reset(dst)\n",
    "    #         self.overwrite.remove(nm)\n",
    "\n",
    "    #     print(nm, dst)\n",
    "    #     new = False\n",
    "    #     if not nm in self:\n",
    "    #         if not dst.exists():\n",
    "    #             print('creating')\n",
    "    #             [f() for f in unique(prereq)]\n",
    "    #             print(f'creating {dst}', end=': ')\n",
    "    #             with codetiming.Timer():\n",
    "    #                 self[nm] = dump(dst, fcn(**kwargs))\n",
    "    #             if divide:\n",
    "    #                 print(divider)\n",
    "    #             new = True\n",
    "    #         elif read:\n",
    "    #             print('file exists')\n",
    "    #             self[nm] = load(dst)\n",
    "    #         else:\n",
    "    #             self[nm] = None\n",
    "    #     return self[nm], new\n",
    "##################################################\n",
    "################# get drivetimes #################\n",
    "##################################################\n",
    "    def get_zips(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pgeocode.Nominatim('us')._data  # get all zips\n",
    "                .prep()\n",
    "                .rename(columns={'postal_code':'zip'})\n",
    "                .query(\"state_code.notnull() & state_code not in [None,'mh']\")\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, geo, 'zips')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        return set(self.get_zips()['state_code'])\n",
    "\n",
    "\n",
    "    def get_drivetimes(self):\n",
    "        def fcn():\n",
    "            print()\n",
    "            campus_coords = {\n",
    "                's': '-98.215784,32.216217',\n",
    "                'm': '-97.432975,32.582436',\n",
    "                'w': '-97.172176,31.587908',\n",
    "                'r': '-96.467920,30.642055',\n",
    "                'l': '-96.983211,32.462267',\n",
    "                }\n",
    "            url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "            gdf = gpd.read_file(url).prep().set_index('zcta5ce20')  # get all ZCTA https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n",
    "            pts = gdf.sample_points(size=10, method=\"uniform\").explode().apply(lambda g: f\"{g.x},{g.y}\")  # sample 10 random points in each ZCTA\n",
    "            M = []\n",
    "            for k, v in campus_coords.items():\n",
    "                def fcn1():\n",
    "                    print()\n",
    "                    L = []\n",
    "                    i = 0\n",
    "                    di = 200\n",
    "                    I = pts.shape[0]\n",
    "                    while i < I:\n",
    "                        u = join([v, *pts.iloc[i:i+di]],';')\n",
    "                        url = f\"http://router.project-osrm.org/table/v1/driving/{u}?sources={0}&annotations=duration,distance&fallback_speed=1&fallback_coordinate=snapped\"\n",
    "                        response = requests.get(url).json()\n",
    "                        L.append(np.squeeze(response['durations'])[1:]/60)\n",
    "                        i += di\n",
    "                        print(k,i,round(i/I*100))\n",
    "                    df = pts.to_frame()[[]]\n",
    "                    df[k] = np.concatenate(L)\n",
    "                    return df\n",
    "                df, new = self.get(fcn1, geo, f'drivetimes_{k}')\n",
    "                M.append(df)\n",
    "            D = pd.concat(M, axis=1).groupb(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "\n",
    "            # There are a few USPS zips without equivalent ZCTA, so we assign them drivetimes for the nearest\n",
    "            Z = self.get_zips().merge(D.query(\"camp_code=='s'\"), how='left').set_index('zip')\n",
    "            mask = Z['drivetime'].isnull()  # zips without a ZTCA\n",
    "            Z = Z[['latitude','longitude']]\n",
    "            X = np.radians(Z[~mask])\n",
    "            Y = np.radians(Z[mask])\n",
    "            M = (\n",
    "                pd.DataFrame(haversine_distances(X, Y), index=X.index, columns=Y.index) # haversine distance between pairs with and without ZCTA\n",
    "                .idxmin()  # find nearest ZCTA\n",
    "                .reset_index()\n",
    "                .set_axis(['new_zip','zip'], axis=1)\n",
    "                .prep()\n",
    "                .merge(D)  # merge the drivetimes for that ZCTA\n",
    "                .drop(columns='zip')\n",
    "                .rename(columns={'new_zip':'zip'})\n",
    "            )\n",
    "            df = pd.concat([D,M], ignore_index=True)\n",
    "            return df\n",
    "        df, new = self.get(fcn, geo, 'drivetimes', prereq=self.get_zips)\n",
    "        return df\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, show=False):\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        df, new = self.get(fcn, geo, 'terms')\n",
    "        return df\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id\n",
    "                ,spriden_pidm as pidm\n",
    "                ,spriden_last_name as last_name\n",
    "                ,spriden_first_name as first_name\n",
    "\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self, early_stop=3):\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        counter = 0\n",
    "        divide = False\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > early_stop:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                current_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    current_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                term_code, current_date, current_day, stable_date, term_desc, stem = self.get_dates(term_code, current_date)\n",
    "                def fcn():\n",
    "                    B = book.parse(sheet).prep()\n",
    "                    # if not B.empty:\n",
    "                    B['id'] = B['id'].to_numeric(errors='coerce')  # CRITICAL step - id is stored as string dtype to allow leading 0's, but this opens the door for serious data entry errors (ex: ID=\"D\") which can have catastrophic effects downstream.  This step convert such issues to null, which get removed during the merge below.\n",
    "                    mask = B['id'].isnull()\n",
    "                    if mask.any():\n",
    "                        print(f'WARNING: {mask.sum()} non-numeric ids')\n",
    "                        B[mask].disp(5)\n",
    "                    df = (\n",
    "                        self.get_spriden()[['pidm','id']]\n",
    "                        .assign(current_date=current_date)#, current_day=current_day)\n",
    "                        .merge(B, on='id', how='inner')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, flags_prc, f'{term_code}/{stem}_flags', read=False, divide=False)[1]:\n",
    "                    divide = True\n",
    "                    counter = 0\n",
    "                    dst = flags_prc/f'{term_code//100}_flags.parquet'\n",
    "                    rm(dst)\n",
    "        if divide:\n",
    "            print(divider)\n",
    "            self.combine_flags()\n",
    "\n",
    "\n",
    "    def combine_flags(self):\n",
    "        def fcn(year):\n",
    "            L = [pd.read_parquet(src) for path in flags_prc.iterdir() if path.is_dir() and str(year) in path.stem for src in path.glob('*.parquet')]\n",
    "            df = pd.concat(L, ignore_index=True).prep()\n",
    "            del L\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                if k in df:\n",
    "                    df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        for year in {int(x.stem)//100 for x in flags_prc.iterdir() if x.is_dir()}:\n",
    "            self.get(fcn, flags_prc, f'{year}_flags', read=False, year=year)\n",
    "\n",
    "\n",
    "    def get_flags(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pd.read_parquet(flags_prc/f'{self.year}_flags.parquet')\n",
    "                .query(f\"current_date<='{self.amp_date}'\")\n",
    "                .sort_values(['pidm','current_date'])\n",
    "                .drop_duplicates(subset=['pidm','term_code'], keep='last')\n",
    "            )\n",
    "            df.loc[~df['state'].isin(self.get_states()),'zip'] = pd.NA\n",
    "            df['zip'] = df['zip'].str.split('-', expand=True)[0].str[:5].to_numeric(errors='coerce')\n",
    "            return df\n",
    "        df, new = self.get(fcn, shr, 'flags', prereq=[self.combine_flags, self.get_states])\n",
    "        return df\n",
    "##########################################\n",
    "############ get student data ############\n",
    "##########################################\n",
    "    def newest(self, qry, prt, tbl='', sel=''):\n",
    "        \"\"\"The OPEIR daily snapshot experienced occasional glitched causing incomplete copies.\n",
    "        Consequently, records can have a \"gap\" where they vanish then reappear later. This function fixes this issue.\"\"\"\n",
    "        prt = join(prt, ', ')\n",
    "        if tbl == '':\n",
    "            tbl = qry\n",
    "        if sel != '':\n",
    "            sel = ','+join(sel)\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    {prt}\n",
    "    ,current_date\n",
    "    ,min(current_date) over (partition by {prt}) as first_date  --first date this record appeared\n",
    "    ,max(current_date) over (partition by {prt}) as last_date  --last date this record appeared\n",
    "    ,least(timestamp('{self.amp_date}'), max(current_date) over ()) as amp_date  --clip amp_date to last date of ANY record\n",
    "    --,least(greatest(timestamp('{self.amp_date}'), min(current_date) over ()), max(current_date) over ()) as amp_date  --clip amp_date between first & last date of ANY record\n",
    "from\n",
    "    {qry.strip()}\n",
    "qualify\n",
    "    amp_date between first_date and dateadd(last_date, 5)  -- keep records where amp_date falls between that record's first & last appearance (+5 days in case we are in a gap right now - the record will reappear but has not yet done so\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    *\n",
    "from {subqry(qry)}\n",
    "where\n",
    "    --current_date <= '{self.amp_date}'  -- discard records after amp_date\n",
    "    current_date <= amp_date  -- discard records after amp_date\n",
    "qualify\n",
    "    row_number() over (partition by {prt} order by current_date desc) = 1  -- keep most recent remaining record\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select distinct\n",
    "    pidm\n",
    "    ,first_date\n",
    "    ,current_date\n",
    "    ,amp_date\n",
    "    ,last_date\n",
    "    ,{get_desc('term_code')}\n",
    "    ,{get_desc('levl_code')}\n",
    "    ,{get_desc('styp_code')}\n",
    "    ,{get_desc('camp_code')}\n",
    "    ,{get_desc('coll_code_1')}\n",
    "    ,{get_desc('dept_code')}\n",
    "    ,{get_desc('majr_code_1')}\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    {indent(sel)}\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {tbl}\n",
    "using\n",
    "    ({prt}, current_date)\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\"\"\"\n",
    "        return qry\n",
    "\n",
    "\n",
    "    def get_registrations(self, show=False):\n",
    "        def fcn():\n",
    "            # tbl = f'dev.opeir.opeirregistration_{self.term_desc}'\n",
    "            tbl = f'dev.opeir.registration_{self.term_desc}_v'\n",
    "            if spark.catalog.tableExists(tbl):\n",
    "                qry = self.newest(\n",
    "                    tbl = tbl,\n",
    "                    prt = ['pidm','crn'],\n",
    "                    sel = ['credit_hr as count', 'subj_code || crse_numb as crse_code'],\n",
    "                    qry = f\"\"\"\n",
    "    {tbl} as A\n",
    "where\n",
    "    credit_hr > 0\n",
    "    and subj_code <> 'INST'\"\"\")\n",
    "                A = run(qry, show)\n",
    "                B = A.groupb(['pidm','crse_code'])['count'].sum().reset_index('crse_code')\n",
    "                C = B.groupb('pidm')[['count']].sum().assign(crse_code='_tot_sch')\n",
    "                D = C.copy()\n",
    "                B['count'] = 1\n",
    "                D['count'] = 1\n",
    "                D['crse_code'] = '_headcnt'\n",
    "                E = D.copy()\n",
    "                E['crse_code'] = '_proba'\n",
    "                F = pd.concat([B,C,D,E])\n",
    "                G = A.drop(columns=['count','crse_code']).sort_values('current_date').groupb('pidm', sort=True).last()\n",
    "                df = G.join(F).sort_index()\n",
    "            else:\n",
    "                # placeholder if table DNE\n",
    "                df = pd.DataFrame(columns=['pidm','levl_code','styp_code','count','crse_code']).set_index('pidm')\n",
    "            df.get_duplicates(['pidm','crse_code'])\n",
    "            return df\n",
    "        df, new = self.get(fcn, shr, 'registrations')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_admissions(self, show=False):\n",
    "        def fcn():\n",
    "            def fcn1(season):\n",
    "                # tbl = f'dev.opeir.opeiradmissions_{season}{self.year}'\n",
    "                tbl = f'dev.opeir.admissions_{season}{self.year}_v'\n",
    "                return self.newest(\n",
    "                    tbl = tbl,\n",
    "                    prt = ['pidm', 'appl_no'],\n",
    "                    sel = [\n",
    "                        'appl_no',\n",
    "                        get_desc('apst_code'),\n",
    "                        get_desc('apdc_code'),\n",
    "                        get_desc('admt_code'),\n",
    "                        get_desc('saradap_resd_code'),\n",
    "                        'hs_percentile',\n",
    "                        # 'sbgi_code',\n",
    "                    ],\n",
    "                    qry = f\"\"\"\n",
    "    {tbl} as A\n",
    "inner join\n",
    "    {catalog}saturnstvapdc as B\n",
    "on\n",
    "    apdc_code = stvapdc_code\n",
    "where\n",
    "    stvapdc_inst_acc_ind is not null  --only accepted\"\"\")\n",
    "            L = [run(fcn1(season), show) for season in ['summer','fall']]\n",
    "            df = pd.concat(L, ignore_index=True)\n",
    "            mask = df.groupb('pidm').apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "            df = df.loc[mask]\n",
    "            df.get_duplicates()\n",
    "            return df\n",
    "        df, new = self.get(fcn, shr, 'admissions')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_students(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                self.get_admissions()\n",
    "                .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_drop'])\n",
    "                .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_drop'])\n",
    "                .prep()\n",
    "            )\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "            df['eager'] = (dt(self.stable_date) - df['first_date']).dt.days\n",
    "            df['age'] = (dt(self.stable_date) - df['birth_date']).dt.days / 365\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "            df.get_duplicates()\n",
    "            M = df.get_incoming().query('current_date_drop.isnull()')\n",
    "            if not M.empty:\n",
    "                print(M.shape)\n",
    "                display(M.sort_values('first_date').head(50))\n",
    "                if M.shape[0] > 10:\n",
    "                    raise Exception('Too many unmatched students')\n",
    "                # display(M[['pidm','amp_date','term_code','camp_code','first_date','last_date','current_date','current_date_drop']].sort_values('first_date').head(50))\n",
    "                # display(M['first_date'].value_counts())\n",
    "                # M[['pidm','amp_date','term_code','first_date','last_date','current_date','current_date_drop']].disp(3)\n",
    "                # M['first_date'].value_counts().disp(-1)\n",
    "            return df.loc[:, ~df.columns.str.contains('_drop')].prep().set_index('pidm').sort_index()\n",
    "        df, new = self.get(fcn, shr, 'students', prereq=[self.get_drivetimes, self.get_flags, self.get_admissions])\n",
    "        return df\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "@dataclasses.dataclass\n",
    "class Model(Data):\n",
    "    is_learner: bool = True\n",
    "    submodels: any = 'styp_desc'\n",
    "    aggregates: list = None\n",
    "    features: dict = None\n",
    "    flaml: dict = None\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.aggregates = unique('crse_code', difference(self.aggregates, self.submodels))\n",
    "        self.flaml = self.flaml if self.flaml is not None else dict()\n",
    "        self.stable = Data(amp_date=self.stable_date, **{k:self[k] for k in ['term_code','overwrite']})\n",
    "\n",
    "\n",
    "    def get_imputed(self):\n",
    "        def fcn():\n",
    "            def fcn1(subpop, df):\n",
    "                X = df.fillna(self.features)[self.features.keys()].prep(category=True)\n",
    "                imp = mf.ImputationKernel(X.reset_index(drop=True), random_state=self.seed)\n",
    "                imp.mice(10)\n",
    "                XX = imp.complete_data().set_index(X.index)\n",
    "                XX.get_missing()\n",
    "                return XX\n",
    "            return {subpop: fcn1(subpop, df) for subpop, df in self.get_students().get_incoming().groupb(self.submodels)}\n",
    "        dct, new = self.get(fcn, usr, 'imputed', '.pkl', prereq=self.get_students)\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def set_crse_code(self, crse_code='_headcnt'):\n",
    "        \"\"\"added to allow a single Model to run many crse_codes (fix out of memory issues)\"\"\"\n",
    "        self.crse_code = crse_code\n",
    "        J = {'prepared','learners','predictions','forecasts'}\n",
    "        self.overwrite |= {f\"{j}_{crse_code}\" for j in J.intersection(self.overwrite)}\n",
    "        for k in {k for j in J for k in self.__dict__.keys() if j in k}:\n",
    "            del self[k]\n",
    "\n",
    "\n",
    "    def get_prepared(self):\n",
    "        def fcn():\n",
    "            g = lambda obj=self, crse=self.crse_code, nm=None: obj.get_registrations().query(f\"crse_code=='{crse}'\")['count'].rename(nm if nm is not None else crse)\n",
    "            Z = {subpop: X\n",
    "                 .join(g(crse='_tot_sch'))\n",
    "                 .join(g(nm='current').astype('boolean'))\n",
    "                 .join(g(nm='stable', obj=self.stable).astype('boolean'))\n",
    "                .fillna({'_tot_sch':0, 'current':False, 'stable':False})\n",
    "                for subpop, X in self.get_imputed().items()}\n",
    "            {subpop: z.get_duplicates() for subpop, z in Z.items()}\n",
    "            return {subpop: [X, X.pop('stable')] for subpop, X in Z.items()}\n",
    "        dct, new = self.get(fcn, usr, f'prepared_{self.crse_code}', '.pkl', prereq=[self.get_imputed,self.get_registrations,self.stable.get_registrations])\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_learners(self):\n",
    "        \"\"\"train model - biggest bottleneck - can we run multiple (crse_code, year) in parallel?\"\"\"\n",
    "        def fcn():\n",
    "            def fcn1(subpop, Z):\n",
    "                log_file = self.get_dst(usr, f'learners_{self.crse_code}_{subpop}', suffix='.log')\n",
    "                reset(log_file)\n",
    "                dct = {\n",
    "                    'time_budget':30,\n",
    "                    # 'max_iter': 100,\n",
    "                    'task':'classification',\n",
    "                    'log_file_name': log_file,\n",
    "                    'log_type': 'all',\n",
    "                    'log_training_metric':True,\n",
    "                    'verbose':0,\n",
    "                    'metric':custom_log_loss,\n",
    "                    'eval_method':'cv',\n",
    "                    'n_splits':3,\n",
    "                    'seed':self.seed,\n",
    "                    # 'early_stop':True,\n",
    "                    'estimator_list': ['xgboost'],\n",
    "                } | self.flaml\n",
    "                learner = fl.AutoML(**dct)\n",
    "                learner.fit(*Z, **dct)\n",
    "                return learner\n",
    "            return {subpop: fcn1(subpop, Z) for subpop, Z in self.get_prepared().items()}\n",
    "        if self.is_learner:\n",
    "            clf, new = self.get(fcn, usr, f'learners_{self.crse_code}', '.pkl', prereq=[self.get_prepared,self.get_enrollments])\n",
    "        else:\n",
    "            clf = None\n",
    "        return clf\n",
    "\n",
    "\n",
    "    def get_predictions(self, models=None):\n",
    "        \"\"\"Generates predictions when models dict is passed\n",
    "        Otherwise, reads predictions from self or file, throwing error if neither exists (NoneType' object has no attribute 'items)\n",
    "        run_prediction uses this to quickly load existing predictions and only trigger model training on error\"\"\"\n",
    "        def fcn():\n",
    "            L = [\n",
    "                learner.prediction(*self.get_prepared()[subpop], cross=self.term_code==model.term_code)\n",
    "                .assign(\n",
    "                    crse_code=self.crse_code,\n",
    "                    prediction_term_code=self.term_code,\n",
    "                    model_term_code=model.term_code,\n",
    "                    mlt=model.get_enrollments()['crse_code'].loc['_headcnt'].loc[subpop]['mlt']\n",
    "                ).reset_index().set_index(['crse_code','prediction_term_code','model_term_code','pidm'])\n",
    "                for term_code, model in models.items() if model.is_learner\n",
    "                for subpop, learner in model.get_learners().items()]\n",
    "            return pd.concat(L).sort_index() if len(L)>0 else pd.DataFrame()\n",
    "        df, new = self.get(fcn, usr, f'predictions_{self.crse_code}', prereq=self.get_prepared)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_enrollments(self):\n",
    "        def fcn():\n",
    "            def fcn1(agg):\n",
    "                grp = unique('crse_code', self.submodels, agg)\n",
    "                g = lambda X, Y: X.join(Y, rsuffix='_y').get_incoming().groupb(grp)['count'].sum()  # get stuff from Y that is not in X\n",
    "                df = pd.DataFrame({\n",
    "                    'current':g(self.get_students(), self.stable.get_registrations()),\n",
    "                    'actual' :g(self.stable.get_registrations(), self.stable.get_students()),\n",
    "                    }).fillna(0)\n",
    "                df['mlt'] = df['actual'] / df['current']\n",
    "                return df.sort_index()\n",
    "            return {agg: fcn1(agg) for agg in self.aggregates}\n",
    "        dct, new = self.get(fcn, usr, f'enrollments', '.pkl', prereq=[self.get_students,self.stable.get_students,self.stable.get_registrations])\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_forecasts(self):\n",
    "        def fcn():\n",
    "            dct = {'prediction':'sum'}\n",
    "            err = [\n",
    "                'cv_score',\n",
    "                # 'mse',\n",
    "                # 'mae',\n",
    "                # 'log_loss',\n",
    "                ]\n",
    "            if self.is_learner:\n",
    "                dct |= {k:'mean' for k in err}\n",
    "            Z = self.get_students().join(self.get_predictions(), how='inner').copy()\n",
    "            Z['prediction'] = Z['prediction'] * Z['mlt']\n",
    "            def fcn1(agg):\n",
    "                df = Z.groupb(unique('crse_code',self.submodels,agg,'prediction_term_code','model_term_code')).agg(dct)\n",
    "                df['prediction'] = df['prediction'].round()\n",
    "                if self.is_learner:\n",
    "                    df = df.join(self.get_enrollments()[agg]['actual']).fillna(0)\n",
    "                    df['error'] = df['prediction'] - df['actual']\n",
    "                    df['error_pct'] = df['error'] / df['actual'] * 100\n",
    "                    df[err] *= 100\n",
    "                    df = df[['prediction','actual','error','error_pct',*err]]\n",
    "                return df.prep()\n",
    "            return {agg: fcn1(agg) for agg in self.aggregates}\n",
    "        dct, new = self.get(fcn, usr, f'forecasts_{self.crse_code}', '.pkl')\n",
    "        return dct\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "@dataclasses.dataclass\n",
    "class AMP(Model):\n",
    "    model_term_codes: tuple = (202408,202508)\n",
    "    crse_codes: tuple = '_headcnt'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print(self.amp_date)\n",
    "        self.amp_date = dt(self.amp_date, now)\n",
    "        self.amp_date -= pd.Timedelta(days=(self.amp_date.weekday()-2)%7)  # move to preceeding Wednesday (flags release)\n",
    "        super().__post_init__()\n",
    "        self.crse_codes = unique(self.crse_codes, sort=True)\n",
    "        self.model_term_codes = unique(self.term_code, self.model_term_codes, sort=True, reverse=True)\n",
    "        kwargs = {k: copy.deepcopy(self[k]) for k in ['features','submodels','aggregates','flaml','overwrite']}\n",
    "        self.models = {term_code: Model(term_code=term_code, is_learner=term_code<self.term_code, amp_date=self.amp_date.replace(year=term_code//100), **kwargs) for term_code in self.model_term_codes}\n",
    "    \n",
    "\n",
    "    def get_students(self):\n",
    "        nm = 'students'\n",
    "        if nm not in self:\n",
    "            self[nm] = pd.concat([model.get_students().get_incoming().assign(prediction_term_code=term_code) for term_code, model in self.models.items()]).prep().set_index('prediction_term_code', append=True).sort_index()\n",
    "        return self[nm]\n",
    "\n",
    "\n",
    "    def run_predictions(self, crse_code='_headcnt'):\n",
    "        [model.set_crse_code(crse_code) for model in self.models.values()]\n",
    "        try:\n",
    "            L = [model.get_predictions() for model in self.models.values()]\n",
    "            print(f'{crse_code} predictions read from file')\n",
    "        except:\n",
    "            for model in self.models.values():\n",
    "                model.get_prepared()\n",
    "                if crse_code == '_proba':\n",
    "                    model[f'prepared_{crse_code}'] = {subpop: [X.drop([*races,'gender','international'], errors='ignore'), y] for subpop, [X,y] in model.get_prepared().items()}\n",
    "                model.get_learners()\n",
    "            L = [model.get_predictions(self.models) for model in self.models.values()]\n",
    "        rm(usr/'prepared')\n",
    "        return pd.concat(L).sort_index()\n",
    "\n",
    "\n",
    "    def get_results(self):\n",
    "        def fcn():\n",
    "            print()\n",
    "            results = dict()\n",
    "            for crse_code in self.crse_codes:\n",
    "                if crse_code != '_proba':\n",
    "                    self.run_predictions(crse_code)\n",
    "                    for model in self.models.values():\n",
    "                        dct = {'predictions': model.get_predictions(), **model.get_forecasts()}\n",
    "                        for key, val in dct.items():\n",
    "                            results.setdefault(key, []).append(val)\n",
    "            return {agg: pd.concat(L).sort_index(ascending=['term_code' not in k for k in L[0].index.names]) for agg, L in results.items()}\n",
    "        dct, new = self.get(fcn, usr, 'results', '.pkl')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_AMP(self):\n",
    "        instructions = pd.DataFrame({\"\":[\n",
    "            f\"Admitted Matriculation Projections (AMP) for {self.amp_date}\",\n",
    "            '',\n",
    "            f'''Executive Summary''',\n",
    "            f'''AMP is a predictive model designed to forecast the incoming (not continuing) Fall cohort to help leaders proactively allocate resources (instructors, sections, labs, etc) in advance.''',\n",
    "            f'''It is intended to supplement, not replace, the knowledge, expertise, and insights developed by institutional leaders through years of experience.''',\n",
    "            f'''Like all AI/ML models (and humans), AMP is fallible and should be used with caution.''',\n",
    "            f'''AMP learns exclusively from historical data captured in EMs Flags reports and IDAs daily admissions and registration snapshots.''',\n",
    "            f'''It cannot account for factors not present in these datasets, including curriculum changes, policy shifts, structural changes, demographic variation, changes in oversight, etc.''',\n",
    "            f'''''',\n",
    "            f'''AMP provides both Summary and Details files. For most users, rows in the Summary file with model_term_code = 202408 will suffice.''',\n",
    "            f'''Because AMPs accuracy varies across courses, the Details file includes historical error analyses to help users assess the reliability of each forecast (details below).''',\n",
    "            '',\n",
    "            f'''As widely requested, AMP includes predictions for the Fall 2025 cohort in Ft. Worth, despite having no prior Ft. Worth FTIC example to learn from.''',\n",
    "            f'''These are a good-faith effort to offer my best data-driven insights, but due to the lack of training data,''',\n",
    "            f'''they are inherently more speculative and should be treated with lower confidence (details below).''',\n",
    "            f'''''',\n",
    "            f'''Definitions''',\n",
    "            f'''crse_code = course code (_headcnt = total headcount)''',\n",
    "            f'''styp_desc = student type; returning = re-enrolling after a previous attempt (not continuing)''',\n",
    "            f'''prediction_term_code = cohort being forecast''',\n",
    "            f'''model_term_code = cohort used to train AMP''',\n",
    "            f'''prediction = forecast headcount''',\n",
    "            f'''*actual = true headcount''',\n",
    "            f'''*error = prediction - actual''',\n",
    "            f'''*error_pct = error / actual * 100''',\n",
    "            f'''*cv_score = average validation log-loss from 3-fold cross-validation''',\n",
    "            f'''*=appears only in Details & not available for 2025 (since actuals are not yet known)''',\n",
    "            '',\n",
    "            f'''Methodology''',\n",
    "            f'''AMP uses XGBoost, a machine learning algorithm, to forecast the number, characteristics, and likely course enrollments of incoming Fall students.''',\n",
    "            f'''Predictions are based on application and pre-semester engagement (orientation, course registration, financial aid, etc.) from EMs Flags and IDAs daily snapshots.''',\n",
    "            f'''For each student admitted for Fall 2025, AMP identifies similar students from past Fall cohorts, analyzes their course enrollments (if any),''',\n",
    "            f'''learns relevant patterns, then forecasts Fall 2025 course enrollment for the admitted student in question.''',\n",
    "            f'''More precisely, for each (incoming student, course)-pair, AMP assigns a probability whether that student will be enrolled in that course on the Friday after Census.''',\n",
    "            f'''These (student, course)-level probabilities are then aggregated in many different ways to forecast headcounts for courses, campuss, majors, colleges, TSI statuses, etc.''',\n",
    "            f'''These appear on different sheets in this workbook.''',\n",
    "            f'''''',\n",
    "            f'''Since admissions and registration data evolve through the spring and summer, AMP is trained only on data available as of the same date in previous years.''',\n",
    "            f'''AMP's forecast for Ft. Worth's Fall 2025 cohort are necessarily based on previous Stephenville cohorts since no Ft. Worth FTIC's existed on this date.''',\n",
    "            f'''Suppose AMP predicts, \"Based on similar FTIC's in Stephenville in 2024, I predict Alice has a 75% probability to matriculate in Fall 2025\".''',\n",
    "            f'''If Alice is applying to Ft. Worth, then 0.75 is added to Ft. Worth's forecast.''',\n",
    "            f'''However, AMP can not yet understand how to adjust its 75% projection to reflect how Ft. Worth FTIC's behave differently than Stephenville FTIC since there are no Ft. Worth FTIC's to learn from.''',\n",
    "            f'''Though not ideal, this is the best idea we've found to forecast Ft. Worth FTIC in the absence of valid training examples.''',\n",
    "            f'',\n",
    "            f'''AMP is trained separately using the Fall 2024, 2023, and 2022 cohorts.''',\n",
    "            f'''Most users should focus on prediction_term_code = 202508 and model_term_code = 202408, as Fall 2025 is likely to resemble Fall 2024 more closely than Fall 2023 & Fall 2022.''',\n",
    "            f'''Users with domain expertise may choose to incorporate older cohorts (e.g., weighted average of model_term_codes 2024, 2023, & 2022) if they believe those terms are similarly relevant.''',\n",
    "            f'',\n",
    "            f'''Rows for prediction_term_code < 202508 appear only in the Details file and include retrospective \"predictions\" and actual outcomes.''',\n",
    "            f'''This allows users to assess AMP's ability to forecast each individual course and calibrate their confidence accordingly.''',\n",
    "            f'',\n",
    "            f'''Predictions for small values are less reliable than for large numbers (Central Limit Theorem).''',\n",
    "            f'',\n",
    "            f'''AMP only models students who have already applied and been admitted (eager).''',\n",
    "            f'''However, more students will apply between now and start of term, especially transfer & returning (lagging).''',\n",
    "            f'''AMP generates forecasts based on eager students then inflates using the eager-lagging ratio from that model_term_code.''',\n",
    "            f'''This assumes the eager-lagging behavior will be approximately the same this year.''',\n",
    "            f'''While this assumption cannot be verified in advance, we must make SOME assumption. This one has proven sufficiently accurate in past cycles.''',\n",
    "            f'',\n",
    "            f'''Dr. Scott Cook is eager to provide as much additional detail as the user desires: scook@tarleton.edu.''',\n",
    "            f'''source code: https://github.com/drscook/admitted_matriculation_predictor'''\n",
    "        ]}).set_index(\"\")\n",
    "\n",
    "        def format_xlsx(sheet):\n",
    "            from openpyxl.styles import Alignment\n",
    "            sheet.auto_filter.ref = sheet.dimensions\n",
    "            for cell in sheet[1]:\n",
    "                cell.alignment = Alignment(horizontal=\"left\")\n",
    "            for column in sheet.columns:\n",
    "                width = 1+max(len(str(cell.value))+3*(i==0) for i, cell in enumerate(column))\n",
    "                sheet.column_dimensions[column[0].column_letter].width = width\n",
    "            sheet.freeze_panes = \"A2\"\n",
    "\n",
    "        def write_xlsx(rpt, sheets):\n",
    "            file = f'AMP_{rpt}'\n",
    "            dst = self.get_dst(usr, file, '.xlsx')\n",
    "            if file in self.overwrite or not dst.exists():\n",
    "                print(f'creating {dst.name}')\n",
    "                src = \"report.xlsx\"\n",
    "                reset(src)\n",
    "                with pd.ExcelWriter(src, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "                    for sheet_name, df in sheets.items():\n",
    "                        df.reset_index().to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                        format_xlsx(writer.sheets[sheet_name])\n",
    "                reset(dst)\n",
    "                shutil.copy(src, dst)\n",
    "                rm(src)\n",
    "        \n",
    "        self.get_results()\n",
    "\n",
    "        get_key = lambda df: df.reset_index().assign(key=lambda X: X['pidm']*1000000+X['prediction_term_code']).prep().set_index('key').sort_index()\n",
    "        R = self.get(lambda: get_key(self.get_results()['predictions']), usr, 'AMP_predictions')[0]\n",
    "        S = self.get(lambda: get_key(self.get_students()), usr, 'AMP_students')[0]\n",
    "\n",
    "        for rpt, fcn in {\n",
    "            'summary':lambda df: df.query(f\"prediction_term_code=={self.term_code}\").iloc[:,:1],\n",
    "            'details':lambda df: df,\n",
    "            }.items():\n",
    "            sheets = {'instructions':instructions} | {agg: fcn(df).round().prep() for agg, df in self.get_results().items() if agg!='predictions'}\n",
    "            write_xlsx(rpt, sheets)\n",
    "\n",
    "        A = self.get_spriden().set_index('pidm')\n",
    "        B = self.get_students().loc[:,'term_code':]\n",
    "        C = self.run_predictions('_proba').loc[('_proba',self.term_code,self.term_code-100),'prediction']\n",
    "        Z = A.join(B, how='inner').join(C, how='inner').prep().sort_values('prediction', ascending=False)\n",
    "        sheets = {'instructions':instructions} | {subpop: df for subpop, df in Z.groupb(self.submodels)}\n",
    "        write_xlsx('em', sheets)\n",
    "\n",
    "    # def get_dashboard(self):\n",
    "    #     # self.get_results()\n",
    "    #     get_key = lambda df: df.reset_index().assign(key=lambda X: X['pidm']*1000000+X['prediction_term_code']).prep().set_index('key').sort_index()\n",
    "    #     self.get(lambda: get_key(self.get_results()['predictions']), usr, 'AMP_predictions')[0]\n",
    "    #     self.get(lambda: get_key(self.get_students()), usr, 'AMP_attributes')[0]\n",
    "\n",
    "\n",
    "self = AMP(\n",
    "    term_code = 202508,\n",
    "    model_term_codes = [202208, 202308, 202408],\n",
    "    flaml = {\n",
    "        'time_budget': 60,\n",
    "    },\n",
    "    crse_codes = {\n",
    "    '_headcnt',\n",
    "    '_proba',    \n",
    "    'agec2317',\n",
    "    'ansc1119',\n",
    "    'ansc1319',\n",
    "    'anth2302',\n",
    "    'anth2351',\n",
    "    'arts1301',\n",
    "    'arts1303',\n",
    "    'arts1304',\n",
    "    'arts3331',\n",
    "    'biol1305',\n",
    "    'biol1406',\n",
    "    'biol1407',\n",
    "    'biol2401',\n",
    "    'biol2402',\n",
    "    'busi1301',\n",
    "    'busi1307',\n",
    "    'chem1111',\n",
    "    'chem1112',\n",
    "    'chem1302',\n",
    "    'chem1311',\n",
    "    'chem1312',\n",
    "    'chem1407',\n",
    "    'chem1409',\n",
    "    'cnst1301',\n",
    "    'comm1311',\n",
    "    'comm1315',\n",
    "    'comm2302',\n",
    "    'crij1301',\n",
    "    'dram1310',\n",
    "    'dram2361',\n",
    "    'easc2310',\n",
    "    'econ1301',\n",
    "    'econ2301',\n",
    "    'educ1301',\n",
    "    'engl1301',\n",
    "    'engl1302',\n",
    "    'engl2307',\n",
    "    'engl2320',\n",
    "    'engl2321',\n",
    "    'engl2326',\n",
    "    'engl2340',\n",
    "    'engl2350',\n",
    "    'engl2360',\n",
    "    'engl2362',\n",
    "    'engl2364',\n",
    "    'engl2366',\n",
    "    'engl2368',\n",
    "    'engr1211',\n",
    "    'engr2303',\n",
    "    'envs1302',\n",
    "    'fina1360',\n",
    "    'geog1303',\n",
    "    'geog1320',\n",
    "    'geog1451',\n",
    "    'geog2301',\n",
    "    'geol1403',\n",
    "    'geol1404',\n",
    "    'geol1407',\n",
    "    'geol1408',\n",
    "    'govt2305',\n",
    "    'govt2306',\n",
    "    'hist1301',\n",
    "    'hist1302',\n",
    "    'hist2321',\n",
    "    'hist2322',\n",
    "    'huma1315',\n",
    "    'kine1301',\n",
    "    'kine1338',\n",
    "    'kine2315',\n",
    "    'math1314',\n",
    "    'math1316',\n",
    "    'math1324',\n",
    "    'math1325',\n",
    "    'math1332',\n",
    "    'math1342',\n",
    "    'math1352',\n",
    "    'math2412',\n",
    "    'math2413',\n",
    "    'musi1303',\n",
    "    'musi1310',\n",
    "    'musi1311',\n",
    "    'musi2350',\n",
    "    'musi3325',\n",
    "    'phil1301',\n",
    "    'phil1304',\n",
    "    'phil2303',\n",
    "    'phil3301',\n",
    "    'phys1302',\n",
    "    'phys1401',\n",
    "    'phys1402',\n",
    "    'phys1403',\n",
    "    'phys1411',\n",
    "    'phys2425',\n",
    "    'phys2426',\n",
    "    'psyc2301',\n",
    "    'psyc3303',\n",
    "    'psyc3307',\n",
    "    'soci1301',\n",
    "    'soci1306',\n",
    "    'soci2303',\n",
    "    'univ0010',\n",
    "    'univ0200',\n",
    "    'univ0204',\n",
    "    'univ0301',\n",
    "    'univ0314',\n",
    "    'univ0324',\n",
    "    'univ0332',\n",
    "    'univ0342',\n",
    "    },\n",
    "    aggregates = [\n",
    "        'styp_desc',\n",
    "        'camp_desc',\n",
    "        'coll_desc',\n",
    "        'dept_desc',\n",
    "        'majr_desc',\n",
    "        'hs_qrtl',\n",
    "        'tsi_math',\n",
    "        'tsi_reading',\n",
    "        'tsi_writing',\n",
    "        # 'gender',\n",
    "        # *races,\n",
    "        # 'international',\n",
    "        'resd_desc',\n",
    "        'oriented',\n",
    "        'waiver',\n",
    "        'lgcy',\n",
    "    ],\n",
    "    features = {\n",
    "        'act_equiv':pd.NA,\n",
    "        'age':pd.NA,\n",
    "        'camp_desc':'stephenville',\n",
    "        'drivetime':pd.NA,\n",
    "        'eager':pd.NA,\n",
    "        'fafsa': False,\n",
    "        'finaid': False,\n",
    "        'gap_score':0,\n",
    "        'gender':pd.NA,\n",
    "        'hs_qrtl':pd.NA,\n",
    "        'international':False,\n",
    "        'lgcy':False,\n",
    "        'oriented':False,\n",
    "        **{r: False for r in races},\n",
    "        'schlship':False,\n",
    "        'ssb':False,\n",
    "        'tsi_math':False,\n",
    "        'tsi_reading':False,\n",
    "        'tsi_writing':False,\n",
    "        'verified':False,\n",
    "        'waiver':False,\n",
    "    },\n",
    "    overwrite = set({\n",
    "        # 'registrations',\n",
    "        # 'admissions',\n",
    "        # 'flags',\n",
    "        # 'students',\n",
    "        # 'imputed',\n",
    "        # 'prepared',\n",
    "        # 'enrollments',\n",
    "        # 'learners',\n",
    "        # 'predictions',\n",
    "        # 'forecasts',\n",
    "        # 'results',\n",
    "        # 'AMP_details',\n",
    "        # 'AMP_summary',\n",
    "        # 'AMP_em',\n",
    "        # 'AMP_predictions',\n",
    "        # 'AMP_attributes',\n",
    "    }),\n",
    "    # amp_date = '2025-05-14',\n",
    ")\n",
    "\n",
    "# self.process_flags()\n",
    "self.get_AMP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c413e2e-331d-4a79-a162-e48909afb954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### old useful code - do not delete\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# A = run(f'saturnstvcamp').T\n",
    "# repl = dict(zip(A.iloc[0], A.iloc[1]))\n",
    "# df = pd.read_parquet('/Volumes/aiml/amp/amp_files/2025/geo/drivetimes.parquet').pivot_table(index='zip', columns='camp_code', values='drivetime').rename(columns=repl).rename_axis(columns=None).reset_index()\n",
    "# df.to_csv('/Volumes/aiml/amp/amp_files/2025/geo/drivetimes.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # def get_reports(self):\n",
    "    #     self.get_em_report()\n",
    "\n",
    "    #     instructions = pd.DataFrame({\"\":[\n",
    "    #         f\"Admitted Matriculation Projections (AMP) for {self.amp_date}\",\n",
    "    #         '',\n",
    "    #         f'''Executive Summary''',\n",
    "    #         f'''AMP is a predictive model designed to forecasr the incoming (not continuing) Fall cohort to help leaders proactively allocate resources (instructors, sections, labs, etc) in advance.''',\n",
    "    #         f'''It is intended to supplement, not replace, the knowledge, expertise, and insights developed by institutional leaders over years of experience.''',\n",
    "    #         f'''Like all AI/ML models (and humans), AMP is fallible and should be used with caution.''',\n",
    "    #         f'''AMP learns exclusively from historical data captured in EMs Flags reports and IDAs daily admissions and registration snapshots.''',\n",
    "    #         f'''It cannot account for factors not present in these datasets, including curriculum changes, policy shifts, structural changes, demographic variation, changes in oversight, etc.''',\n",
    "    #         f'''''',\n",
    "    #         f'''AMP provides both Summary and Details files. For most users, rows in the Summary file with learner_year = 2024 will suffice.''',\n",
    "    #         f'''Because AMPs accuracy varies across courses, the Details file includes historical error analyses to help users assess the reliability of each forecast.''',\n",
    "    #         '',\n",
    "    #         f'''As widely requested, AMP includes predictions for the 2025 cohort in Ft. Worth, despite having no prior Ft. Worth FTIC example to learn from.''',\n",
    "    #         f'''These are a good-faith effort to offer my best data-driven insights, but due to the lack of training data,''',\n",
    "    #         f'''they are inherently more speculative and should be treated with lower confidence (details below).''',\n",
    "    #         f'''''',\n",
    "    #         f'''Definitions''',\n",
    "    #         f'''crse_code = course code (_headcnt = total headcount)''',\n",
    "    #         f'''styp_desc = student type; returning = re-enrolling after a previous attempt (not continuing)''',\n",
    "    #         f'''prediction_year = cohort being forecast''',\n",
    "    #         f'''model_year = cohort used to train AMP''',\n",
    "    #         f'''prediction = forecast headcount''',\n",
    "    #         f'''*actual = true headcount''',\n",
    "    #         f'''*error = prediction - actual''',\n",
    "    #         f'''*error_pct = error / actual * 100''',\n",
    "    #         f'''*cv_score = average validation log-loss from 3-fold cross-validation''',\n",
    "    #         f'''*=appears only in Details & not available for 2025 (since actuals are not yet known)''',\n",
    "    #         '',\n",
    "    #         f'''Methodology''',\n",
    "    #         f'''AMP uses XGBoost, a machine learning algorithm, to forecast the number, characteristics, and likely course enrollments of incoming Fall students.''',\n",
    "    #         f'''Predictions are based on application and pre-semester engagement (orientation, course registration, financial aid, etc.) from EMs Flags and IDAs daily snapshots.''',\n",
    "    #         f'''For each student admitted for Fall 2025, AMP identifies similar students from past Fall admits, analyzes their course enrollments (if any),''',\n",
    "    #         f'''learns relevant patterns, then forecasts Fall 2025 course enrollment for the admitted student in question.''',\n",
    "    #         f'''More precisely, for each (incoming student, course)-pair, AMP assigns a probability whether that student will be enrolled in that course on the Friday after Census.''',\n",
    "    #         f'''These (student, course)-level predictions are then aggregated in many different ways to forecast headcounts for courses, campuss, majors, colleges, TSI statuses, etc.''',\n",
    "    #         f'''These appear on different sheets in this workbook.''',\n",
    "    #         f'''''',\n",
    "    #         f'''Since admissions and registration data evolve through the spring and summer, AMP is trained only on data available as of the same date in previous years.''',\n",
    "    #         f'''AMP's forecast for Ft. Worth's 2025 cohort are necessarily based on previous Stephenville cohorts since no Ft. Worth FTIC's existed on this date.''',\n",
    "    #         f'''Suppose AMP predicts, \"Based on similar FTIC's in Stephenville in 2024, I predict Alice has a 75% probability to matriculate in Fall 2025\".''',\n",
    "    #         f'''If Alice is applying to Ft. Worth, then 0.75 is added to Ft. Worth's forecast.''',\n",
    "    #         f'''However, AMP can not yet understand how to adjust its 75% projection to reflect how Ft. Worth FTIC's behave differently than Stephenville FTIC since there are no Ft. Worth FTIC's to learn from.''',\n",
    "    #         f'''Though not ideal, this appears to be the most reasonable mechanism to forecast Ft. Worth FTIC in the absence of training examples.''',\n",
    "    #         '',\n",
    "    #         f'''AMP is trained separately using the 2024, 2023, and 2022 cohorts.''',\n",
    "    #         f'''Most users should focus on prediction_year = 2025 and learner_year = 2024, as 2025 is likely to resemble 2024 more closely than 2023 & 2022.''',\n",
    "    #         f'''Users with domain expertise may choose to incorporate older cohorts (e.g., weighted average of learner_years 2024, 2023, & 2022) if they believe those years are similarly relevant.''',\n",
    "    #         '',\n",
    "    #         f'''Rows for prediction_year < 2025 appear only in the Details file and include retrospective \"predictions\" and actual outcomes.''',\n",
    "    #         f'''This allows users to assess AMP's ability to forecast each individual course and calibrate their confidence accordingly.''',\n",
    "    #         '',\n",
    "    #         f'''Predictions for small values are less reliable than for large numbers (central limit theorem).''',\n",
    "    #         '',\n",
    "    #         f'''AMP only models students who have already applied and been admitted (eager).''',\n",
    "    #         f'''However, more students will apply between now and start of term, especially transfer & returning (lagging).''',\n",
    "    #         f'''AMP generates forecasts based on eager students then inflates using the eager-lagging ratio from the learner_year.''',\n",
    "    #         f'''This assumes the eager-lagging behavior will be approximately the same this year. approach has proven sufficiently accurate in prior years.''',\n",
    "    #         f'''While this assumption cannot be verified in advance, some assumption is needed. This one has proven sufficiently accurate in past cycles.''',\n",
    "    #         '',\n",
    "    #         f'''Dr. Scott Cook is eager to provide as much additional detail on AMP's workings as the user desires - email scook@tarleton.edu.''',\n",
    "    #         f'''source code: https://github.com/drscook/admitted_matriculation_predictor'''\n",
    "    #     ]})\n",
    "\n",
    "    #     def format_xlsx(sheet):\n",
    "    #         from openpyxl.styles import Alignment\n",
    "    #         sheet.auto_filter.ref = sheet.dimensions\n",
    "    #         for cell in sheet[1]:\n",
    "    #             cell.alignment = Alignment(horizontal=\"left\")\n",
    "    #         for j, column in enumerate(sheet.columns):\n",
    "    #             width = max(len(str(cell.value))+3*(i==0) for i, cell in enumerate(column))\n",
    "    #             sheet.column_dimensions[chr(65+j)].width = width\n",
    "    #         sheet.freeze_panes = \"A2\"\n",
    "\n",
    "    #     def fcn_details(df):\n",
    "    #         return df\n",
    "\n",
    "    #     def fcn_summary(df):\n",
    "    #         return df.query(f\"prediction_year==prediction_year.max()\").iloc[:,:1]\n",
    "\n",
    "    #     for nm, fcn in {'details':fcn_details, 'summary':fcn_summary}.items():\n",
    "    #         src = \"report.xlsx\"\n",
    "    #         reset(src)\n",
    "    #         with pd.ExcelWriter(src, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "    #             instructions.to_excel(writer, sheet_name='instructions', index=False)\n",
    "    #             for key, df in self.get_results().items():\n",
    "    #                 fcn(df).reset_index().round().prep().to_excel(writer, sheet_name=key, index=False)\n",
    "    #                 format_xlsx(writer.sheets[key])\n",
    "    #         dst = data/f\"reports/{self.term_code}/{self.amp_date}/AMP_{self.amp_date}_{nm}.xlsx\"\n",
    "    #         reset(dst)\n",
    "    #         shutil.copy(src, dst)\n",
    "    #         rm(src)\n",
    "\n",
    "\n",
    "    # def get_reports_em(self):\n",
    "    #     def fcn():\n",
    "    #         df = self.get_spriden().set_index('pidm').join(self.models[self.term_code].get_predictions(), how='inner').loc[:,:'prediction']\n",
    "    #         return df\n",
    "    #     df, new = self.get(fcn, 'reports_amp_em', self.run, suffix='.csv')\n",
    "    #     # df, new = self.get(fcn, data/f\"reports/{self.term_code}/{self.amp_date}/AMP_{self.amp_date}_EM\", self.run, suffix='.csv')\n",
    "    #     return df\n",
    "\n",
    "\n",
    "## useful old code\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#                 'ssbsect_crn':'crn',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.amp_date}' -- added before amp_day\n",
    "#     and (sfrstcr_rsts_date > '{self.amp_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after amp_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.amp_date}' -- added before amp_day\n",
    "#     and (sfrstcr_rsts_date > '{self.amp_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after amp_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "\n",
    "#         S = self.students[['pidm','term_code','styp_code']]\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "    #     def fcn():\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     sfrstcr_pidm as pidm\n",
    "#     ,sfrstcr_term_code as term_code\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     --and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.amp_date}' -- added before amp_day\n",
    "#     and (sfrstcr_rsts_date > '{self.amp_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after amp_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     sfrstcr_pidm\n",
    "#     ,sfrstcr_term_code\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# don't delete - could be useful & was hard to create\n",
    "            # stat_codes = ['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY'] # not AK & HI b/c can't get driving distance\n",
    "#     ,{get_desc('spraddr_cnty_code')[0]}\n",
    "#     ,{get_desc('spraddr_stat_code')[0]}\n",
    "#     ,zip_code\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#         ,try_to_number(left(spraddr_zip, 5), '00000') as zip_code\n",
    "#         ,case\n",
    "#             when spraddr_atyp_code = 'PA' then 6\n",
    "#             when spraddr_atyp_code = 'PR' then 5\n",
    "#             when spraddr_atyp_code = 'MA' then 4\n",
    "#             when spraddr_atyp_code = 'BU' then 3\n",
    "#             when spraddr_atyp_code = 'BI' then 2\n",
    "#             when spraddr_atyp_code = 'P1' then 1\n",
    "#             when spraddr_atyp_code = 'P2' then 0\n",
    "#             end as spraddr_atyp_rank\n",
    "#     from\n",
    "#         {catalog}spraddr_amp_v\n",
    "#     where\n",
    "#         spraddr_stat_code in ('{join(stat_codes, \"','\")}')\n",
    "#         and spraddr_zip is not null\n",
    "#     qualify\n",
    "#         row_number() over (partition by spraddr_pidm order by spraddr_atyp_rank desc, spraddr_seqno desc) = 1\n",
    "# )\n",
    "# on\n",
    "#     pidm = spraddr_pidm\n",
    "\n",
    "# {get_desc('spraddr_cnty_code')[1]}\n",
    "# {get_desc('spraddr_stat_code')[1]}\n",
    "\n",
    "\n",
    "\n",
    "    # def get_zips(self, show=False):\n",
    "    #     \"\"\"takes ~3 hours toget zip codes and find nearest point on road network to the provided representative point\"\"\"\n",
    "    #     def fcn():\n",
    "    #         from pgeocode import Nominatim\n",
    "    #         nomi = Nominatim('us')\n",
    "    #         df = nomi.query_postal_code(pd.Series(nomi._data['postal_code'])).query(\"state_code.notnull() & state_code not in ['AK', 'HI', 'MH']\").prep().set_index('postal_code').rename_axis('zip')\n",
    "    #         nearest = lambda x: join(requests.get(f\"http://router.project-osrm.org/nearest/v1/driving/{x['longitude']},{x['latitude']}\").json()['waypoints'][0]['location'],',')\n",
    "    #         df['point'] = df.apply(nearest, axis=1)\n",
    "    #         return df\n",
    "    #     df, new = self.get(fcn, root/'zips')\n",
    "    #     self.states = set(df['state_code'])\n",
    "    #     return df\n",
    "\n",
    "\n",
    "    # def get_drivetimes(self, show=False):\n",
    "    #     def fcn():\n",
    "    #         campus_coords = {\n",
    "    #             's': [-98.215784,32.216217],\n",
    "    #             # 'm': '-97.432975,32.582436',\n",
    "    #             # 'w': 76708,\n",
    "    #             # 'r': 77807,\n",
    "    #             }\n",
    "\n",
    "    #         url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "    #         gdf = gpd.read_file(url).prep().set_index('zcta5ce20').iloc[:5]\n",
    "    #         pts = gdf.sample_points(size=5,method=\"uniform\").explode()#.apply(lambda geom: f\"{geom.x},{geom.y}\")\n",
    "    #         df = pts.to_frame()[[]]\n",
    "    #         url = \"http://router.project-osrm.org/table/v1/driving\"\n",
    "    #         headers = {\"Content-Type\": \"application/json\"}\n",
    "    #         for k, v in campus_coords.items():\n",
    "    #             u = [v, *pts]\n",
    "    #             print(u)\n",
    "    #             data = {\n",
    "    #                 \"coordinates\": u,\n",
    "    #                 \"annotations\": [\"duration\", \"distance\"],\n",
    "    #                 \"sources\": 0,\n",
    "    #             }\n",
    "    #             response = requests.post(url, json=data, headers=headers)\n",
    "    #             print(response.json())\n",
    "    #             assert 1==2\n",
    "\n",
    "            # for k, v in campus_coords.items():\n",
    "            #     u = join([v, *pts], ';')\n",
    "            #     url = f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={0}&annotations=duration,distance\"\n",
    "            #     print(url)\n",
    "            #     print(requests.get(url))\n",
    "            #     df[k] = np.squeeze(requests.get(url).json()['durations'])[1:]/60\n",
    "    #         # df.disp(10)\n",
    "    #         # df = df.groupby('zip').min()\n",
    "    #         # df.disp(10)\n",
    "    #         df = df.groupby(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "    #         return df\n",
    "\n",
    "            # df = self.zips.iloc[34339:34349].copy()\n",
    "            # u = join(df.apply(lambda x: f\"{x['longitude']},{x['latitude']}\", axis=1),';')\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     df[k] = np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={df.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['distances'])/1609\n",
    "\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     df[k] = np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={df.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['durations'])/60\n",
    "\n",
    "            # self.zips = self.zips.iloc[34339:34349]\n",
    "            # self.zips.disp(20)\n",
    "            # u = join(self.zips['point'],';')\n",
    "            \n",
    "            # dct = dict()\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     i = self.zips.index.get_loc(z)\n",
    "            #     print(self.zips.iloc[i])\n",
    "            #     url = f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={0}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     print(url)\n",
    "            #     response = requests.get(url).json()\n",
    "            #     for k,v in response.items():\n",
    "            #         print(k)\n",
    "            #         display(v)\n",
    "            #         print()\n",
    "            #     print(response['distance'])\n",
    "            #     dct[k] = np.squeeze(response['durations'])\n",
    "\n",
    "            # # dct = {k: np.squeeze(\n",
    "            # #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={self.zips.index.get_loc(z)}&fallback_speed=600&fallback_coordinate=snapped\"\n",
    "            # #     ).json()['durations'])/60 for k, z in campus_zips.items()}\n",
    "            # dct = {k: np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={self.zips.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['distances']) for k, z in campus_zips.items()}\n",
    "\n",
    "            # df = pd.DataFrame(dct, index=self.zips.index).stack().rename_axis(['zip','camp_code']).rename('drivetime') / 1609\n",
    "            # return df\n",
    "            # print()\n",
    "            # dct = {k: self.zips.loc[y] for k, y in {\n",
    "            #     's': 76402,\n",
    "            #     'm': 76036,\n",
    "            #     'w': 76708,\n",
    "            #     'r': 77807,\n",
    "            #     }.items()}\n",
    "            # L = [\n",
    "            #     self.get(\n",
    "            #         lambda: X.apply(get_driving_distance, y=y, axis=1).rename('distance').reset_index().assign(camp_code=k),\n",
    "            #         root/f'distances/distances_{s}_{k}',\n",
    "            #         divide=False,\n",
    "            #     )[0] for s, X in self.zips.groupby('state_code') for k, y in dct.items()]\n",
    "            # return pd.concat(L, ignore_index=True)\n",
    "        # df, new = self.get(fcn, root/'drivetimes')#, self.get_zips)\n",
    "        # return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def get_flags_history(self, cutoff=202206):\n",
    "    #     def fcn():\n",
    "    #         import pyarrow.parquet as pq\n",
    "    #         print()\n",
    "    #         L = []\n",
    "    #         for path in sorted(flags_prc.iterdir(), reverse=True):\n",
    "    #             print(path)\n",
    "    #             for src in path.iterdir():\n",
    "    #                 _, term_code, amp_date, amp_day = src.stem.split('_')\n",
    "    #                 col = pq.ParquetFile(src).schema.names\n",
    "    #                 df = pd.DataFrame(columns=col).assign(term_code=[int(term_code)], amp_date=[amp_date]).fillna(True)\n",
    "    #                 L.append(df)\n",
    "    #         df = pd.concat(L).fillna(False).set_index(['amp_date','term_code']).sort_index()\n",
    "    #         return df[sorted(df.columns)]\n",
    "    #     df, new = self.get(fcn, path=data/'flags_history')\n",
    "    #     A = df.query(f'term_code>={cutoff}').groupby('term_code').sum().sort_index(ascending=False).T.rename_axis('variable')\n",
    "    #     B = A == A.max()\n",
    "    #     B.insert(0, 'n', B.sum(axis=1))\n",
    "    #     return B.reset_index().sort_values(['n', 'variable'], ascending=[False, True])\n",
    "\n",
    "\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "# [warnings.filterwarnings(action='ignore', message=f\".*{w}.*\") for w in [\n",
    "#     \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "#     \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "#     \"errors='ignore' is deprecated\"\n",
    "#     \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "#     \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "#     \"DataFrame is highly fragmented\",\n",
    "# ]]\n",
    "\n",
    "\n",
    "\n",
    "# for fore in amp.values():\n",
    "#     for base in amp.values():\n",
    "#         if base.year < max(years):\n",
    "#             for styp, clf in base.get_models().items():\n",
    "#                 for k in ['predictions','headcounts']:\n",
    "#                     fore.__dict__.setdefault(k, dict()).setdefault(styp, dict())\n",
    "#                 y = clf.prediction(fore.get_prepared()[styp])\n",
    "#                 fore.predictions[styp][base.year] = y\n",
    "#                 s = (\n",
    "#                     (y[['pred']].sum() * base.get_enrollments().loc[base.crse_code,styp]['mlt']).round()\n",
    "#                     .rename(base.year).to_frame().T.rename_axis('base_year')\n",
    "#                     .assign(styp_code=styp, forecast_year=fore.year)\n",
    "#                     .reset_index().set_index(['styp_code','forecast_year','base_year'])\n",
    "#                 )\n",
    "#                 if fore.year < max(years):\n",
    "#                     s['true'] = fore.get_enrollments().loc[base.crse_code,styp]['stable']\n",
    "#                     s['error'] = s['pred'] - s['true']\n",
    "#                     s['error_pct'] = round(s['error'] / s['true'] * 100, 2)\n",
    "#                 fore.headcounts[styp][base.year] = s.prep()\n",
    "#     fore.forecasts = {styp: pd.concat(v.values()) for styp, v in fore.headcounts.items()}\n",
    "# amp[2023].forecasts['n']\n",
    "\n",
    "\n",
    "\n",
    "# def get_desc(code):\n",
    "#     for nm in code.split('_'):\n",
    "#         if len(nm) == 4:\n",
    "#             break\n",
    "#     return [f'{code} as {nm}_code, stv{nm}_desc as {nm}_desc', f'left join {catalog}saturnstv{nm} on {code} = stv{nm}_code']\n",
    "\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     pidm\n",
    "#     ,{self.amp_day} as amp_day\n",
    "#     ,timestamp('{self.amp_date}') as amp_date\n",
    "#     ,current_date\n",
    "#     ,first_date\n",
    "#     ,final_date\n",
    "#     ,{get_desc('term_code')[0]}\n",
    "#     ,appl_no\n",
    "#     ,{get_desc('apst_code')[0]}\n",
    "#     ,{get_desc('apdc_code')[0]}\n",
    "#     ,{get_desc('admt_code')[0]}\n",
    "#     ,{get_desc('wrsn_code')[0]}\n",
    "#     ,{get_desc('levl_code')[0]}\n",
    "#     ,{get_desc('styp_code')[0]}\n",
    "#     ,{get_desc('camp_code')[0]}\n",
    "#     ,{get_desc('coll_code_1')[0]}\n",
    "#     ,{get_desc('dept_code')[0]}\n",
    "#     ,{get_desc('majr_code_1')[0]}\n",
    "#     ,{get_desc('saradap_resd_code')[0]}\n",
    "#     ,gender\n",
    "#     ,birth_date\n",
    "#     ,{get_desc('spbpers_lgcy_code')[0]}\n",
    "#     ,gorvisa_vtyp_code is not null as international\n",
    "#     ,gorvisa_natn_code_issue as natn_code, stvnatn_nation as natn_desc\n",
    "#     ,{coalesce('race_asian')}\n",
    "#     ,{coalesce('race_black')}\n",
    "#     ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "#     ,{coalesce('race_native')}\n",
    "#     ,{coalesce('race_pacific')}\n",
    "#     ,{coalesce('race_white')}\n",
    "#     ,hs_percentile\n",
    "#     ,sbgi_code\n",
    "#     ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "# from {subqry(qry)} as A\n",
    "\n",
    "# left join\n",
    "#     {catalog}spbpers_amp_v\n",
    "# on\n",
    "#     pidm = spbpers_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}generalgorvisa\n",
    "#         --{catalog}gorvisa_amp_v\n",
    "#     qualify\n",
    "#         row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorvisa_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         gorprac_pidm\n",
    "#         ,max(gorprac_race_cde='AS') as race_asian\n",
    "#         ,max(gorprac_race_cde='BL') as race_black\n",
    "#         ,max(gorprac_race_cde='IN') as race_native\n",
    "#         ,max(gorprac_race_cde='HA') as race_pacific\n",
    "#         ,max(gorprac_race_cde='WH') as race_white\n",
    "#     from\n",
    "#         {catalog}generalgorprac\n",
    "#     group by\n",
    "#         gorprac_pidm\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorprac_pidm\n",
    "\n",
    "# {get_desc('term_code')[1]}\n",
    "# {get_desc('levl_code')[1]}\n",
    "# {get_desc('styp_code')[1]}\n",
    "# {get_desc('admt_code')[1]}\n",
    "# {get_desc('wrsn_code')[1]}\n",
    "# {get_desc('apst_code')[1]}\n",
    "# {get_desc('apdc_code')[1]}\n",
    "# {get_desc('camp_code')[1]}\n",
    "# {get_desc('coll_code_1')[1]}\n",
    "# {get_desc('dept_code')[1]}\n",
    "# {get_desc('majr_code_1')[1]}\n",
    "# {get_desc('saradap_resd_code')[1]}\n",
    "# {get_desc('gorvisa_natn_code_issue')[1]}\n",
    "# {get_desc('spbpers_lgcy_code')[1]}\n",
    "\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.amp_date}' -- added before amp_day\n",
    "#     and (sfrstcr_rsts_date > '{self.amp_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after amp_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "#     and ssbsect_credit_hrs > 0\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             grp = join([\n",
    "#                 'pidm',\n",
    "#                 'term_code','term_desc',\n",
    "#                 'levl_code','levl_desc',\n",
    "#                 'styp_code','styp_desc',\n",
    "#                 ], ', ')\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     sfrstcr_pidm as pidm\n",
    "#     ,{get_desc('ssbsect_term_code')}\n",
    "#     ,{get_desc('sgbstdn_levl_code')}\n",
    "#     ,{get_desc('sgbstdn_styp_code')}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term parts\n",
    "#     and sfrstcr_add_date <= '{self.amp_date}' -- added before amp_day\n",
    "#     and (sfrstcr_rsts_date > '{self.amp_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after amp_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "#     and ssbsect_credit_hrs > 0\n",
    "# group by\n",
    "#     {grp}, crse_code\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with CTE as {subqry(qry)}\n",
    "\n",
    "# --individual courses\n",
    "# select\n",
    "#     *\n",
    "# from\n",
    "#     CTE\n",
    "\n",
    "# union all\n",
    "\n",
    "# --total credit hours\n",
    "# select\n",
    "#     {grp}\n",
    "#     ,'_total_sch' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     {grp}\n",
    "\n",
    "# union all\n",
    "\n",
    "# --headcount \n",
    "# select\n",
    "#     {grp}\n",
    "#     ,'_headcount' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     {grp}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "    def get_students(self, show=False):\n",
    "        def fcn():\n",
    "            df = (self.admissions\n",
    "                  .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                  .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])                \n",
    "            )\n",
    "            mask = df.eval(\"drivetime.isnull() & zip.notnull() & camp_code!='o'\")\n",
    "            if mask.any():\n",
    "                df[mask].set_index(['state','city','zip','camp_code'])[[]].sort_index().reset_index().disp(50)\n",
    "\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            \n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            \n",
    "            \n",
    "            # df['oriented'] = np.where(df['orien_sess'].notnull() | df['registered'].notnull(), 'y', np.where(df['orientation_hold_exists'].notnull(), 'n', 'w'))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "\n",
    "            # df['verified'] = np.where(df['ver_complete'].notnull(), 'y', np.where(df['selected_for_ver'].notnull(), 'n', 'w'))\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            \n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "\n",
    "            df['eager'] = (self.stable_date - df['first_date']).dt.days\n",
    "            df['age'] = (self.stable_date - df['birth_date']).dt.days\n",
    "\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            \n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "\n",
    "\n",
    "\n",
    "            # df['majr_code'] = df['majr_code'].replace({'0000':pd.NA, 'und':pd.NA, 'eled':'eted', 'agri':'unda'})\n",
    "\n",
    "            # df['coll_code'] = df['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "\n",
    "            # df['coll_desc'] = df['coll_code'].map({\n",
    "            #     'an': 'ag & natural_resources',\n",
    "            #     'ba': 'business',\n",
    "            #     'ed': 'education',\n",
    "            #     'en': 'engineering',\n",
    "            #     'hl': 'health sciences',\n",
    "            #     'la': 'liberal & fine arts',\n",
    "            #     'sm': 'science & mathematics',\n",
    "            #     pd.NA: 'no college designated',\n",
    "            # })\n",
    "\n",
    "\n",
    "\n",
    "            # checks = [\n",
    "            #     'amp_day >= 0',\n",
    "            #     'eager >= amp_day',\n",
    "            #     'age >= 5000',\n",
    "            #     'distance >= 0',\n",
    "            #     'hs_pctl >=0',\n",
    "            #     'hs_pctl <= 100',\n",
    "            #     'hs_qrtl >= 0',\n",
    "            #     'hs_qrtl <= 4',\n",
    "            #     'act_equiv >= 1',\n",
    "            #     'act_equiv <= 36',\n",
    "            #     'gap_score >= 0',\n",
    "            #     'gap_score <= 100',\n",
    "            # ]\n",
    "            # for check in checks:\n",
    "            #     mask = df.eval(check)\n",
    "            #     assert mask.all(), [check,df[~mask].disp(5)]\n",
    "            mask = df['amp_date_flags'].isnull()  # rows from admissions not on flags - should not be any\n",
    "            if mask.any():\n",
    "                display(df[mask]['styp_code'].value_counts().sort_index().to_frame().T)\n",
    "            return df.set_index(['pidm'])\n",
    "        df, new = self.get(fcn, 'students', [self.get_admissions,self.get_flags,self.get_drivetimes])\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "for A in list(flags_prc.iterdir()):\n",
    "#     if A.is_dir():\n",
    "#         for src in list(A.iterdir()):\n",
    "#             if src.is_file():\n",
    "#                 date = [x for x in src.name.split('_') if '-' in x][0]\n",
    "#                 dst = A / date / src.name\n",
    "#                 reset(dst)\n",
    "#                 print(src, dst)\n",
    "#                 # assert False\n",
    "#                 src.rename(dst)\n",
    "\n",
    "#             # for src in list(B.iterdir()):\n",
    "#             #     if src.is_file():\n",
    "#             #         date = [x for x in src.name.split('_') if '-' in x][0]\n",
    "#             #         dst = B / date / src.name\n",
    "#             #         reset(dst)\n",
    "#             #         print(src, dst)\n",
    "#             #         src.rename(dst)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9000138541370679,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AMP_2025",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
