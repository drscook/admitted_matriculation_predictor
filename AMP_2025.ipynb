{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66d214e-5b29-4cd7-b18a-090abac60882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import display, HTML, clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming openpyxl numpy pandas geopandas pgeocode flaml[automl] git+https://github.com/AnotherSamWilson/miceforest.git\n",
    "    # scikit-learn\n",
    "    dbutils.library.restartPython()\n",
    "    clear_output()\n",
    "    dbutils.notebook.exit('Rerun to use newly installed/updated packages')\n",
    "\n",
    "import os, sys, copy, pathlib, shutil, pickle, warnings, requests, dataclasses, time, codetiming, numpy as np, pandas as pd, geopandas as gpd, pgeocode, miceforest as mf, flaml as fl\n",
    "from pgeocode import Nominatim\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "clear_output()\n",
    "now = pd.Timestamp.now()\n",
    "eps = np.finfo(float).eps\n",
    "tab = '    '\n",
    "divider = '\\n##############################################################################################################'\n",
    "catalog = 'dev.bronze.'\n",
    "# root = pathlib.Path(f'/Workspace/Users/{username}@tarleton.edu/admitted_matriculation_predictor_2025/')\n",
    "root = pathlib.Path('/Volumes/aiml/amp/amp_files/2025')\n",
    "data = root/'data'\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "for w in [\n",
    "    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "    ]:\n",
    "    warnings.filterwarnings(action='ignore', message=f\".*{w}.*\")\n",
    "\n",
    "def dt(*args):\n",
    "    return pd.to_datetime(args).dropna().min().normalize()\n",
    "\n",
    "def setmeth(cls, fcn):\n",
    "    \"\"\"monkey-patch new method into a mutable class (fails for immutable class)\"\"\"\n",
    "    setattr(cls, fcn.__name__, fcn)\n",
    "\n",
    "def listify(*args, sort=False, reverse=False):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        L = list(*args)\n",
    "    except Exception as e:\n",
    "        L = list(args)\n",
    "    if sort:\n",
    "        try:\n",
    "            L = sorted(L, reverse=reverse) \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return L\n",
    "\n",
    "def setify(*args):\n",
    "    \"\"\"ensure it is a set\"\"\"\n",
    "    return set(listify(*args))\n",
    "\n",
    "def unique(*args):\n",
    "    \"\"\"get unique items maintaining order\"\"\"\n",
    "    return listify(dict.fromkeys(listify(*args)))\n",
    "\n",
    "def difference(A, B):\n",
    "    return unique([x for x in listify(A) if x not in listify(B)])\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    qry = '\\n' + qry.strip()\n",
    "    qry = '(' + qry + '\\n)' if 'select' in qry else qry\n",
    "    return indent(qry, lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=42):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep().sort_index()\n",
    "\n",
    "def get_size(path):\n",
    "    os.system(f'du -h {path}')\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "def mkdir(path):\n",
    "    path = pathlib.Path(path)\n",
    "    (path if path.suffix == '' else path.parent).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def reset(path):\n",
    "    rm(path)\n",
    "    mkdir(path)\n",
    "    return path\n",
    "\n",
    "def load(path):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.suffix == '.parquet':\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.suffix == '.csv':\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump(path, obj):\n",
    "    path = reset(path)\n",
    "    if path.suffix == '.parquet':\n",
    "        pd.DataFrame(obj).prep().to_parquet(path)  # forced to wrap with explicit pd.DataFrame to due strange error under pandas 2.2.3 \"Object of type PlanMetrics is not JSON serializable\" with to_parquet\n",
    "    elif path.suffix == '.csv':\n",
    "        pd.DataFrame(obj).prep().to_csv(path)\n",
    "    else:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def get_desc(code):\n",
    "    for nm in code.split('_'):\n",
    "        if len(nm) == 4:\n",
    "            break\n",
    "    return f'{code} as {nm}_code, (select stv{nm}_desc from {catalog}saturnstv{nm} where {code} = stv{nm}_code limit 1) as {nm}_desc'\n",
    "\n",
    "def coalesce(x, y=False):\n",
    "    return f'coalesce({x}, {y}) as {x}'\n",
    "\n",
    "races = [f'race_{r}' for r in ['asian','black','hispanic','native','pacific','white']]\n",
    "############ pandas functions ############\n",
    "pd.options.display.max_columns = None\n",
    "def disp(df, rows=4, head=True, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    df = df.sort_index(axis=1) if sort else df\n",
    "    # missing = df.isnull().sum().to_frame().T\n",
    "    # X = pd.concat([df.dtypes.to_frame().T, missing, (missing/df.shape[0]*100).round(2), df.head(rows) if head else df.tails(rows)])\n",
    "    X = df if rows < 0 else df.head(rows) if head else df.tails(rows)\n",
    "    display(HTML(X.to_html()))\n",
    "    print(df.shape)\n",
    "\n",
    "def to_numeric(df, downcast='integer', errors='ignore', category=False, **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: s.astype('string').str.lower().str.strip() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s.astype('category') if s.dtype=='string' and category else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "        idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "        return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def groupb(df, by=None, sort=False, observed=False, dropna=False, **kwargs):\n",
    "    return df.groupby(by=by, sort=sort, observed=observed, dropna=dropna, **kwargs)\n",
    "\n",
    "def get_incoming(df):\n",
    "    return df.query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    "\n",
    "def get_duplicates(df, subset='pidm', quit=True, rows=10):\n",
    "    mask = df.groupby(subset).transform('size') > 1\n",
    "    if mask.any():\n",
    "        df[mask].disp(rows)\n",
    "        if quit:\n",
    "            raise Exception(f'{mask.sum()} duplicates detected')\n",
    "    return df[mask]\n",
    "\n",
    "def get_missing(df, rows=-1):\n",
    "    miss = df.isnull().mean()*100\n",
    "    if miss.any():\n",
    "        miss[miss>0].sort_values(ascending=False).round(1).disp(rows)\n",
    "    return miss\n",
    "\n",
    "# def inser(df, column, value, loc=0):\n",
    "#     df.insert(loc, column, value)\n",
    "#     return df\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    wrapper.__name__ = fcn.__name__\n",
    "    return wrapper\n",
    "\n",
    "for fcn in [\n",
    "    disp,\n",
    "    to_numeric,\n",
    "    prep,\n",
    "    # inser,\n",
    "    get_incoming,\n",
    "    get_duplicates,\n",
    "    get_missing,\n",
    "    groupb,\n",
    "    ]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setmeth(pd.DataFrame, fcn)\n",
    "    setmeth(pd.Series, wrap(fcn))\n",
    "\n",
    "\n",
    "def prediction(clf, X, y, cross=False):\n",
    "    Z = X.copy()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        p = (cross_val_predict(clf.model, X, y, cv=min(10,y.sum()), method='predict_proba') if cross and y.sum()>1 else clf.predict_proba(X)).T[1]\n",
    "    Z['prediction'] = p\n",
    "    Z['actual'] = y\n",
    "    Z['error'] = p - y\n",
    "    # Z['mse'] = (p - y)**2\n",
    "    # Z['log_loss'] = -(y*np.log(p.clip(eps)) + (1-y)*np.log((1-p).clip(eps)))\n",
    "    Z['cv_score'] = clf.best_loss\n",
    "    return Z\n",
    "setmeth(fl.automl.automl.AutoML, prediction)\n",
    "\n",
    "\n",
    "def custom_log_loss(X_val, y_val, estimator, labels, X_train, y_train, weight_val=None, weight_train=None, config=None, groups_val=None, groups_train=None):\n",
    "    \"\"\"Some (crse,styp) are entirely False which causes an error with built-in log_loss. We create a custom_log_loss simply to set labels=[False, True] https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML/\"\"\"\n",
    "    start = time.time()\n",
    "    y_pred = estimator.predict_proba(X_val)\n",
    "    pred_time = (time.time() - start) / len(X_val)\n",
    "    val_loss = log_loss(y_val, y_pred, labels=[False,True], sample_weight=weight_val)\n",
    "    y_pred = estimator.predict_proba(X_train)\n",
    "    train_loss = log_loss(y_train, y_pred, labels=[False,True], sample_weight=weight_train)\n",
    "    return val_loss, {\"val_loss\": val_loss, \"train_loss\": train_loss, \"pred_time\": pred_time}\n",
    "\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "@dataclasses.dataclass\n",
    "class Data():\n",
    "    cycle_date: str = ''\n",
    "    overwrite: set = None\n",
    "    seed: int = 42\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Because these take about 1 hour each, force user to manually delete drivetimes parquet files to avoid accidental deletion requring lengthy re-creation\n",
    "        self.overwrite = {x for x in listify(self.overwrite) if 'drivetimes' not in x}\n",
    "        self.cycle_date = dt(self.cycle_date, now)\n",
    "        self.cycle_date += pd.Timedelta(days=2-self.cycle_date.weekday())  # move to closest Wednesday (Flags release)\n",
    "        x = self.cycle_date.replace(month=10, day=1)\n",
    "        self.year = int(x.year) + int(self.cycle_date > x)\n",
    "        self.term_code = self.year*100+8\n",
    "        self.term_code, self.cycle_date, self.cycle_day, self.stable_date, self.year, self.term_desc, self.stem = self.get_cycle(self.term_code, self.cycle_date)\n",
    "        self.get_zips()\n",
    "        self.get_drivetimes()\n",
    "\n",
    "\n",
    "    def get_cycle(self, term_code, cycle_date):\n",
    "        term_code = int(term_code)\n",
    "        year = term_code//100\n",
    "        term_desc, stable_date = self.get_terms().loc[term_code,['term_desc','stable_date']]\n",
    "        cycle_date = dt(cycle_date).date()\n",
    "        stable_date = dt(stable_date).date()\n",
    "        cycle_day = (stable_date - cycle_date).days\n",
    "        stem = f'{term_code}_{cycle_date}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        # stem = f'{cycle_date}_{term_code}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        return term_code, cycle_date, cycle_day, stable_date, year, term_desc, stem\n",
    "\n",
    "\n",
    "    def get(self, fcn, dst, prereq=[], *, divide=True, read=True, suffix='.parquet', **kwargs):\n",
    "        nm = str(dst)\n",
    "        if '/' in nm:\n",
    "            dst = pathlib.Path(dst).with_suffix(suffix)\n",
    "            nm = dst.stem\n",
    "        else:\n",
    "            # dst = data/f\"{nm.split('_')[0]}/{self.term_code}/{dst}_{self.stem}{suffix}\"\n",
    "            dst = data/f\"{nm.split('_')[0]}/{self.term_code}/{self.cycle_date}/{dst}_{self.stem}{suffix}\"\n",
    "\n",
    "        if nm in self.overwrite:\n",
    "            del self[nm]\n",
    "            reset(dst)\n",
    "            self.overwrite.remove(nm)\n",
    "\n",
    "        new = False\n",
    "        if not nm in self:\n",
    "            if not dst.exists():\n",
    "                new = True\n",
    "                [f() for f in listify(prereq)]\n",
    "                print(f'\\ncreating {dst.name}: ', end='')\n",
    "                with codetiming.Timer():\n",
    "                    rslt = fcn(**kwargs)\n",
    "                    dump(dst, rslt)\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "            self[nm] = load(dst) if read else None\n",
    "        return self[nm], new\n",
    "##################################################\n",
    "################# get drivetimes #################\n",
    "##################################################\n",
    "    def get_zips(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pgeocode.Nominatim('us')._data  # get all zips\n",
    "                .prep()\n",
    "                .rename(columns={'postal_code':'zip'})\n",
    "                .query(\"state_code.notnull() & state_code not in [None,'mh']\")\n",
    "            )\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/zips')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_states(self):\n",
    "        return set(self.get_zips()['state_code'])\n",
    "\n",
    "\n",
    "    def get_drivetimes(self):\n",
    "        def fcn():\n",
    "            print()\n",
    "            campus_coords = {\n",
    "                's': '-98.215784,32.216217',\n",
    "                'm': '-97.432975,32.582436',\n",
    "                'w': '-97.172176,31.587908',\n",
    "                'r': '-96.467920,30.642055',\n",
    "                'l': '-96.983211,32.462267',\n",
    "                }\n",
    "            url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "            gdf = gpd.read_file(url).prep().set_index('zcta5ce20')  # get all ZCTA https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n",
    "            pts = gdf.sample_points(size=10, method=\"uniform\").explode().apply(lambda g: f\"{g.x},{g.y}\")  # sample 10 random points in each ZCTA\n",
    "            M = []\n",
    "            for k, v in campus_coords.items():\n",
    "                def fcn1():\n",
    "                    print()\n",
    "                    L = []\n",
    "                    i = 0\n",
    "                    di = 200\n",
    "                    I = pts.shape[0]\n",
    "                    while i < I:\n",
    "                        u = join([v, *pts.iloc[i:i+di]],';')\n",
    "                        url = f\"http://router.project-osrm.org/table/v1/driving/{u}?sources={0}&annotations=duration,distance&fallback_speed=1&fallback_coordinate=snapped\"\n",
    "                        response = requests.get(url).json()\n",
    "                        L.append(np.squeeze(response['durations'])[1:]/60)\n",
    "                        i += di\n",
    "                        print(k,i,round(i/I*100))\n",
    "                    df = pts.to_frame()[[]]\n",
    "                    df[k] = np.concatenate(L)\n",
    "                    return df\n",
    "                df, new = self.get(fcn1, root/f'geo/drivetimes_{k}')\n",
    "                M.append(df)\n",
    "            D = pd.concat(M, axis=1).groupb(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "\n",
    "            # There are a few USPS zips without equivalent ZCTA, so we assign them drivetimes for the nearest\n",
    "            Z = self.get_zips().merge(D.query(\"camp_code=='s'\"), how='left').set_index('zip')\n",
    "            mask = Z['drivetime'].isnull()  # zips without a ZTCA\n",
    "            Z = Z[['latitude','longitude']]\n",
    "            X = np.radians(Z[~mask])\n",
    "            Y = np.radians(Z[mask])\n",
    "            M = (\n",
    "                pd.DataFrame(haversine_distances(X, Y), index=X.index, columns=Y.index) # haversine distance between pairs with and without ZCTA\n",
    "                .idxmin()  # find nearest ZCTA\n",
    "                .reset_index()\n",
    "                .set_axis(['new_zip','zip'], axis=1)\n",
    "                .prep()\n",
    "                .merge(D)  # merge the drivetimes for that ZCTA\n",
    "                .drop(columns='zip')\n",
    "                .rename(columns={'new_zip':'zip'})\n",
    "            )\n",
    "            df = pd.concat([D,M], ignore_index=True)\n",
    "            return df\n",
    "        df, new = self.get(fcn, root/'geo/drivetimes', self.get_zips)\n",
    "        return df\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, show=False):\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        df, new = self.get(fcn, data/'terms')\n",
    "        return df\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id,\n",
    "                spriden_pidm as pidm,\n",
    "                spriden_last_name as last_name,\n",
    "                spriden_first_name as first_name,\n",
    "\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            # qry = f\"\"\"\n",
    "            # select distinct\n",
    "            #     spriden_id as id,\n",
    "            #     spriden_pidm as pidm\n",
    "            # from\n",
    "            #     {catalog}saturnspriden as A\n",
    "            # where\n",
    "            #     spriden_change_ind is null\n",
    "            #     and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "            #     and spriden_id REGEXP '^[0-9]+'\n",
    "            # \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self, early_stop=3):\n",
    "        # GA's should not have permissions to run this because it can see pii\n",
    "        counter = 0\n",
    "        divide = False\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > early_stop:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                current_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    current_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                term_code, current_date, current_day, stable_date, year, term_desc, stem = self.get_cycle(term_code, current_date)\n",
    "                def fcn():\n",
    "                    df = (\n",
    "                        self.get_spriden()[['pidm','id']]\n",
    "                        .assign(current_day=current_day, current_date=current_date)\n",
    "                        .merge(book.parse(sheet).prep(), on='id', how='right')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, flags_prc/f'{term_code}/{current_date}/flags_{stem}', read=False, divide=False)[1]:\n",
    "                    divide = True\n",
    "                    counter = 0\n",
    "                    dst = flags_prc/f'flags_{year}.parquet'\n",
    "                    rm(dst)\n",
    "        if divide:\n",
    "            print(divider)\n",
    "            self.combine_flags()\n",
    "\n",
    "\n",
    "    def combine_flags(self):\n",
    "        def fcn(year):\n",
    "            L = [pd.read_parquet(src) for A in flags_prc.iterdir() if A.is_dir() and str(year) in A.stem for B in A.iterdir() for src in B.glob('*.parquet')]\n",
    "            df = pd.concat(L, ignore_index=True).prep()\n",
    "            del L\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                if k in df:\n",
    "                    df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        for year in {int(x.stem)//100 for x in flags_prc.iterdir() if x.is_dir()}:\n",
    "            self.get(fcn, flags_prc/f'flags_{year}', read=False, year=year)\n",
    "\n",
    "\n",
    "    def get_flags(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                pd.read_parquet(flags_prc/f'flags_{self.year}.parquet')\n",
    "                .query(f\"current_date<='{self.cycle_date}'\")\n",
    "                .sort_values(['pidm','current_date'])\n",
    "                .drop_duplicates(subset=['pidm','term_code'], keep='last')\n",
    "            )\n",
    "            df.loc[~df['state'].isin(self.get_states()),'zip'] = pd.NA\n",
    "            df['zip'] = df['zip'].str.split('-', expand=True)[0].str[:5].to_numeric(errors='coerce')\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'flags', self.combine_flags)\n",
    "        return df\n",
    "##########################################\n",
    "############ get student data ############\n",
    "##########################################\n",
    "    def newest(self, qry, prt, tbl='', sel=''):\n",
    "        \"\"\"The OPEIR daily snapshot experienced occasional glitched causing incomplete copies.\n",
    "        Consequently, record can vanished then reappear later. This function fixes this issue.\"\"\"\n",
    "        prt = join(prt, ', ')\n",
    "        if tbl == '':\n",
    "            tbl = qry\n",
    "        if sel != '':\n",
    "            sel = ','+join(sel)\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    {prt}\n",
    "    ,current_date\n",
    "    ,min(current_date) over (partition by {prt}) as first_date  --first date this record appeared\n",
    "    ,max(current_date) over (partition by {prt}) as last_date  --last date this record appeared\n",
    "    ,least(greatest(timestamp('{self.cycle_date}'), min(current_date) over ()), max(current_date) over ()) as cycle_date  --clip cycle date between first & last date of ANY record\n",
    "from\n",
    "    {qry.strip()}\n",
    "qualify\n",
    "    cycle_date between first_date and last_date  -- keep records where cycle_date falls between that record's first & last appearance (+5 days for safety)\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select\n",
    "    *\n",
    "from {subqry(qry)}\n",
    "where\n",
    "    current_date <= '{self.cycle_date}'  -- discard records after cycle_date\n",
    "qualify\n",
    "    row_number() over (partition by {prt} order by current_date desc) = 1  -- keep most recent remaining record\n",
    "\"\"\"\n",
    "\n",
    "        qry = f\"\"\"\n",
    "select distinct\n",
    "    pidm\n",
    "    ,cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,last_date\n",
    "    ,{get_desc('term_code')}\n",
    "    ,{get_desc('levl_code')}\n",
    "    ,{get_desc('styp_code')}\n",
    "    ,{get_desc('camp_code')}\n",
    "    ,{get_desc('coll_code_1')}\n",
    "    ,{get_desc('dept_code')}\n",
    "    ,{get_desc('majr_code_1')}\n",
    "    --,gender\n",
    "    ,spbpers_sex as gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, (select stvnatn_nation from {catalog}saturnstvnatn where gorvisa_natn_code_issue = stvnatn_nation limit 1) as natn_desc\n",
    "    ,{coalesce('race_asian')}\n",
    "    ,{coalesce('race_black')}\n",
    "    ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "    ,{coalesce('race_native')}\n",
    "    ,{coalesce('race_pacific')}\n",
    "    ,{coalesce('race_white')}\n",
    "    {indent(sel)}\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {tbl}\n",
    "using\n",
    "    ({prt}, current_date)\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\"\"\"\n",
    "        return qry\n",
    "\n",
    "\n",
    "    def get_registrations(self, show=False):\n",
    "        def fcn():\n",
    "            # tbl = f'dev.opeir.opeirregistration_{self.term_desc}'\n",
    "            tbl = f'dev.opeir.registration_{self.term_desc}_v'\n",
    "            if spark.catalog.tableExists(tbl):\n",
    "                qry = self.newest(\n",
    "                    tbl = tbl,\n",
    "                    prt = ['pidm','crn'],\n",
    "                    sel = ['credit_hr as count', 'subj_code || crse_numb as crse_code'],\n",
    "                    qry = f\"\"\"\n",
    "    {tbl} as A\n",
    "where\n",
    "    credit_hr > 0\n",
    "    and subj_code <> 'INST'\"\"\")\n",
    "                A = run(qry, show)\n",
    "                B = A.groupb(['pidm','crse_code'])['count'].sum().reset_index('crse_code')\n",
    "                C = B.groupb('pidm')[['count']].sum().assign(crse_code='_tot_sch')\n",
    "                D = C.copy()\n",
    "                B['count'] = 1\n",
    "                D['count'] = 1\n",
    "                D['crse_code'] = '_headcnt'\n",
    "                E = D.copy()\n",
    "                E['crse_code'] = '_proba'\n",
    "                F = pd.concat([B,C,D,E])\n",
    "                G = A.drop(columns=['count','crse_code']).sort_values('current_date').groupb('pidm').last()\n",
    "                df = G.join(F)\n",
    "            else:\n",
    "                # placeholder if table DNE\n",
    "                df = pd.DataFrame(columns=['pidm','levl_code','styp_code','count','crse_code']).set_index('pidm')\n",
    "            df.get_duplicates(['pidm','crse_code'])\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'registrations')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_admissions(self, show=False):\n",
    "        def fcn():\n",
    "            def fcn1(season):\n",
    "                # tbl = f'dev.opeir.opeiradmissions_{season}{self.year}'\n",
    "                tbl = f'dev.opeir.admissions_{season}{self.year}_v'\n",
    "                return self.newest(\n",
    "                    tbl = tbl,\n",
    "                    prt = ['pidm', 'appl_no'],\n",
    "                    sel = [\n",
    "                        'appl_no',\n",
    "                        get_desc('apst_code'),\n",
    "                        get_desc('apdc_code'),\n",
    "                        get_desc('admt_code'),\n",
    "                        get_desc('saradap_resd_code'),\n",
    "                        'hs_percentile',\n",
    "                        # 'sbgi_code',\n",
    "                    ],\n",
    "                    qry = f\"\"\"\n",
    "    {tbl} as A\n",
    "inner join\n",
    "    {catalog}saturnstvapdc as B\n",
    "on\n",
    "    apdc_code = stvapdc_code\n",
    "where\n",
    "    stvapdc_inst_acc_ind is not null  --only accepted\"\"\")\n",
    "            L = [run(fcn1(season), show) for season in ['summer','fall']]\n",
    "            df = pd.concat(L, ignore_index=True)\n",
    "            mask = df.groupby('pidm', group_keys=False).apply(lambda x: (x['levl_code']=='ug').all() & (x['appl_no']==x['appl_no'].max()))\n",
    "            df = df.loc[mask]\n",
    "            df.get_duplicates()\n",
    "            return df\n",
    "        df, new = self.get(fcn, 'admissions')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_students(self):\n",
    "        def fcn():\n",
    "            df = (\n",
    "                self.get_admissions()\n",
    "                .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_drop'])\n",
    "                .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_drop'])\n",
    "                .prep().set_index('pidm').sort_index()\n",
    "            )\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "            df['eager'] = (dt(self.stable_date) - df['first_date']).dt.days\n",
    "            df['age'] = (dt(self.stable_date) - df['birth_date']).dt.days / 365\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "            df.get_duplicates()\n",
    "            df.get_incoming().query('current_date_drop.isnull()')['first_date'].value_counts().sort_index().disp(10)\n",
    "            return df.loc[:, ~df.columns.str.contains('_drop')]\n",
    "        df, new = self.get(fcn, 'students', [self.get_admissions,self.get_drivetimes,self.get_flags])\n",
    "        return df\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "@dataclasses.dataclass\n",
    "class Cohort(Data):\n",
    "    crse_code: str = '_headcnt'\n",
    "    is_learner: bool = True\n",
    "    idx: str = 'styp_desc'\n",
    "    features: dict = None\n",
    "    agg: tuple = ('styp_desc','camp_desc')\n",
    "    drop: list = None\n",
    "    flaml: dict = None\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.set_crse_code(self.crse_code)\n",
    "        self.agg = listify(self.agg)\n",
    "        self.drop = listify(self.drop)\n",
    "        self.flaml = dict() if self.flaml is None else self.flaml\n",
    "        kwargs = {k: self[k] for k in ['cycle_date','overwrite']}\n",
    "        self.current = Data(**kwargs)\n",
    "        self.stable = Data(**kwargs | {'cycle_date':self.stable_date})\n",
    "\n",
    "\n",
    "    def get_imputed(self):\n",
    "        def fcn():\n",
    "            def fcn1(df):\n",
    "                feat = listify(self.features.keys())\n",
    "                idx = difference(listify(self.idx)+listify(self.agg), feat)\n",
    "                # idx = difference(df.columns, feat)\n",
    "                X = df.fillna(self.features).prep(category=True).set_index(idx, append=True)[feat]\n",
    "                imp = mf.ImputationKernel(X.reset_index(drop=True), random_state=self.seed)\n",
    "                imp.mice(10)\n",
    "                XX = imp.complete_data().set_index(X.index)\n",
    "                XX.get_missing()\n",
    "                return XX\n",
    "            return {key: fcn1(df) for key, df in self.current.get_students().get_incoming().groupby(self.idx)}\n",
    "        dct, new = self.get(fcn, f'imputed', self.current.get_students, suffix='.pkl')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_enrollments(self, key='crse_code'):\n",
    "        grp = unique('crse_code',*listify(self.idx),key)\n",
    "        g = lambda X, Y: X.join(Y, rsuffix='_y').get_incoming().groupb(grp)['count'].sum()  # get stuff from Y that is not in X\n",
    "        df = pd.DataFrame({\n",
    "            'current':g(self['current'].get_students(), self['stable'].get_registrations()),\n",
    "            'actual' :g(self['stable'].get_registrations(), self['stable'].get_students()),\n",
    "            }).fillna(0)\n",
    "        df['mlt'] = df['actual'] / df['current']\n",
    "        return df.sort_index()\n",
    "\n",
    "\n",
    "    def set_crse_code(self, crse_code):\n",
    "        \"\"\"added to allow a single AMP instance to run all courses in its year to fix out of memory issues\"\"\"\n",
    "        self.crse_code = crse_code\n",
    "        J = {'prepared','models','predictions','forecasts'}\n",
    "        self.overwrite |= {f\"{j}_{crse_code}\" for j in J.intersection(self.overwrite)}\n",
    "        for k in {k for j in J for k in self.__dict__.keys() if j in k}:\n",
    "            del self[k]\n",
    "\n",
    "\n",
    "    def get_prepared(self):\n",
    "        def fcn():\n",
    "            g = lambda k, v, n=None: self[k].get_registrations().query(f\"crse_code=='{v}'\")['count'].rename(k if n is None else n)\n",
    "            Z = {key: X\n",
    "                .join(g('current', '_tot_sch', '_tot_sch'))\n",
    "                .join(g('current', self.crse_code).astype('boolean'))\n",
    "                .join(g('stable' , self.crse_code).astype('boolean'))\n",
    "                .fillna({'_tot_sch':0, 'current':False, 'stable':False})\n",
    "                for key, X in self.get_imputed().items()}\n",
    "            {key: z.get_duplicates() for key, z in Z.items()}\n",
    "            return {key: [X, X.pop('stable')] for key, X in Z.items()}\n",
    "        dct, new = self.get(fcn, f'prepared_{self.crse_code}', self.get_imputed, suffix='.pkl')\n",
    "        return dct\n",
    "\n",
    "\n",
    "    def get_models(self):\n",
    "        \"\"\"train model - biggest bottleneck - can we run multiple (crse_code, year) in parallel?\"\"\"\n",
    "        def fcn():\n",
    "            def fcn1(key, Z):\n",
    "                log_file = data/f'models/{self.term_code}/flaml_{self.crse_code}_{self.stem}_{key}.log'\n",
    "                reset(log_file)\n",
    "                dct = {\n",
    "                    'time_budget':30,\n",
    "                    # 'max_iter': 100,\n",
    "                    'task':'classification',\n",
    "                    'log_file_name': log_file,\n",
    "                    'log_type': 'all',\n",
    "                    'log_training_metric':True,\n",
    "                    'verbose':0,\n",
    "                    'metric':custom_log_loss,\n",
    "                    'eval_method':'cv',\n",
    "                    'n_splits':3,\n",
    "                    'seed':self.seed,\n",
    "                    # 'early_stop':True,\n",
    "                    'estimator_list': ['xgboost'],\n",
    "                } | self.flaml\n",
    "                clf = fl.AutoML(**dct)\n",
    "                clf.fit(*Z, **dct)\n",
    "                return clf\n",
    "            return {key: fcn1(key, Z) for key, Z in self.get_prepared().items()}\n",
    "        if self.is_learner:\n",
    "            clf, new = self.get(fcn, f'models_{self.crse_code}', self.get_prepared, suffix='.pkl')\n",
    "        else:\n",
    "            clf = None\n",
    "        return clf\n",
    "\n",
    "\n",
    "    def get_predictions(self, learners=dict()):\n",
    "        def fcn():\n",
    "            L = [\n",
    "                clf.prediction(*self.get_prepared()[key], cross=self.year==learner.year)\n",
    "                .assign(\n",
    "                    crse_code=self.crse_code,\n",
    "                    prediction_year=self.year,\n",
    "                    learner_year=learner.year,\n",
    "                    mlt=learner.get_enrollments().loc['_headcnt',*listify(key)]['mlt'],\n",
    "                )\n",
    "                for year, learner in learners.items() if learner.is_learner\n",
    "                for key, clf in learner.get_models().items()]\n",
    "            return pd.concat(L).prep(category=True) if len(L)>0 else pd.DataFrame()\n",
    "        df, new = self.get(fcn, f'predictions_{self.crse_code}', self.get_prepared)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_forecasts(self):\n",
    "        def fcn():\n",
    "            err = [\n",
    "                'cv_score',\n",
    "                # 'mse',\n",
    "                # 'mae',\n",
    "                # 'log_loss',\n",
    "                ]\n",
    "            agg = {'prediction':'sum'}\n",
    "            if self.is_learner:\n",
    "                agg |= {k:'mean' for k in err}\n",
    "\n",
    "            def fcn1(key):\n",
    "                df = (\n",
    "                    self.get_predictions()\n",
    "                    .assign(prediction=lambda Z: Z['prediction']*Z['mlt'])\n",
    "                    .groupb(unique('crse_code',*listify(self.idx),key,'prediction_year','learner_year'))\n",
    "                    .agg(agg)\n",
    "                )\n",
    "                df['prediction'] = df['prediction'].round()\n",
    "                if self.is_learner:\n",
    "                    df = df.join(self.get_enrollments(key)['actual']).fillna(0)\n",
    "                    df['error'] = df['prediction'] - df['actual']\n",
    "                    df['error_pct'] = df['error'] / df['actual'] * 100\n",
    "                    df[err] *= 100\n",
    "                    df = df[['prediction','actual','error','error_pct',*err]]\n",
    "                return df.prep()\n",
    "            return {key: fcn1(key) for key in self.agg}\n",
    "        dct, new = self.get(fcn, f'forecasts_{self.crse_code}', self.get_predictions, suffix='.pkl')\n",
    "        return dct\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "@dataclasses.dataclass\n",
    "class Amp(Cohort):\n",
    "    years: tuple = (2024,2025)\n",
    "    crse_codes: tuple = ('_headcnt',)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print(self.cycle_date)\n",
    "        self.cycle_date = dt(self.cycle_date, now)\n",
    "        self.cycle_date -= pd.Timedelta(days=(self.cycle_date.weekday()-2)%7)  # move to preceeding Wednesday (flags release)\n",
    "        super().__post_init__()\n",
    "        self.crse_codes = listify(self.crse_codes, sort=True)\n",
    "        self.years = listify(self.years, sort=True, reverse=True)\n",
    "        self.prediction_year = max(self.years)\n",
    "        kwargs = {k: copy.deepcopy(self[k]) for k in ['features','idx','agg','flaml','overwrite']}\n",
    "        self.cohorts = {year: Cohort(cycle_date=self.cycle_date.replace(year=year), is_learner=year<self.prediction_year, **kwargs) for year in self.years}\n",
    "\n",
    "\n",
    "    def run(self, crse_code):\n",
    "        for year, cohort in self.cohorts.items():\n",
    "            cohort.set_crse_code(crse_code)\n",
    "            # cohort.get_students()\n",
    "            # cohort.get_imputed()\n",
    "            # cohort.get_prepared()\n",
    "            if crse_code == '_proba':\n",
    "                cohort[f'prepared_{crse_code}'] = {key: [X[difference(X.columns, [*races,'gender','international'])] ,y] for key, [X,y] in cohort.get_prepared().items()}\n",
    "            cohort.get_models()\n",
    "        for year, cohort in self.cohorts.items():\n",
    "            cohort.get_predictions(self.cohorts)\n",
    "        rm(data/'prepared')\n",
    "        return self.cohorts\n",
    "\n",
    "\n",
    "    def get_results(self):\n",
    "        def fcn():\n",
    "            results = dict()\n",
    "            for crse_code in self.crse_codes:\n",
    "                if crse_code != '_proba':\n",
    "                    for year, cohort in self.run(crse_code).items():\n",
    "                        for key, df in cohort.get_forecasts().items():\n",
    "                            results.setdefault(key, []).append(df)\n",
    "            return {key: pd.concat(L).sort_index(ascending=['year' not in k for k in L[0].index.names]) for key, L in results.items()}\n",
    "        dct, new = self.get(fcn, 'results', suffix='.pkl')\n",
    "        return dct\n",
    "    \n",
    "\n",
    "    def get_em_report(self):\n",
    "        def fcn():\n",
    "            self.run('_proba')\n",
    "            self.get_spriden().set_index('pidm').join(self.cohorts[self.prediction_year].get_predictions(), how='inner')\n",
    "            return df[[*df.loc[:,:'prediction'].columns,'learner_year']]\n",
    "            # df = self.cohorts[self.prediction_year].get_predictions()\n",
    "            # return self.get_spriden().set_index(['id','pidm']).join(df, how='inner')[[*df.loc[:,:'prediction'].columns,'learner_year']]#.reset_index()\n",
    "        df, new = self.get(fcn, data/f\"reports/{self.term_code}/{self.cycle_date}/AMP_{self.cycle_date}_EM\", suffix='.csv')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_reports(self):\n",
    "\n",
    "        self.get_em_report()\n",
    "\n",
    "        instructions = pd.DataFrame({\"\":[\n",
    "            f\"Admitted Matriculation Projections (AMP) for {self.cycle_date}\",\n",
    "            '',\n",
    "            f'''Executive Summary''',\n",
    "            f'''AMP is a predictive model designed to forecasr the incoming (not continuing) Fall cohort to help leaders proactively allocate resources (instructors, sections, labs, etc) in advance.''',\n",
    "            f'''It is intended to supplement, not replace, the knowledge, expertise, and insights developed by institutional leaders over years of experience.''',\n",
    "            f'''Like all AI/ML models (and humans), AMP is fallible and should be used with caution.''',\n",
    "            f'''AMP learns exclusively from historical data captured in EM’s Flags reports and IDA’s daily admissions and registration snapshots.''',\n",
    "            f'''It cannot account for factors not present in these datasets, including curriculum changes, policy shifts, structural changes, demographic variation, changes in oversight, etc.''',\n",
    "            f'''''',\n",
    "            f'''AMP provides both “Summary” and “Details” files. For most users, rows in the “Summary” file with learner_year = 2024 will suffice.''',\n",
    "            f'''Because AMP’s accuracy varies across courses, the “Details” file includes historical error analyses to help users assess the reliability of each forecast.''',\n",
    "            '',\n",
    "            f'''As widely requested, AMP includes predictions for the 2025 cohort in Ft. Worth, despite having no prior Ft. Worth FTIC example to learn from.''',\n",
    "            f'''These are a good-faith effort to offer my best data-driven insights, but due to the lack of training data,''',\n",
    "            f'''they are inherently more speculative and should be treated with lower confidence (details below).''',\n",
    "            f'''''',\n",
    "            f'''Definitions''',\n",
    "            f'''crse_code = course code (_headcnt = total headcount)''',\n",
    "            f'''styp_desc = student type; returning = re-enrolling after a previous attempt (not continuing)''',\n",
    "            f'''prediction_year = cohort being forecast''',\n",
    "            f'''learner_year = cohort used to train AMP''',\n",
    "            f'''prediction = forecast headcount''',\n",
    "            f'''*actual = true headcount''',\n",
    "            f'''*error = prediction - actual''',\n",
    "            f'''*error_pct = error / actual * 100''',\n",
    "            f'''*cv_score = average validation log-loss from 3-fold cross-validation''',\n",
    "            f'''*=appears only in “Details” & not available for 2025 (since actuals are not yet known)''',\n",
    "            '',\n",
    "            f'''Methodology''',\n",
    "            f'''AMP uses XGBoost, a machine learning algorithm, to forecast the number, characteristics, and likely course enrollments of incoming Fall students.''',\n",
    "            f'''Predictions are based on application and pre-semester engagement (orientation, course registration, financial aid, etc.) from EM’s Flags and IDA’s daily snapshots.''',\n",
    "            f'''For each student admitted for Fall 2025, AMP identifies similar students from past Fall admits, analyzes their course enrollments (if any),''',\n",
    "            f'''learns relevant patterns, then forecasts Fall 2025 course enrollment for the admitted student in question.''',\n",
    "            f'''More precisely, for each (incoming student, course)-pair, AMP assigns a probability whether that student will be enrolled in that course on the Friday after Census.''',\n",
    "            f'''These (student, course)-level predictions are then aggregated in many different ways to forecast headcounts for courses, campuss, majors, colleges, TSI statuses, etc.''',\n",
    "            f'''These appear on different sheets in this workbook.''',\n",
    "            f'''''',\n",
    "            f'''Since admissions and registration data evolve through the spring and summer, AMP is trained only on data available as of the same date in previous years.''',\n",
    "            f'''AMP's forecast for Ft. Worth's 2025 cohort are necessarily based on previous Stephenville cohorts since no Ft. Worth FTIC's existed on this date.''',\n",
    "            f'''Suppose AMP predicts, \"Based on similar FTIC's in Stephenville in 2024, I predict Alice has a 75% probability to matriculate in Fall 2025\".''',\n",
    "            f'''If Alice is applying to Ft. Worth, then 0.75 is added to Ft. Worth's forecast.''',\n",
    "            f'''However, AMP can not yet understand how to adjust its 75% projection to reflect how Ft. Worth FTIC's behave differently than Stephenville FTIC since there are no Ft. Worth FTIC's to learn from.''',\n",
    "            f'''Though not ideal, this appears to be the most reasonable mechanism to forecast Ft. Worth FTIC in the absence of training examples.''',\n",
    "            '',\n",
    "            f'''AMP is trained separately using the 2024, 2023, and 2022 cohorts.''',\n",
    "            f'''Most users should focus on prediction_year = 2025 and learner_year = 2024, as 2025 is likely to resemble 2024 more closely than 2023 & 2022.''',\n",
    "            f'''Users with domain expertise may choose to incorporate older cohorts (e.g., weighted average of learner_years 2024, 2023, & 2022) if they believe those years are similarly relevant.''',\n",
    "            '',\n",
    "            f'''Rows for prediction_year < 2025 appear only in the “Details” file and include retrospective \"predictions\" and actual outcomes.''',\n",
    "            f'''This allows users to assess AMP's ability to forecast each individual course and calibrate their confidence accordingly.''',\n",
    "            '',\n",
    "            f'''Predictions for small values are less reliable than for large numbers (central limit theorem).''',\n",
    "            '',\n",
    "            f'''AMP only models students who have already applied and been admitted (eager).''',\n",
    "            f'''However, more students will apply between now and start of term, especially transfer & returning (lagging).''',\n",
    "            f'''AMP generates forecasts based on eager students then inflates using the eager-lagging ratio from the learner_year.''',\n",
    "            f'''This assumes the eager-lagging behavior will be approximately the same this year. approach has proven sufficiently accurate in prior years.''',\n",
    "            f'''While this assumption cannot be verified in advance, some assumption is needed. This one has proven sufficiently accurate in past cycles.''',\n",
    "            '',\n",
    "            f'''Dr. Scott Cook is eager to provide as much additional detail on AMP's workings as the user desires - email scook@tarleton.edu.''',\n",
    "            f'''source code: https://github.com/drscook/admitted_matriculation_predictor'''\n",
    "        ]})\n",
    "\n",
    "        def format_xlsx(sheet):\n",
    "            from openpyxl.styles import Alignment\n",
    "            sheet.auto_filter.ref = sheet.dimensions\n",
    "            for cell in sheet[1]:\n",
    "                cell.alignment = Alignment(horizontal=\"left\")\n",
    "            for j, column in enumerate(sheet.columns):\n",
    "                width = max(len(str(cell.value))+3*(i==0) for i, cell in enumerate(column))\n",
    "                sheet.column_dimensions[chr(65+j)].width = width\n",
    "            sheet.freeze_panes = \"A2\"\n",
    "\n",
    "        def fcn_details(df):\n",
    "            return df\n",
    "\n",
    "        def fcn_summary(df):\n",
    "            return df.query(f\"prediction_year==prediction_year.max()\").iloc[:,:1]\n",
    "\n",
    "        for nm, fcn in {'details':fcn_details, 'summary':fcn_summary}.items():\n",
    "            src = \"report.xlsx\"\n",
    "            reset(src)\n",
    "            with pd.ExcelWriter(src, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "                instructions.to_excel(writer, sheet_name='instructions', index=False)\n",
    "                for key, df in self.get_results().items():\n",
    "                    fcn(df).reset_index().round().prep().to_excel(writer, sheet_name=key, index=False)\n",
    "                    format_xlsx(writer.sheets[key])\n",
    "            dst = data/f\"reports/{self.term_code}/{self.cycle_date}/AMP_{self.cycle_date}_{nm}.xlsx\"\n",
    "            reset(dst)\n",
    "            shutil.copy(src, dst)\n",
    "            rm(src)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "self = Amp(\n",
    "    years = [2022,2023,2024,2025],\n",
    "    flaml= {\n",
    "        'time_budget': 60,\n",
    "    },\n",
    "\n",
    "    overwrite = set({\n",
    "        # 'terms',\n",
    "        # 'zips',\n",
    "        # 'registrations',\n",
    "        # 'admissions',\n",
    "        # 'flags',\n",
    "        # 'students',\n",
    "        # 'imputed',\n",
    "        # 'prepared',\n",
    "        # 'models',\n",
    "        # 'predictions',\n",
    "        # 'forecasts',\n",
    "        # 'results',\n",
    "        'reports',\n",
    "    }),\n",
    "\n",
    "    crse_codes = {\n",
    "    '_proba',\n",
    "    '_headcnt',\n",
    "    'math1314',\n",
    "    'biol1407',\n",
    "    'comm1311',\n",
    "    'agec2317',\n",
    "    'ansc1119',\n",
    "    'ansc1319',\n",
    "    'anth2302',\n",
    "    'anth2351',\n",
    "    'arts1301',\n",
    "    'arts1303',\n",
    "    'arts1304',\n",
    "    'arts3331',\n",
    "    'biol1305',\n",
    "    'biol1406',\n",
    "    'biol1407',\n",
    "    'biol2401',\n",
    "    'biol2402',\n",
    "    'busi1301',\n",
    "    'busi1307',\n",
    "    'chem1111',\n",
    "    'chem1112',\n",
    "    'chem1302',\n",
    "    'chem1311',\n",
    "    'chem1312',\n",
    "    'chem1407',\n",
    "    'chem1409',\n",
    "    'cnst1301',\n",
    "    'comm1311',\n",
    "    'comm1315',\n",
    "    'comm2302',\n",
    "    'crij1301',\n",
    "    'dram1310',\n",
    "    'dram2361',\n",
    "    'easc2310',\n",
    "    'econ1301',\n",
    "    'econ2301',\n",
    "    'educ1301',\n",
    "    'engl1301',\n",
    "    'engl1302',\n",
    "    'engl2307',\n",
    "    'engl2320',\n",
    "    'engl2321',\n",
    "    'engl2326',\n",
    "    'engl2340',\n",
    "    'engl2350',\n",
    "    'engl2360',\n",
    "    'engl2362',\n",
    "    'engl2364',\n",
    "    'engl2366',\n",
    "    'engl2368',\n",
    "    'engr1211',\n",
    "    'engr2303',\n",
    "    'envs1302',\n",
    "    'fina1360',\n",
    "    'geog1303',\n",
    "    'geog1320',\n",
    "    'geog1451',\n",
    "    'geog2301',\n",
    "    'geol1403',\n",
    "    'geol1404',\n",
    "    'geol1407',\n",
    "    'geol1408',\n",
    "    'govt2305',\n",
    "    'govt2306',\n",
    "    'hist1301',\n",
    "    'hist1302',\n",
    "    'hist2321',\n",
    "    'hist2322',\n",
    "    'huma1315',\n",
    "    'kine1301',\n",
    "    'kine1338',\n",
    "    'kine2315',\n",
    "    'math1314',\n",
    "    'math1316',\n",
    "    'math1324',\n",
    "    'math1325',\n",
    "    'math1332',\n",
    "    'math1342',\n",
    "    'math1352',\n",
    "    'math2412',\n",
    "    'math2413',\n",
    "    'musi1303',\n",
    "    'musi1310',\n",
    "    'musi1311',\n",
    "    'musi2350',\n",
    "    'musi3325',\n",
    "    'phil1301',\n",
    "    'phil1304',\n",
    "    'phil2303',\n",
    "    'phil3301',\n",
    "    'phys1302',\n",
    "    'phys1401',\n",
    "    'phys1402',\n",
    "    'phys1403',\n",
    "    'phys1411',\n",
    "    'phys2425',\n",
    "    'phys2426',\n",
    "    'psyc2301',\n",
    "    'psyc3303',\n",
    "    'psyc3307',\n",
    "    'soci1301',\n",
    "    'soci1306',\n",
    "    'soci2303',\n",
    "    'univ0010',\n",
    "    'univ0200',\n",
    "    'univ0204',\n",
    "    'univ0301',\n",
    "    'univ0314',\n",
    "    'univ0324',\n",
    "    'univ0332',\n",
    "    'univ0342',\n",
    "    },\n",
    "    agg = [\n",
    "        'styp_desc',\n",
    "        'camp_desc',\n",
    "        'coll_desc',\n",
    "        'dept_desc',\n",
    "        'majr_desc',\n",
    "        'hs_qrtl',\n",
    "        'tsi_math',\n",
    "        'tsi_reading',\n",
    "        'tsi_writing',\n",
    "        # 'gender',\n",
    "        # *races,\n",
    "        # 'international',\n",
    "        'resd_desc',\n",
    "        'oriented',\n",
    "        'waiver',\n",
    "        'lgcy',\n",
    "    ],\n",
    "    features = {\n",
    "        'act_equiv':pd.NA,\n",
    "        'age':pd.NA,\n",
    "        'camp_desc':'stephenville',\n",
    "        'drivetime':pd.NA,\n",
    "        'eager':pd.NA,\n",
    "        'fafsa': False,\n",
    "        'finaid': False,\n",
    "        'gap_score':0,\n",
    "        'gender':pd.NA,\n",
    "        'hs_qrtl':pd.NA,\n",
    "        'international':False,\n",
    "        'lgcy':False,\n",
    "        'oriented':False,\n",
    "        **{r: False for r in races},\n",
    "        'schlship':False,\n",
    "        'ssb':False,\n",
    "        'tsi_math':False,\n",
    "        'tsi_reading':False,\n",
    "        'tsi_writing':False,\n",
    "        'verified':False,\n",
    "        'waiver':False,\n",
    "    },\n",
    ")\n",
    "\n",
    "# self.process_flags(early_stop=1)\n",
    "# self.run('_headcnt')bj,bmb\n",
    "# self.get_reports()\n",
    "# cohort.set_crse_code(crse_code)\n",
    "# self.get_em_report()\n",
    "# df\n",
    "\n",
    "#EM probabilities checklist\n",
    "#edit data path\n",
    "#comment race, gender, international features\n",
    "#comment all crse_codes but _headcnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fc6c88-adc8-4add-8f43-3db62007a422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select distinct\n",
    "    spriden_id as id,\n",
    "    spriden_pidm as pidm,\n",
    "    spriden_last_name as last_name,\n",
    "    spriden_first_name as first_name,\n",
    "\n",
    "from\n",
    "    {catalog}saturnspriden as A\n",
    "where\n",
    "    spriden_change_ind is null\n",
    "    and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "    and spriden_id REGEXP '^[0-9]+'\n",
    "\"\"\"\n",
    "run(qry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2df0e4f-c490-4df9-9635-5dddaf081abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_spriden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34dcd33c-2206-4fad-9064-0041a9d87723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "p = pathlib.Path()\n",
    "list(p.iterdir())\n",
    "str(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ef7098-ce30-4f92-ba1a-a8ad751c6dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8cf6fa-8064-46ff-8758-982841cbaf8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dst = '/Volumes/aiml/amp/amp_files/2025/data/em/202508/em_report_202508_2025-04-09_+156.csv'\n",
    "rm(dst)\n",
    "self.em_report.prep().to_csv('/Volumes/aiml/amp/amp_files/2025/data/em/202508/em_report_202508_2025-04-09_+156.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f76402-4668-452a-9cb0-38240f7f5da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# [*df.loc[:,:'prediction'].columns, 'learner_year']\n",
    "# df.columns.\n",
    "self.get_spriden().set_index(['id','pidm']).join(df, how='inner')[[*df.loc[:,:'prediction'].columns,'learner_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4540b9c3-0cd9-44d7-b882-ff8a6cab3bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "A = df.loc[:,:'prediction']\n",
    "B = self.get_spriden().set_index(['id','pidm']).join(df.loc[:,:'prediction'])\n",
    "C = B.join(A, how='inner')\n",
    "A.shape, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fdb5db-de80-40b0-b9cb-18831831d1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crse_code = '_proba'\n",
    "for year, cohort in self.cohorts.items():\n",
    "    cohort.set_crse_code(crse_code)\n",
    "    cohort.get_prepared()\n",
    "    if crse_code == '_proba':\n",
    "        cohort[f'prepared_{crse_code}'] = {key: [X[difference(X.columns, [*races,'gender','international'])] ,y] for key, [X,y] in cohort.get_prepared().items()}\n",
    "    cohort.get_models()\n",
    "for year, cohort in self.cohorts.items():\n",
    "    cohort.get_predictions(self.cohorts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cb285a8-96b5-44d8-8cfd-12818098179f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.cohorts[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c51414c5-a119-46b7-a882-6aba1b552a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc86cb2-fcc3-41c3-b7b6-e316695aea8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.cohorts[2025].forecasts__headcnt['camp_desc']#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660400fe-968c-40ec-b9c7-4d9f49266232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qry = f\"\"\"\n",
    "select\n",
    "    min(current_date) over ()\n",
    "    ,max(current_date) over ()\n",
    "    ,timestamp('{self.cycle_date}')\n",
    "    ,greatest(timestamp('2025-04-09'), min(current_date) over ())\n",
    "    \n",
    "    --least(greatest(timestamp('{self.cycle_date}'), min(current_date) over ()), max(current_date) over ()) as cycle_date  --clip cycle date between first & last date of ANY record\n",
    "from\n",
    "    --dev.opeir.registration_fall2024_v\n",
    "    dev.opeir.opeirregistration_fall2024\n",
    "limit 5\"\"\"\n",
    "run(qry, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94ecf72-9e16-4e14-aa7a-e84fbb477a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_predictions().drop(columns=list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad23ad1-5885-4100-bb9b-0cee5ee87720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "self.get_predictions().to_csv(data/'predictions/202508/predictions__headcnt_202508_2025-04-02_+163.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e96577-173f-47e0-b050-34e0c797ef99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for nm in ['details','summary']:\n",
    "    stem = f\"results/{self.term_code}/AMP_{self.cycle_date}_{nm}.xlsx\"\n",
    "    src = data/stem\n",
    "    dst = pathlib.Path(\"/Volumes/aiml/scook/scook_files\")/stem\n",
    "    reset(dst)\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c413e2e-331d-4a79-a162-e48909afb954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## useful old code\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#                 'ssbsect_crn':'crn',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, overwrite=False, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show)\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations', overwrite)\n",
    "\n",
    "#         S = self.students[['pidm','term_code','styp_code']]\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "    #     def fcn():\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     sfrstcr_pidm as pidm\n",
    "#     ,sfrstcr_term_code as term_code\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     --and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "# group by\n",
    "#     sfrstcr_pidm\n",
    "#     ,sfrstcr_term_code\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     pidm\n",
    "#     ,term_code\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# don't delete - could be useful & was hard to create\n",
    "            # stat_codes = ['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY'] # not AK & HI b/c can't get driving distance\n",
    "#     ,{get_desc('spraddr_cnty_code')[0]}\n",
    "#     ,{get_desc('spraddr_stat_code')[0]}\n",
    "#     ,zip_code\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#         ,try_to_number(left(spraddr_zip, 5), '00000') as zip_code\n",
    "#         ,case\n",
    "#             when spraddr_atyp_code = 'PA' then 6\n",
    "#             when spraddr_atyp_code = 'PR' then 5\n",
    "#             when spraddr_atyp_code = 'MA' then 4\n",
    "#             when spraddr_atyp_code = 'BU' then 3\n",
    "#             when spraddr_atyp_code = 'BI' then 2\n",
    "#             when spraddr_atyp_code = 'P1' then 1\n",
    "#             when spraddr_atyp_code = 'P2' then 0\n",
    "#             end as spraddr_atyp_rank\n",
    "#     from\n",
    "#         {catalog}spraddr_amp_v\n",
    "#     where\n",
    "#         spraddr_stat_code in ('{join(stat_codes, \"','\")}')\n",
    "#         and spraddr_zip is not null\n",
    "#     qualify\n",
    "#         row_number() over (partition by spraddr_pidm order by spraddr_atyp_rank desc, spraddr_seqno desc) = 1\n",
    "# )\n",
    "# on\n",
    "#     pidm = spraddr_pidm\n",
    "\n",
    "# {get_desc('spraddr_cnty_code')[1]}\n",
    "# {get_desc('spraddr_stat_code')[1]}\n",
    "\n",
    "\n",
    "\n",
    "    # def get_zips(self, show=False):\n",
    "    #     \"\"\"takes ~3 hours toget zip codes and find nearest point on road network to the provided representative point\"\"\"\n",
    "    #     def fcn():\n",
    "    #         from pgeocode import Nominatim\n",
    "    #         nomi = Nominatim('us')\n",
    "    #         df = nomi.query_postal_code(pd.Series(nomi._data['postal_code'])).query(\"state_code.notnull() & state_code not in ['AK', 'HI', 'MH']\").prep().set_index('postal_code').rename_axis('zip')\n",
    "    #         nearest = lambda x: join(requests.get(f\"http://router.project-osrm.org/nearest/v1/driving/{x['longitude']},{x['latitude']}\").json()['waypoints'][0]['location'],',')\n",
    "    #         df['point'] = df.apply(nearest, axis=1)\n",
    "    #         return df\n",
    "    #     df, new = self.get(fcn, root/'zips')\n",
    "    #     self.states = set(df['state_code'])\n",
    "    #     return df\n",
    "\n",
    "\n",
    "    # def get_drivetimes(self, show=False):\n",
    "    #     def fcn():\n",
    "    #         campus_coords = {\n",
    "    #             's': [-98.215784,32.216217],\n",
    "    #             # 'm': '-97.432975,32.582436',\n",
    "    #             # 'w': 76708,\n",
    "    #             # 'r': 77807,\n",
    "    #             }\n",
    "\n",
    "    #         url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip\"\n",
    "    #         gdf = gpd.read_file(url).prep().set_index('zcta5ce20').iloc[:5]\n",
    "    #         pts = gdf.sample_points(size=5,method=\"uniform\").explode()#.apply(lambda geom: f\"{geom.x},{geom.y}\")\n",
    "    #         df = pts.to_frame()[[]]\n",
    "    #         url = \"http://router.project-osrm.org/table/v1/driving\"\n",
    "    #         headers = {\"Content-Type\": \"application/json\"}\n",
    "    #         for k, v in campus_coords.items():\n",
    "    #             u = [v, *pts]\n",
    "    #             print(u)\n",
    "    #             data = {\n",
    "    #                 \"coordinates\": u,\n",
    "    #                 \"annotations\": [\"duration\", \"distance\"],\n",
    "    #                 \"sources\": 0,\n",
    "    #             }\n",
    "    #             response = requests.post(url, json=data, headers=headers)\n",
    "    #             print(response.json())\n",
    "    #             assert 1==2\n",
    "\n",
    "            # for k, v in campus_coords.items():\n",
    "            #     u = join([v, *pts], ';')\n",
    "            #     url = f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={0}&annotations=duration,distance\"\n",
    "            #     print(url)\n",
    "            #     print(requests.get(url))\n",
    "            #     df[k] = np.squeeze(requests.get(url).json()['durations'])[1:]/60\n",
    "    #         # df.disp(10)\n",
    "    #         # df = df.groupby('zip').min()\n",
    "    #         # df.disp(10)\n",
    "    #         df = df.groupby(level=0).min().stack().reset_index().set_axis(['zip','camp_code','drivetime'], axis=1)\n",
    "    #         return df\n",
    "\n",
    "            # df = self.zips.iloc[34339:34349].copy()\n",
    "            # u = join(df.apply(lambda x: f\"{x['longitude']},{x['latitude']}\", axis=1),';')\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     df[k] = np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={df.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['distances'])/1609\n",
    "\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     df[k] = np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={df.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['durations'])/60\n",
    "\n",
    "            # self.zips = self.zips.iloc[34339:34349]\n",
    "            # self.zips.disp(20)\n",
    "            # u = join(self.zips['point'],';')\n",
    "            \n",
    "            # dct = dict()\n",
    "            # for k, z in campus_zips.items():\n",
    "            #     i = self.zips.index.get_loc(z)\n",
    "            #     print(self.zips.iloc[i])\n",
    "            #     url = f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={0}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     print(url)\n",
    "            #     response = requests.get(url).json()\n",
    "            #     for k,v in response.items():\n",
    "            #         print(k)\n",
    "            #         display(v)\n",
    "            #         print()\n",
    "            #     print(response['distance'])\n",
    "            #     dct[k] = np.squeeze(response['durations'])\n",
    "\n",
    "            # # dct = {k: np.squeeze(\n",
    "            # #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={self.zips.index.get_loc(z)}&fallback_speed=600&fallback_coordinate=snapped\"\n",
    "            # #     ).json()['durations'])/60 for k, z in campus_zips.items()}\n",
    "            # dct = {k: np.squeeze(\n",
    "            #     requests.get(f\"http://router.project-osrm.org/table/v1/driving/{u}?destinations={self.zips.index.get_loc(z)}&annotations=duration,distance&fallback_speed=26.8&fallback_coordinate=snapped\"\n",
    "            #     ).json()['distances']) for k, z in campus_zips.items()}\n",
    "\n",
    "            # df = pd.DataFrame(dct, index=self.zips.index).stack().rename_axis(['zip','camp_code']).rename('drivetime') / 1609\n",
    "            # return df\n",
    "            # print()\n",
    "            # dct = {k: self.zips.loc[y] for k, y in {\n",
    "            #     's': 76402,\n",
    "            #     'm': 76036,\n",
    "            #     'w': 76708,\n",
    "            #     'r': 77807,\n",
    "            #     }.items()}\n",
    "            # L = [\n",
    "            #     self.get(\n",
    "            #         lambda: X.apply(get_driving_distance, y=y, axis=1).rename('distance').reset_index().assign(camp_code=k),\n",
    "            #         root/f'distances/distances_{s}_{k}',\n",
    "            #         divide=False,\n",
    "            #     )[0] for s, X in self.zips.groupby('state_code') for k, y in dct.items()]\n",
    "            # return pd.concat(L, ignore_index=True)\n",
    "        # df, new = self.get(fcn, root/'drivetimes')#, self.get_zips)\n",
    "        # return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def get_flags_history(self, cutoff=202206):\n",
    "    #     def fcn():\n",
    "    #         import pyarrow.parquet as pq\n",
    "    #         print()\n",
    "    #         L = []\n",
    "    #         for path in sorted(flags_prc.iterdir(), reverse=True):\n",
    "    #             print(path)\n",
    "    #             for src in path.iterdir():\n",
    "    #                 _, term_code, cycle_date, cycle_day = src.stem.split('_')\n",
    "    #                 col = pq.ParquetFile(src).schema.names\n",
    "    #                 df = pd.DataFrame(columns=col).assign(term_code=[int(term_code)], cycle_date=[cycle_date]).fillna(True)\n",
    "    #                 L.append(df)\n",
    "    #         df = pd.concat(L).fillna(False).set_index(['cycle_date','term_code']).sort_index()\n",
    "    #         return df[sorted(df.columns)]\n",
    "    #     df, new = self.get(fcn, path=data/'flags_history')\n",
    "    #     A = df.query(f'term_code>={cutoff}').groupby('term_code').sum().sort_index(ascending=False).T.rename_axis('variable')\n",
    "    #     B = A == A.max()\n",
    "    #     B.insert(0, 'n', B.sum(axis=1))\n",
    "    #     return B.reset_index().sort_values(['n', 'variable'], ascending=[False, True])\n",
    "\n",
    "\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "# [warnings.filterwarnings(action='ignore', message=f\".*{w}.*\") for w in [\n",
    "#     \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "#     \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "#     \"errors='ignore' is deprecated\"\n",
    "#     \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "#     \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "#     \"DataFrame is highly fragmented\",\n",
    "# ]]\n",
    "\n",
    "\n",
    "\n",
    "# for fore in amp.values():\n",
    "#     for base in amp.values():\n",
    "#         if base.year < max(years):\n",
    "#             for styp, clf in base.get_models().items():\n",
    "#                 for k in ['predictions','headcounts']:\n",
    "#                     fore.__dict__.setdefault(k, dict()).setdefault(styp, dict())\n",
    "#                 y = clf.prediction(fore.get_prepared()[styp])\n",
    "#                 fore.predictions[styp][base.year] = y\n",
    "#                 s = (\n",
    "#                     (y[['pred']].sum() * base.get_enrollments().loc[base.crse_code,styp]['mlt']).round()\n",
    "#                     .rename(base.year).to_frame().T.rename_axis('base_year')\n",
    "#                     .assign(styp_code=styp, forecast_year=fore.year)\n",
    "#                     .reset_index().set_index(['styp_code','forecast_year','base_year'])\n",
    "#                 )\n",
    "#                 if fore.year < max(years):\n",
    "#                     s['true'] = fore.get_enrollments().loc[base.crse_code,styp]['stable']\n",
    "#                     s['error'] = s['pred'] - s['true']\n",
    "#                     s['error_pct'] = round(s['error'] / s['true'] * 100, 2)\n",
    "#                 fore.headcounts[styp][base.year] = s.prep()\n",
    "#     fore.forecasts = {styp: pd.concat(v.values()) for styp, v in fore.headcounts.items()}\n",
    "# amp[2023].forecasts['n']\n",
    "\n",
    "\n",
    "\n",
    "# def get_desc(code):\n",
    "#     for nm in code.split('_'):\n",
    "#         if len(nm) == 4:\n",
    "#             break\n",
    "#     return [f'{code} as {nm}_code, stv{nm}_desc as {nm}_desc', f'left join {catalog}saturnstv{nm} on {code} = stv{nm}_code']\n",
    "\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     pidm\n",
    "#     ,{self.cycle_day} as cycle_day\n",
    "#     ,timestamp('{self.cycle_date}') as cycle_date\n",
    "#     ,current_date\n",
    "#     ,first_date\n",
    "#     ,final_date\n",
    "#     ,{get_desc('term_code')[0]}\n",
    "#     ,appl_no\n",
    "#     ,{get_desc('apst_code')[0]}\n",
    "#     ,{get_desc('apdc_code')[0]}\n",
    "#     ,{get_desc('admt_code')[0]}\n",
    "#     ,{get_desc('wrsn_code')[0]}\n",
    "#     ,{get_desc('levl_code')[0]}\n",
    "#     ,{get_desc('styp_code')[0]}\n",
    "#     ,{get_desc('camp_code')[0]}\n",
    "#     ,{get_desc('coll_code_1')[0]}\n",
    "#     ,{get_desc('dept_code')[0]}\n",
    "#     ,{get_desc('majr_code_1')[0]}\n",
    "#     ,{get_desc('saradap_resd_code')[0]}\n",
    "#     ,gender\n",
    "#     ,birth_date\n",
    "#     ,{get_desc('spbpers_lgcy_code')[0]}\n",
    "#     ,gorvisa_vtyp_code is not null as international\n",
    "#     ,gorvisa_natn_code_issue as natn_code, stvnatn_nation as natn_desc\n",
    "#     ,{coalesce('race_asian')}\n",
    "#     ,{coalesce('race_black')}\n",
    "#     ,coalesce(spbpers_ethn_cde=2, False) as race_hispanic\n",
    "#     ,{coalesce('race_native')}\n",
    "#     ,{coalesce('race_pacific')}\n",
    "#     ,{coalesce('race_white')}\n",
    "#     ,hs_percentile\n",
    "#     ,sbgi_code\n",
    "#     ,enrolled_ind='Y' as enrolled_ind\n",
    "\n",
    "# from {subqry(qry)} as A\n",
    "\n",
    "# left join\n",
    "#     {catalog}spbpers_amp_v\n",
    "# on\n",
    "#     pidm = spbpers_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}generalgorvisa\n",
    "#         --{catalog}gorvisa_amp_v\n",
    "#     qualify\n",
    "#         row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorvisa_pidm\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         gorprac_pidm\n",
    "#         ,max(gorprac_race_cde='AS') as race_asian\n",
    "#         ,max(gorprac_race_cde='BL') as race_black\n",
    "#         ,max(gorprac_race_cde='IN') as race_native\n",
    "#         ,max(gorprac_race_cde='HA') as race_pacific\n",
    "#         ,max(gorprac_race_cde='WH') as race_white\n",
    "#     from\n",
    "#         {catalog}generalgorprac\n",
    "#     group by\n",
    "#         gorprac_pidm\n",
    "#     )\n",
    "# on\n",
    "#     pidm = gorprac_pidm\n",
    "\n",
    "# {get_desc('term_code')[1]}\n",
    "# {get_desc('levl_code')[1]}\n",
    "# {get_desc('styp_code')[1]}\n",
    "# {get_desc('admt_code')[1]}\n",
    "# {get_desc('wrsn_code')[1]}\n",
    "# {get_desc('apst_code')[1]}\n",
    "# {get_desc('apdc_code')[1]}\n",
    "# {get_desc('camp_code')[1]}\n",
    "# {get_desc('coll_code_1')[1]}\n",
    "# {get_desc('dept_code')[1]}\n",
    "# {get_desc('majr_code_1')[1]}\n",
    "# {get_desc('saradap_resd_code')[1]}\n",
    "# {get_desc('gorvisa_natn_code_issue')[1]}\n",
    "# {get_desc('spbpers_lgcy_code')[1]}\n",
    "\n",
    "# qualify\n",
    "#     min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "#     and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             dct = {\n",
    "#                 'sfrstcr_pidm':'pidm',\n",
    "#                 'ssbsect_term_code':'term_code',\n",
    "#                 'sgbstdn_levl_code':'levl_code',\n",
    "#                 'sgbstdn_styp_code':'styp_code',\n",
    "#             }\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     {indent(join(alias(dct)))}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term part\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "#     and ssbsect_credit_hrs > 0\n",
    "# group by\n",
    "#     {indent(join(dct.keys()))}\n",
    "#     ,ssbsect_subj_code\n",
    "#     ,ssbsect_crse_numb\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with A as {subqry(qry)}\n",
    "# select * from A\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_allcrse' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "\n",
    "# union all\n",
    "\n",
    "# select\n",
    "#     {indent(join(dct.values()))}\n",
    "#     ,'_anycrse' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from A\n",
    "# group by\n",
    "#     {indent(join(dct.values()))}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def get_registrations(self, show=False):\n",
    "#         def fcn():\n",
    "#             grp = join([\n",
    "#                 'pidm',\n",
    "#                 'term_code','term_desc',\n",
    "#                 'levl_code','levl_desc',\n",
    "#                 'styp_code','styp_desc',\n",
    "#                 ], ', ')\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# select\n",
    "#     sfrstcr_pidm as pidm\n",
    "#     ,{get_desc('ssbsect_term_code')}\n",
    "#     ,{get_desc('sgbstdn_levl_code')}\n",
    "#     ,{get_desc('sgbstdn_styp_code')}\n",
    "#     ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "#     ,max(ssbsect_credit_hrs) as credit_hr\n",
    "# from\n",
    "#     {catalog}saturnsfrstcr as A\n",
    "# inner join\n",
    "#     {catalog}saturnssbsect as B\n",
    "# on\n",
    "#     sfrstcr_term_code = ssbsect_term_code\n",
    "#     and sfrstcr_crn = ssbsect_crn\n",
    "# inner join (\n",
    "#     select\n",
    "#         *\n",
    "#     from\n",
    "#         {catalog}sgbstdn_amp_v\n",
    "#     where\n",
    "#         sgbstdn_term_code_eff <= {self.term_code}\n",
    "#     qualify\n",
    "#         row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "#     ) as C\n",
    "# on\n",
    "#     sfrstcr_pidm = sgbstdn_pidm\n",
    "# where\n",
    "#     sfrstcr_term_code = {self.term_code}\n",
    "#     and sfrstcr_error_flag is null\n",
    "#     and sfrstcr_ptrm_code not in ('28','R3') -- drop weird term parts\n",
    "#     and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "#     and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "#     and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "#     and ssbsect_credit_hrs > 0\n",
    "# group by\n",
    "#     {grp}, crse_code\n",
    "# \"\"\"\n",
    "\n",
    "#             qry = f\"\"\"\n",
    "# with CTE as {subqry(qry)}\n",
    "\n",
    "# --individual courses\n",
    "# select\n",
    "#     *\n",
    "# from\n",
    "#     CTE\n",
    "\n",
    "# union all\n",
    "\n",
    "# --total credit hours\n",
    "# select\n",
    "#     {grp}\n",
    "#     ,'_total_sch' as crse_code\n",
    "#     ,sum(credit_hr) as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     {grp}\n",
    "\n",
    "# union all\n",
    "\n",
    "# --headcount \n",
    "# select\n",
    "#     {grp}\n",
    "#     ,'_headcount' as crse_code\n",
    "#     ,1 as credit_hr\n",
    "# from\n",
    "#     CTE\n",
    "# group by\n",
    "#     {grp}\n",
    "# \"\"\"\n",
    "#             df = run(qry, show).set_index(['crse_code','pidm'])\n",
    "#             return df\n",
    "#         df, new = self.get(fcn, 'registrations')\n",
    "#         return df\n",
    "\n",
    "\n",
    "\n",
    "    def get_students(self, show=False):\n",
    "        def fcn():\n",
    "            df = (self.admissions\n",
    "                  .merge(self.get_flags(), on=['pidm','term_code'], how='left', suffixes=['', '_flags'])\n",
    "                  .merge(self.get_drivetimes(), on=['zip','camp_code'], how='left', suffixes=['', '_zips'])                \n",
    "            )\n",
    "            mask = df.eval(\"drivetime.isnull() & zip.notnull() & camp_code!='o'\")\n",
    "            if mask.any():\n",
    "                df[mask].set_index(['state','city','zip','camp_code'])[[]].sort_index().reset_index().disp(50)\n",
    "\n",
    "            for c in ['gap_score','t_gap_score','ftic_gap_score']:\n",
    "                if c not in df:\n",
    "                    df[c] = pd.NA\n",
    "            \n",
    "            df['gap_score'] = np.where(\n",
    "                df['styp_code']=='n',\n",
    "                df['ftic_gap_score'].combine_first(df['t_gap_score']).combine_first(df['gap_score']),\n",
    "                df['t_gap_score'].combine_first(df['ftic_gap_score']).combine_first(df['gap_score']))\n",
    "            \n",
    "            \n",
    "            # df['oriented'] = np.where(df['orien_sess'].notnull() | df['registered'].notnull(), 'y', np.where(df['orientation_hold_exists'].notnull(), 'n', 'w'))\n",
    "            df['oriented'] = df['orientation_hold_exists'].isnull() | df['orien_sess'].notnull() | df['registered'].notnull()\n",
    "\n",
    "            # df['verified'] = np.where(df['ver_complete'].notnull(), 'y', np.where(df['selected_for_ver'].notnull(), 'n', 'w'))\n",
    "            df['verified'] = df['selected_for_ver'].isnull() | df['ver_complete'].notnull()\n",
    "            \n",
    "            df['sat10_total_score'] = (36-9) / (1600-590) * (df['sat10_total_score']-590) + 9\n",
    "            df['act_equiv'] = df[['act_new_comp_score','sat10_total_score']].max(axis=1)\n",
    "\n",
    "            df['eager'] = (self.stable_date - df['first_date']).dt.days\n",
    "            df['age'] = (self.stable_date - df['birth_date']).dt.days\n",
    "\n",
    "            for k in ['reading', 'writing', 'math']:\n",
    "                df[f'tsi_{k}'] = ~df[k].isin(['not college ready', 'retest required', pd.NA, None, np.nan])\n",
    "            \n",
    "            repl = {'ae':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'r1':1, 'r2':2, 'r3':3, 'r4':4}\n",
    "            df['hs_qrtl'] = pd.cut(df['hs_pctl'], bins=[-1,25,50,75,90,101], labels=[4,3,2,1,0], right=False).combine_first(df['apdc_code'].map(repl))\n",
    "\n",
    "            df['lgcy'] = ~df['lgcy_code'].isin(['o',pd.NA,None,np.nan])\n",
    "            df['resd'] = df['resd_code'] == 'r'\n",
    "\n",
    "            for k in ['waiver_desc','fafsa_app','ssb_last_accessed','finaid_accepted','schlship_app']:\n",
    "                df[k.split('_')[0]] = df[k].notnull()\n",
    "\n",
    "\n",
    "\n",
    "            # df['majr_code'] = df['majr_code'].replace({'0000':pd.NA, 'und':pd.NA, 'eled':'eted', 'agri':'unda'})\n",
    "\n",
    "            # df['coll_code'] = df['coll_code'].replace({'ae':'an', 'eh':'ed', 'hs':'hl', 'st':'sm', '00':pd.NA})\n",
    "\n",
    "            # df['coll_desc'] = df['coll_code'].map({\n",
    "            #     'an': 'ag & natural_resources',\n",
    "            #     'ba': 'business',\n",
    "            #     'ed': 'education',\n",
    "            #     'en': 'engineering',\n",
    "            #     'hl': 'health sciences',\n",
    "            #     'la': 'liberal & fine arts',\n",
    "            #     'sm': 'science & mathematics',\n",
    "            #     pd.NA: 'no college designated',\n",
    "            # })\n",
    "\n",
    "\n",
    "\n",
    "            # checks = [\n",
    "            #     'cycle_day >= 0',\n",
    "            #     'eager >= cycle_day',\n",
    "            #     'age >= 5000',\n",
    "            #     'distance >= 0',\n",
    "            #     'hs_pctl >=0',\n",
    "            #     'hs_pctl <= 100',\n",
    "            #     'hs_qrtl >= 0',\n",
    "            #     'hs_qrtl <= 4',\n",
    "            #     'act_equiv >= 1',\n",
    "            #     'act_equiv <= 36',\n",
    "            #     'gap_score >= 0',\n",
    "            #     'gap_score <= 100',\n",
    "            # ]\n",
    "            # for check in checks:\n",
    "            #     mask = df.eval(check)\n",
    "            #     assert mask.all(), [check,df[~mask].disp(5)]\n",
    "            mask = df['cycle_date_flags'].isnull()  # rows from admissions not on flags - should not be any\n",
    "            if mask.any():\n",
    "                display(df[mask]['styp_code'].value_counts().sort_index().to_frame().T)\n",
    "            return df.set_index(['pidm'])\n",
    "        df, new = self.get(fcn, 'students', [self.get_admissions,self.get_flags,self.get_drivetimes])\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "for A in list(flags_prc.iterdir()):\n",
    "#     if A.is_dir():\n",
    "#         for src in list(A.iterdir()):\n",
    "#             if src.is_file():\n",
    "#                 date = [x for x in src.name.split('_') if '-' in x][0]\n",
    "#                 dst = A / date / src.name\n",
    "#                 reset(dst)\n",
    "#                 print(src, dst)\n",
    "#                 # assert False\n",
    "#                 src.rename(dst)\n",
    "\n",
    "#             # for src in list(B.iterdir()):\n",
    "#             #     if src.is_file():\n",
    "#             #         date = [x for x in src.name.split('_') if '-' in x][0]\n",
    "#             #         dst = B / date / src.name\n",
    "#             #         reset(dst)\n",
    "#             #         print(src, dst)\n",
    "#             #         src.rename(dst)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9000138541370679,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AMP_2025",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
