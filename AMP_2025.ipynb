{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dd982f-3630-4ea9-8756-47674ae4dec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import display, HTML, clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming openpyxl\n",
    "    %reload_ext autotime\n",
    "clear_output()\n",
    "\n",
    "import pathlib, shutil, warnings, dataclasses, numpy as np, pandas as pd\n",
    "from codetiming import Timer\n",
    "seed = 42\n",
    "catalog = 'dev.bronze.'\n",
    "root = pathlib.Path(f'/Workspace/Users/{username}@tarleton.edu/admitted_matriculation_predictor_2025/')\n",
    "flags_raw = pathlib.Path('/Volumes/aiml/scook/scook_files/admitted_flags_raw')\n",
    "flags_prc = pathlib.Path('/Volumes/aiml/flags/flags_volume/')\n",
    "\n",
    "##########################################\n",
    "############ helper functions ############\n",
    "##########################################\n",
    "tab = '    '\n",
    "divider = '\\n##############################################################################################################\\n'\n",
    "\n",
    "def listify(*args):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        return list(*args)\n",
    "    except Exception as e:\n",
    "        return list(args)\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    return \"(\" + indent('\\n'+qry.strip()+'\\n)', lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=seed):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep()\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "############ annoying warnings to suppress ############\n",
    "# [warnings.filterwarnings(action='ignore', message=f\".*{w}.*\") for w in [\n",
    "#     \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\",\n",
    "#     \"Engine has switched to 'python' because numexpr does not support extension array dtypes\",\n",
    "#     \"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
    "#     \"errors='ignore' is deprecated\"\n",
    "#     \"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n",
    "#     \"The behavior of array concatenation with empty entries is deprecated\",\n",
    "#     \"DataFrame is highly fragmented\",\n",
    "# ]]\n",
    "\n",
    "############ pandas functions ############\n",
    "pd.options.display.max_columns = None\n",
    "def disp(df, rows=4, head=True):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    with pd.option_context('display.min_rows', rows, 'display.max_rows', rows):\n",
    "        X = df.head(rows) if head else df.tails(rows)\n",
    "        display(HTML(X.to_html()))\n",
    "        print(df.shape)\n",
    "\n",
    "def to_numeric(df, downcast='integer', errors='ignore', **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: s.astype('string').str.lower().str.strip() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "    idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "    return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    return wrapper\n",
    "\n",
    "for f in [disp, to_numeric, prep]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setattr(pd.DataFrame, f.__name__, f)\n",
    "    setattr(pd.Series, f.__name__, wrap(f))\n",
    "\n",
    "\n",
    "#########################################\n",
    "################## AMP ##################\n",
    "#########################################\n",
    "@dataclasses.dataclass\n",
    "class Term():\n",
    "    term_code: int = 202408\n",
    "    cycle_day: int = None\n",
    "    cycle_date: str = None\n",
    "\n",
    "    #Allows self['attr'] and self.attr syntax\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __delitem__(self, key):\n",
    "        if key in self:\n",
    "            delattr(self, key)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.get_terms()\n",
    "\n",
    "    def get(self, fcn, nm, overwrite=False, path=None, divide=True):\n",
    "        dst = root/f'{nm}/{self.term_code}/{nm}_{self.stem}.parquet' if path is None else pathlib.Path(path).with_suffix('.parquet')\n",
    "        new = False\n",
    "        if overwrite:\n",
    "            del self[nm]\n",
    "            dst.unlink(missing_ok=True)\n",
    "        if not nm in self:\n",
    "            if dst.exists():\n",
    "                self[nm] = pd.read_parquet(dst)\n",
    "            else:\n",
    "                print(f'creating {dst}: ', end='')\n",
    "                new = True\n",
    "                with Timer():\n",
    "                    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    self[nm] = fcn().prep()\n",
    "                    self[nm].to_parquet(dst)\n",
    "                if divide:\n",
    "                    print(divider)\n",
    "        return self[nm], new\n",
    "\n",
    "########################################################\n",
    "################# get term information #################\n",
    "########################################################\n",
    "    def get_terms(self, overwrite=False, show=False):\n",
    "        self.term_code = int(self.term_code)\n",
    "        def fcn():\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    stvterm_code as term_code\n",
    "    ,replace(stvterm_desc, ' ', '') as term_desc\n",
    "    ,stvterm_start_date as start_date\n",
    "    ,stvterm_end_date as end_date\n",
    "    ,stvterm_fa_proc_yr as fa_proc_yr\n",
    "    ,stvterm_housing_start_date as housing_start_date\n",
    "    ,stvterm_housing_end_date as housing_end_date\n",
    "    ,sobptrm_census_date as census_date\n",
    "from\n",
    "    {catalog}saturnstvterm as A\n",
    "inner join\n",
    "    {catalog}saturnsobptrm as B\n",
    "on\n",
    "    stvterm_code = sobptrm_term_code\n",
    "where\n",
    "    sobptrm_ptrm_code='1'\n",
    "\"\"\"\n",
    "            df = run(qry, show).set_index('term_code')\n",
    "            df['stable_date'] = df['census_date'].apply(lambda x: x+pd.Timedelta(days=7+4-x.weekday())) # Friday of week following census\n",
    "            return df\n",
    "        self.get(fcn, 'terms', overwrite, path=root/'terms')\n",
    "        self.cycle_day, self.cycle_date, self.stem = self.get_cycle(self.term_code, self.cycle_day, self.cycle_date)\n",
    "\n",
    "\n",
    "    def get_cycle(self, term_code, cycle_day=None, cycle_date=None):\n",
    "        stable_date = self.terms.loc[int(term_code),'stable_date']\n",
    "        if cycle_day is None:\n",
    "            if cycle_date is None:\n",
    "                cycle_date = pd.Timestamp.now()\n",
    "            cycle_date = pd.to_datetime(cycle_date).normalize()\n",
    "            cycle_day = (stable_date - cycle_date).days\n",
    "        cycle_date = str((stable_date - pd.Timedelta(days=cycle_day)).date())\n",
    "        stem = f'{term_code}_{cycle_date}_{\"-\" if cycle_day < 0 else \"+\"}{rjust(abs(cycle_day),3,0)}'\n",
    "        return cycle_day, cycle_date, stem\n",
    "\n",
    "#######################################################\n",
    "############ process flags reports archive ############\n",
    "#######################################################\n",
    "    def get_spriden(self, overwrite=False, show=False):\n",
    "        # Get id-pidm crosswalk so we can replace id by pidm in flags below\n",
    "        if 'spriden' not in self:\n",
    "            qry = f\"\"\"\n",
    "            select distinct\n",
    "                spriden_id as id,\n",
    "                spriden_pidm as pidm\n",
    "            from\n",
    "                {catalog}saturnspriden as A\n",
    "            where\n",
    "                spriden_change_ind is null\n",
    "                and spriden_activity_date between '2000-09-01' and '2025-09-01'\n",
    "                and spriden_id REGEXP '^[0-9]+'\n",
    "            \"\"\"\n",
    "            self.spriden = run(qry, show)\n",
    "        return self.spriden\n",
    "\n",
    "\n",
    "    def process_flags(self, overwrite=False, show=False):\n",
    "        counter = 0\n",
    "        for src in sorted(flags_raw.iterdir(), reverse=True):\n",
    "            counter += 1\n",
    "            if counter > 5:\n",
    "                break\n",
    "            a,b = src.name.lower().split('.')\n",
    "            if b != 'xlsx' or 'melt' in a or 'admitted' not in a:\n",
    "                print(a, 'SKIP')\n",
    "                continue\n",
    "            # Handles 2 naming conventions that were used at different times\n",
    "            try:\n",
    "                cycle_date = pd.to_datetime(a[:10].replace('_','-'))\n",
    "                multi = True\n",
    "            except:\n",
    "                try:\n",
    "                    cycle_date = pd.to_datetime(a[-6:])\n",
    "                    multi = False\n",
    "                except:\n",
    "                    print(a, 'FAIL')\n",
    "                    continue\n",
    "            # print(a, dt.date())\n",
    "            book = pd.ExcelFile(src, engine='openpyxl')\n",
    "            # Again, handles the 2 different versions with different sheet names\n",
    "            if multi:\n",
    "                sheets = {sheet:sheet for sheet in book.sheet_names if sheet.isnumeric() and int(sheet) % 100 in [1,6,8]}\n",
    "            else:\n",
    "                sheets = {a[:6]: book.sheet_names[0]}\n",
    "            for term_code, sheet in sheets.items():\n",
    "                cycle_day, cycle_date, stem = self.get_cycle(term_code, cycle_date=cycle_date)\n",
    "                def fcn():\n",
    "                    df = (\n",
    "                        self.get_spriden()\n",
    "                        .assign(cycle_day=cycle_day, cycle_date=cycle_date)\n",
    "                        .merge(book.parse(sheet).prep(), on='id', how='right')\n",
    "                        .drop(columns=['id','last_name','first_name','mi','pref_fname','street1','street2','primary_phone','call_em_all','email'], errors='ignore')\n",
    "                    )\n",
    "                    return df\n",
    "                if self.get(fcn, 'flags', overwrite, path=flags_prc/f'{term_code}/flags_{stem}', divide=False)[1]:\n",
    "                    counter = 0\n",
    "                del self['flags']\n",
    "        print(divider)\n",
    "\n",
    "\n",
    "    def get_flags_history(self, overwrite=False, show=False, cutoff=202206):\n",
    "        def fcn():\n",
    "            import pyarrow.parquet as pq\n",
    "            print()\n",
    "            L = []\n",
    "            for path in sorted(flags_prc.iterdir(), reverse=True):\n",
    "                print(path)\n",
    "                # if int(path.stem) < 202506:\n",
    "                #     break\n",
    "                for src in path.iterdir():\n",
    "                    _, term_code, cycle_date, cycle_day = src.stem.split('_')\n",
    "                    col = pq.ParquetFile(src).schema.names\n",
    "                    df = pd.DataFrame(columns=col).assign(term_code=[int(term_code)], cycle_date=[cycle_date]).fillna(True)\n",
    "                    L.append(df)\n",
    "            df = pd.concat(L).fillna(False).set_index(['cycle_date','term_code']).sort_index()\n",
    "            return df[sorted(df.columns)]\n",
    "        df, new = self.get(fcn, 'flags_history', overwrite, path=root/'flags_history')\n",
    "        A = df.query(f'term_code>={cutoff}').groupby('term_code').sum().sort_index(ascending=False).T.rename_axis('variable')\n",
    "        B = A == A.max()\n",
    "        B.insert(0, 'n', B.sum(axis=1))\n",
    "        return B.reset_index().sort_values(['n', 'variable'], ascending=[False, True])\n",
    "\n",
    "\n",
    "    def get_flags(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            L = []\n",
    "            for term_code in [self.term_code-2,self.term_code]:  # summer & fall\n",
    "                for src in sorted((flags_prc/f'{term_code}').iterdir()):\n",
    "                    if src.stem.split('_')[2] < self.cycle_date:  # find first flags before cycle_date\n",
    "                        L.append(pd.read_parquet(src))\n",
    "                        break\n",
    "            df = pd.concat(L, ignore_index=True)\n",
    "            for k in ['dob',*df.filter(like='date').columns]:  # convert date columns\n",
    "                df[k] = pd.to_datetime(df[k], errors='coerce')\n",
    "            return df\n",
    "        self.get(fcn, 'flags', overwrite)\n",
    "\n",
    "########################################################\n",
    "############### get course registrations ###############\n",
    "########################################################\n",
    "    def get_registrations(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            dct = {\n",
    "                'sfrstcr_pidm':'pidm',\n",
    "                'ssbsect_term_code':'term_code',\n",
    "                'sgbstdn_levl_code':'levl_code',\n",
    "                'sgbstdn_styp_code':'styp_code',\n",
    "                'ssbsect_crn':'crn',\n",
    "            }\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    {indent(join(alias(dct)))}\n",
    "    ,lower(ssbsect_subj_code) || ssbsect_crse_numb as crse_code\n",
    "    ,max(ssbsect_credit_hrs) as credit_hr\n",
    "from\n",
    "    {catalog}saturnsfrstcr as A\n",
    "inner join\n",
    "    {catalog}saturnssbsect as B\n",
    "on\n",
    "    sfrstcr_term_code = ssbsect_term_code\n",
    "    and sfrstcr_crn = ssbsect_crn\n",
    "inner join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}sgbstdn_amp_v\n",
    "    where\n",
    "        sgbstdn_term_code_eff <= {self.term_code}\n",
    "    qualify\n",
    "        row_number() over (partition by sgbstdn_pidm order by sgbstdn_term_code_eff desc) = 1\n",
    "    ) as C\n",
    "on\n",
    "    sfrstcr_pidm = sgbstdn_pidm\n",
    "where\n",
    "    sfrstcr_term_code = {self.term_code}\n",
    "    and sfrstcr_ptrm_code not in ('28','R3') -- drop weird parts of term\n",
    "    and sfrstcr_add_date <= '{self.cycle_date}' -- added before cycle_day\n",
    "    and (sfrstcr_rsts_date > '{self.cycle_date}' or sfrstcr_rsts_code in ('DC','DL','RD','RE','RW','WD','WF')) -- dropped after cycle_day or still enrolled\n",
    "    and ssbsect_subj_code <> 'INST' -- exceptional sections\n",
    "group by\n",
    "    {indent(join(dct.keys()))}\n",
    "    ,ssbsect_subj_code\n",
    "    ,ssbsect_crse_numb\n",
    "\"\"\"\n",
    "\n",
    "            qry = f\"\"\"\n",
    "with A as {subqry(qry)}\n",
    "select * from A\n",
    "\n",
    "union all\n",
    "\n",
    "select\n",
    "    {indent(join(dct.values()))}\n",
    "    ,'_allcrse' as crse_code\n",
    "    ,sum(credit_hr) as credit_hr\n",
    "from A\n",
    "group by\n",
    "    {indent(join(dct.values()))}\n",
    "\n",
    "union all\n",
    "\n",
    "select\n",
    "    {indent(join(dct.values()))}\n",
    "    ,'_anycrse' as crse_code\n",
    "    ,case when sum(credit_hr) > 0 then 1 else 0 end as credit_hr\n",
    "from A\n",
    "group by\n",
    "    {indent(join(dct.values()))}\n",
    "\"\"\"\n",
    "            df = run(qry, show)\n",
    "            return df\n",
    "        self.get(fcn, 'registrations', overwrite)\n",
    "\n",
    "##############################################\n",
    "############### get admissions ###############\n",
    "##############################################\n",
    "    def get_admissions(self, overwrite=False, show=False):\n",
    "        def fcn():\n",
    "            L = [f\"\"\"\n",
    "select\n",
    "    *\n",
    "from (\n",
    "    select distinct\n",
    "        A.*\n",
    "        ,min(current_date) over (partition by pidm, appl_no) as first_date\n",
    "        ,max(current_date) over (partition by pidm, appl_no) as final_date\n",
    "    from\n",
    "        dev.opeir.opeiradmissions_{self.terms.loc[term_code,'term_desc']} as A\n",
    "        --dev.opeir.admissions_{self.terms.loc[term_code,'term_desc']}_v as A\n",
    "    inner join\n",
    "        {catalog}saturnstvapdc as B\n",
    "    on\n",
    "        apdc_code = stvapdc_code\n",
    "    where\n",
    "        stvapdc_inst_acc_ind is not null  --only accepted\n",
    "    qualify\n",
    "        '{self.cycle_date}' between first_date and final_date --keep only pidm, appl_no where cycle_date falls between its first and last records\n",
    "    )\n",
    "where\n",
    "    current_date <= '{self.cycle_date}'  -- only records before cycle_date\n",
    "qualify\n",
    "    row_number() over (partition by pidm, appl_no order by current_date desc) = 1  -- most current record remaiming for this pidm, appl_no\n",
    "\"\"\"\n",
    "                for term_code in [self.term_code-2, self.term_code]]\n",
    "            qry = join(L, '\\nunion all\\n')\n",
    "\n",
    "\n",
    "            stat_codes = ['AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','IA','ID','IL','IN','KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY'] # not AK & HI b/c can't get driving distance\n",
    "\n",
    "            def get_desc(code):\n",
    "                for nm in code.split('_'):\n",
    "                    if len(nm) == 4:\n",
    "                        break\n",
    "                return [f'{code} as {nm}_code, stv{nm}_desc as {nm}_desc', f'left join {catalog}saturnstv{nm} on {code} = stv{nm}_code']\n",
    "\n",
    "            qry = f\"\"\"\n",
    "select\n",
    "    pidm\n",
    "    ,{self.cycle_day} as cycle_day\n",
    "    ,{self.cycle_date} as cycle_date\n",
    "    ,current_date\n",
    "    ,first_date\n",
    "    ,final_date\n",
    "    ,{get_desc('term_code')[0]}\n",
    "    ,appl_no\n",
    "    ,{get_desc('apst_code')[0]}\n",
    "    ,{get_desc('apdc_code')[0]}\n",
    "    ,{get_desc('admt_code')[0]}\n",
    "    ,{get_desc('wrsn_code')[0]}\n",
    "    ,{get_desc('levl_code')[0]}\n",
    "    ,{get_desc('styp_code')[0]}    \n",
    "    ,{get_desc('camp_code')[0]}\n",
    "    ,{get_desc('coll_code_1')[0]}\n",
    "    ,{get_desc('dept_code')[0]}\n",
    "    ,{get_desc('majr_code_1')[0]}\n",
    "    ,{get_desc('saradap_resd_code')[0]}\n",
    "    ,gender\n",
    "    ,birth_date\n",
    "    ,{get_desc('spbpers_lgcy_code')[0]}\n",
    "    ,gorvisa_vtyp_code is not null as international\n",
    "    ,gorvisa_natn_code_issue as natn_code, stvnatn_nation as natn_desc\n",
    "    ,spbpers_ethn_cde=2 as race_hispanic\n",
    "    ,race_asian\n",
    "    ,race_black\n",
    "    ,race_native\n",
    "    ,race_pacific\n",
    "    ,race_white\n",
    "    ,hs_percentile\n",
    "    ,sbgi_code\n",
    "    ,enrolled_ind\n",
    "\n",
    "from {subqry(qry)} as A\n",
    "\n",
    "left join\n",
    "    {catalog}spbpers_amp_v\n",
    "on\n",
    "    pidm = spbpers_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        {catalog}generalgorvisa\n",
    "    qualify\n",
    "        row_number() over (partition by gorvisa_pidm order by gorvisa_seq_no desc) = 1\n",
    "    )\n",
    "on\n",
    "    pidm = gorvisa_pidm\n",
    "\n",
    "left join (\n",
    "    select\n",
    "        gorprac_pidm\n",
    "        ,max(gorprac_race_cde='AS') as race_asian\n",
    "        ,max(gorprac_race_cde='BL') as race_black\n",
    "        ,max(gorprac_race_cde='IN') as race_native\n",
    "        ,max(gorprac_race_cde='HA') as race_pacific\n",
    "        ,max(gorprac_race_cde='WH') as race_white\n",
    "    from\n",
    "        {catalog}generalgorprac\n",
    "    group by\n",
    "        gorprac_pidm\n",
    "    )\n",
    "on\n",
    "    pidm = gorprac_pidm\n",
    "\n",
    "{get_desc('term_code')[1]}\n",
    "{get_desc('levl_code')[1]}\n",
    "{get_desc('styp_code')[1]}\n",
    "{get_desc('admt_code')[1]}\n",
    "{get_desc('wrsn_code')[1]}\n",
    "{get_desc('apst_code')[1]}\n",
    "{get_desc('apdc_code')[1]}\n",
    "{get_desc('camp_code')[1]}\n",
    "{get_desc('coll_code_1')[1]}\n",
    "{get_desc('dept_code')[1]}\n",
    "{get_desc('majr_code_1')[1]}\n",
    "{get_desc('saradap_resd_code')[1]}\n",
    "{get_desc('gorvisa_natn_code_issue')[1]}\n",
    "{get_desc('spbpers_lgcy_code')[1]}\n",
    "\n",
    "qualify\n",
    "    min(levl_code='UG') over (partition by pidm) = True  -- remove pidm's with graduate admission even it if also has an undergradute admission, min acts like logical and\n",
    "    and row_number() over (partition by pidm order by appl_no desc) = 1  -- de-duplicate the few remaining pidms with multiple record by keeping highest appl_no\n",
    "\"\"\"\n",
    "# don't delete - could be useful & was hard to create\n",
    "#     ,{get_desc('spraddr_cnty_code')[0]}\n",
    "#     ,{get_desc('spraddr_stat_code')[0]}\n",
    "#     ,zip_code\n",
    "\n",
    "# left join (\n",
    "#     select\n",
    "#         *\n",
    "#         ,try_to_number(left(spraddr_zip, 5), '00000') as zip_code\n",
    "#         ,case\n",
    "#             when spraddr_atyp_code = 'PA' then 6\n",
    "#             when spraddr_atyp_code = 'PR' then 5\n",
    "#             when spraddr_atyp_code = 'MA' then 4\n",
    "#             when spraddr_atyp_code = 'BU' then 3\n",
    "#             when spraddr_atyp_code = 'BI' then 2\n",
    "#             when spraddr_atyp_code = 'P1' then 1\n",
    "#             when spraddr_atyp_code = 'P2' then 0\n",
    "#             end as spraddr_atyp_rank\n",
    "#     from\n",
    "#         {catalog}spraddr_amp_v\n",
    "#     where\n",
    "#         spraddr_stat_code in ('{join(stat_codes, \"','\")}')\n",
    "#         and spraddr_zip is not null\n",
    "#     qualify\n",
    "#         row_number() over (partition by spraddr_pidm order by spraddr_atyp_rank desc, spraddr_seqno desc) = 1\n",
    "# )\n",
    "# on\n",
    "#     pidm = spraddr_pidm\n",
    "\n",
    "# {get_desc('spraddr_cnty_code')[1]}\n",
    "# {get_desc('spraddr_stat_code')[1]}\n",
    "\n",
    "            df = run(qry, show)\n",
    "            return df \n",
    "        self.get(fcn, 'admissions', overwrite)\n",
    "\n",
    "####################################\n",
    "############### main ###############\n",
    "####################################\n",
    "self = Term(\n",
    "    term_code=202308,\n",
    "    cycle_day=50,\n",
    ")\n",
    "\n",
    "# GA's should not have permissions to run this because it can see pii\n",
    "# self.process_flags(\n",
    "#     # overwrite=True,\n",
    "#     # show=True,\n",
    "# )\n",
    "\n",
    "H = self.get_flags_history(\n",
    "    # overwrite=True,\n",
    "    # show=True,\n",
    "    cutoff=202206,\n",
    ")\n",
    "H.disp(100)\n",
    "\n",
    "# self.get_terms(\n",
    "#     overwrite=True,\n",
    "#     show=True,\n",
    "# )\n",
    "\n",
    "# self.get_flags(\n",
    "#     overwrite=True,\n",
    "#     show=True,\n",
    "# )\n",
    "\n",
    "# self.get_registrations(\n",
    "#     overwrite=True,\n",
    "#     show=True,\n",
    "# )\n",
    "\n",
    "self.get_admissions(\n",
    "    overwrite=True,\n",
    "    show=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8185768213204940,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AMP_2025",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
