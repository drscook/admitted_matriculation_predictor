{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716cf558-49e2-4239-a55a-cd26d302894c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = 'scook'\n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    %reload_ext autotime\n",
    "except:\n",
    "    %pip install -U ipython-autotime ipywidgets codetiming Jinja2 numpy pandas pyarrow\n",
    "    dbutils.library.restartPython()\n",
    "    clear_output()\n",
    "    dbutils.notebook.exit('Rerun to use newly installed/updated packages')\n",
    "\n",
    "import os, sys, copy, pathlib, shutil, pickle, warnings, requests, dataclasses, time, codetiming, numpy as np, pandas as pd\n",
    "clear_output()\n",
    "pd.options.display.max_columns = None\n",
    "now = pd.Timestamp.now()\n",
    "root = pathlib.Path('/Volumes/aiml/scook/scook_files/transcript_ocr/')\n",
    "\n",
    "############ helper functions ############\n",
    "def dt(*args):\n",
    "    return pd.to_datetime(args).dropna().min().normalize()\n",
    "\n",
    "def setmeth(cls, fcn):\n",
    "    \"\"\"monkey-patch new method into a mutable class (fails for immutable class)\"\"\"\n",
    "    setattr(cls, fcn.__name__, fcn)\n",
    "\n",
    "def listify(*args, sort=False, reverse=False):\n",
    "    \"\"\"ensure it is a list\"\"\"\n",
    "    if len(args)==1:\n",
    "        if args[0] is None or args[0] is np.nan or args[0] is pd.NA:\n",
    "            return list()\n",
    "        elif isinstance(args[0], str):\n",
    "            return [args[0]]\n",
    "    try:\n",
    "        L = list(*args)\n",
    "    except Exception as e:\n",
    "        L = list(args)\n",
    "    if sort:\n",
    "        try:\n",
    "            L = sorted(L, reverse=reverse) \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return L\n",
    "\n",
    "def setify(*args):\n",
    "    \"\"\"ensure it is a set\"\"\"\n",
    "    return set(listify(*args))\n",
    "\n",
    "def unpack(*args, **kwargs):\n",
    "    L = [y for x in args for y in (unpack(*x) if isinstance(x, (list,tuple,set)) else listify(x))]\n",
    "    return listify(L, **kwargs)\n",
    "\n",
    "def unique(*args, **kwargs):\n",
    "    L = dict.fromkeys(unpack(*args))\n",
    "    return listify(L, **kwargs)\n",
    "\n",
    "def difference(A, B, **kwargs):\n",
    "    return unique([x for x in listify(A) if x not in listify(B)], **kwargs)\n",
    "\n",
    "def rjust(x, width, fillchar=' '):\n",
    "    return str(x).rjust(width,str(fillchar))\n",
    "\n",
    "def ljust(x, width, fillchar=' '):\n",
    "    return str(x).ljust(width,str(fillchar))\n",
    "\n",
    "def join(lst, sep='\\n,', pre='', post=''):\n",
    "    \"\"\"flexible way to join list of strings into a single string\"\"\"\n",
    "    return f\"{pre}{str(sep).join(map(str,listify(lst)))}{post}\"\n",
    "\n",
    "def alias(dct):\n",
    "    \"\"\"convert dict of original column name:new column name into list\"\"\"\n",
    "    return [f'{k} as {v}' for k,v in dct.items()]\n",
    "\n",
    "def indent(x, lev=1):\n",
    "    return x.replace('\\n','\\n'+tab*lev) if lev>0 else x\n",
    "\n",
    "def subqry(qry, lev=1):\n",
    "    \"\"\"make qry into subquery\"\"\"\n",
    "    qry = '\\n' + qry.strip()\n",
    "    qry = '(' + qry + '\\n)' if 'select' in qry else qry\n",
    "    return indent(qry, lev)\n",
    "\n",
    "def run(qry, show=False, sample='10 rows', seed=42):\n",
    "    \"\"\"run qry and return dataframe\"\"\"\n",
    "    L = qry.split(' ')\n",
    "    if len(L) == 1:\n",
    "        qry = f'select * from {catalog}{L[0]}'\n",
    "        if sample is not None:\n",
    "            qry += f' tablesample ({sample}) repeatable ({seed})'\n",
    "    if show:\n",
    "        print(qry)\n",
    "    return spark.sql(qry).toPandas().prep().sort_index()\n",
    "\n",
    "\n",
    "############ pandas functions ############\n",
    "# def disp(X, max_rows=3, precision=None, sort=False, **props):\n",
    "def disp(X, max_rows=3, sort=False):\n",
    "    \"\"\"convenient display method\"\"\"\n",
    "    print(X.shape)\n",
    "    X = (X.sort_index(axis=1) if sort else X).reset_index()\n",
    "    Y = pd.DataFrame({'dtype':X.dtypes.astype('string'), 'missing_pct':X.isnull().mean()*100}).T.rename_axis('column').reset_index().prep(case='')\n",
    "    display(Y)\n",
    "    display(X.head(max_rows))\n",
    "    # props = {\n",
    "    #     'text-align': 'center',\n",
    "    #     'vertical-align': 'top',\n",
    "    #     'border': '1px dotted black',\n",
    "    #     'width': 'auto',\n",
    "    #     'font-size': '16px',\n",
    "    #     } | props\n",
    "    # fmt = {'precision': precision, 'hyperlinks': 'html'}\n",
    "    # display(X.head(max_rows).reset_index())\n",
    "    # display(X)\n",
    "    # return display(X.head(max_rows).style\n",
    "    #     .format(**fmt)\n",
    "    #     .format_index(**fmt, axis=0)\n",
    "    #     .format_index(**fmt, axis=1)\n",
    "    #     .set_table_styles([{'selector':k, 'props':[*props.items()]} for k in ['th','td']])\n",
    "    #     .set_table_attributes('style=\"border-collapse: collapse\"')\n",
    "    # )\n",
    "\n",
    "def to_numeric(df, case='lower', downcast='integer', errors='ignore', category=False, **kwargs):\n",
    "    \"\"\"convert to numeric dtypes if possible\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        case = case if case in dir(pd.Series().str) else 'strip'\n",
    "        return (\n",
    "            df\n",
    "            .apply(lambda s: getattr(s.astype('string').str.strip().str,case)() if s.dtype in ['object','string'] else s)  # prep strings\n",
    "            .apply(lambda s: s if pd.api.types.is_datetime64_any_dtype(s) else pd.to_numeric(s, downcast=downcast, errors=errors, **kwargs))  # convert to numeric if possible\n",
    "            .convert_dtypes()  # convert to new nullable dtypes\n",
    "            .apply(lambda s: s.astype('Int64') if pd.api.types.is_integer_dtype(s) else s.astype('category') if s.dtype=='string' and category else s)\n",
    "        )\n",
    "\n",
    "def prep(df, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h = lambda x: x.to_numeric(**kwargs).rename(columns=lambda s: s.lower().strip().replace(' ','_').replace('-','_') if isinstance(s, str) else s)\n",
    "        idx = h(df[[]].reset_index())  # drop columns, reset_index to move index to columns, then apply g\n",
    "        return h(df).reset_index(drop=True).set_index(pd.MultiIndex.from_frame(idx))  # set idx back to df's index\n",
    "\n",
    "def groupb(df, by=None, **kwargs):\n",
    "    \"\"\"my preferred defaults for groupby\"\"\"\n",
    "    kwargs = {'axis':0,'level':None,'as_index':True,'sort':False,'group_keys':False,'observed':False,'dropna':False}|kwargs\n",
    "    return df.groupby(by, **kwargs)\n",
    "\n",
    "def get_incoming(df):\n",
    "    return df.query(\"levl_code=='ug' & styp_code in ['n','r','t']\")\n",
    "\n",
    "def get_duplicates(df, subset='pidm', quit=True, rows=10):\n",
    "    mask = df.groupb(subset, sort=True).transform('size') > 1\n",
    "    if mask.any():\n",
    "        df[mask].disp(rows)\n",
    "        if quit:\n",
    "            raise Exception(f'{mask.sum()} duplicates detected')\n",
    "    return df[mask]\n",
    "\n",
    "def get_missing(df, rows=-1):\n",
    "    miss = df.isnull().mean()*100\n",
    "    if miss.any():\n",
    "        miss[miss>0].sort_values(ascending=False).round(1).disp(rows)\n",
    "    return miss\n",
    "\n",
    "def wrap(fcn):\n",
    "    \"\"\"Make new methods work for Series and DataFrames\"\"\"\n",
    "    def wrapper(X, *args, **kwargs):\n",
    "        df = fcn(pd.DataFrame(X), *args, **kwargs)\n",
    "        return None if df is None else df.squeeze() if isinstance(X, pd.Series) else df  # squeeze to series if input was series\n",
    "    wrapper.__name__ = fcn.__name__\n",
    "    return wrapper\n",
    "\n",
    "for fcn in [\n",
    "    disp,\n",
    "    to_numeric,\n",
    "    prep,\n",
    "    get_incoming,\n",
    "    get_duplicates,\n",
    "    get_missing,\n",
    "    groupb,\n",
    "    ]:\n",
    "    \"\"\"monkey-patch my helpers into Pandas Series & DataFrame classees so we can use df.method syntax\"\"\"\n",
    "    setmeth(pd.DataFrame, fcn)\n",
    "    setmeth(pd.Series, wrap(fcn))\n",
    "\n",
    "############ file i/o functions ############\n",
    "def get_size(path):\n",
    "    os.system(f'du -h {path}')\n",
    "\n",
    "def rm(path, root=False):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.is_file():\n",
    "        path.unlink()\n",
    "    elif path.is_dir():\n",
    "        if root:\n",
    "            shutil.rmtree(path)\n",
    "        else:\n",
    "            for p in path.iterdir():\n",
    "                rm(p, True)\n",
    "    return path\n",
    "\n",
    "def mkdir(path):\n",
    "    path = pathlib.Path(path)\n",
    "    (path if path.suffix == '' else path.parent).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def reset(path):\n",
    "    rm(path)\n",
    "    mkdir(path)\n",
    "    return path\n",
    "\n",
    "def prepr(X):\n",
    "    if isinstance(X, (pd.DataFrame,pd.Series)):\n",
    "        return X.prep()\n",
    "    elif isinstance(X, dict):\n",
    "        return {k: prepr(v) for k, v in X.items()}\n",
    "    elif isinstance(X, (list,tuple,set)):\n",
    "        return type(X)(prepr(v) for v in X)\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def dump(path, obj):\n",
    "    path = reset(path)\n",
    "    obj = prepr(obj)\n",
    "    if path.suffix == '.parquet':\n",
    "        pd.DataFrame(obj).to_parquet(path)  # forced to wrap with explicit pd.DataFrame to due strange error under pandas 2.2.3 \"Object of type PlanMetrics is not JSON serializable\" with to_parquet\n",
    "    elif path.suffix == '.csv':\n",
    "        pd.DataFrame(obj).to_csv(path)\n",
    "    else:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return obj\n",
    "\n",
    "def load(path):\n",
    "    path = pathlib.Path(path)\n",
    "    if path.suffix == '.parquet':\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.suffix == '.csv':\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c44432f-1b9a-42a5-847f-7c44ddd8c8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for src in root.iterdir():\n",
    "    print(src)\n",
    "    df = load(src).prep()\n",
    "    df.disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea992e5-ae91-450a-baa6-3b1ac7271822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['ay'] = df['attended_term'].str[:4].prep()\n",
    "df['level'] = df.groupby('document_id')['ay'].transform('rank', method='dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72579746-9f2e-4b32-8c70-af0ecb68edef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.disp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0812afd0-de01-465c-ad0b-1738e2f95731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crse = 'alg1'\n",
    "df.value_counts('sem_1').sort_index().disp(-1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ocr",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
